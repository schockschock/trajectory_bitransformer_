{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116854f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f74bb289480>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader \n",
    "import torchvision\n",
    "from torchvision import datasets \n",
    "from torch.nn.functional import softmax\n",
    "from torchvision.transforms import ToTensor \n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "import datetime\n",
    "import cv2\n",
    "from skimage.util import img_as_ubyte\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import argparse\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "# Log\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#Transformer Decoder\n",
    "from scratch_transformer.decoder import TransformerDecoder\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd3ca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebook_data/Introvert_ResnetTransf\n"
     ]
    }
   ],
   "source": [
    "__file__=os.getcwd()\n",
    "print(__file__)\n",
    "\n",
    "#Dataset name\n",
    "dataset_name = \"eth\" # dataset options: 'university', 'zara_01', 'zara_02', 'eth', 'hotel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d26b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now() # current date and time\n",
    "current_time_date = now.strftime(\"%d_%m_%y_%H_%M_%S\")\n",
    "run_folder  = \"Outputs/traj_pred_\"+ dataset_name + \"_\" + str(os.path.basename(__file__)) + str(current_time_date)\n",
    "os.makedirs(run_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "185db074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make log folder for tensorboard\n",
    "#SummaryWriter_path = run_folder + \"/log\"\n",
    "#os.makedirs(SummaryWriter_path) \n",
    "SummaryWriter_path = '/notebook_data/work_dirs/first_test'\n",
    "  \n",
    "writer = SummaryWriter(SummaryWriter_path,comment=\"ADE_FDE_Train\")\n",
    "\n",
    "# Make image folder to save outputs\n",
    "image_path  = run_folder + \"/Visual_Prediction\"\n",
    "os.makedirs(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f71b993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataBase Variables\n",
    "image_folder_path       = 'data_trajpred/'+dataset_name\n",
    "DB_PATH_train     = \"data_trajpred/\"+dataset_name+\"/pos_data_train.db\"\n",
    "cnx_train         = sqlite3.connect(DB_PATH_train)\n",
    "DB_PATH_val     = \"data_trajpred/\"+dataset_name+\"/pos_data_val.db\"\n",
    "cnx_val         = sqlite3.connect(DB_PATH_val)\n",
    "DB_DIR      = run_folder + '/database'\n",
    "os.makedirs( DB_DIR )\n",
    "DB_PATH2    = DB_DIR+'/db_one_ped_delta_coordinates_results.db'\n",
    "cnx2        = sqlite3.connect(DB_PATH2)\n",
    "\n",
    "#Other variables\n",
    "T_obs                   = 8\n",
    "T_pred                  = 12\n",
    "T_total                 = T_obs + T_pred #8+12=20\n",
    "data_id                 = 0 \n",
    "batch_size              = 4 #10#100 #15 #2\n",
    "chunk_size              = batch_size * T_total # Chunksize should be multiple of T_total\n",
    "in_size                 = 2\n",
    "stochastic_out_size     = in_size * 2\n",
    "hidden_size             = 256 #!64\n",
    "embed_size              = 64 #16 #!64\n",
    "global dropout_val\n",
    "dropout_val             = 0.2 #0.5\n",
    "teacher_forcing_ratio   = 0.7 # 0.9\n",
    "regularization_factor   = 0.5 # 0.001\n",
    "avg_n_path_eval         = 20\n",
    "bst_n_path_eval         = 20\n",
    "path_mode               = \"top5\" #\"avg\",\"bst\",\"single\",\"top5\"\n",
    "regularization_mode     = \"regular\" #\"weighted\",\"e_weighted\", \"regular\"\n",
    "startpoint_mode         = \"on\" #\"on\",\"off\"\n",
    "enc_out                 = \"on\" #\"on\",\"off\"\n",
    "biased_loss_mode        = 0 # 0 , 1\n",
    "\n",
    "\n",
    "table_out   = \"results_delta\"\n",
    "table       = \"dataset_T_length_20delta_coordinates\" #\"dataset_T_length_\"+str(T_total)+\"delta_coordinates\"\n",
    "df_id       = pd.read_sql_query(\"SELECT data_id FROM \"+table, cnx_train)\n",
    "data_size   = df_id.data_id.max() * T_total\n",
    "epoch_num   = 100\n",
    "from_epoch  = 0\n",
    "\n",
    "#Visual Variables\n",
    "image_size              = 256  \n",
    "image_dimension         = 3\n",
    "mask_size               = 16\n",
    "visual_features_size    = 128 \n",
    "visual_embed_size       = 64  #128 #256 #64\n",
    "vsn_module_out_size    = 512\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "\n",
    "#Model Path\n",
    "model_path = run_folder + \"/NNmodel\" \n",
    "os.makedirs(model_path)   \n",
    "model_path = model_path + str(\"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d938d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_mode         = 1 #output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c822952d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87ee99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Matrice Standard\n",
    "if dataset_name == 'eth':  \n",
    "    h = np.array([[2.8128700e-02, 2.0091900e-03, -4.6693600e+00],[8.0625700e-04, 2.5195500e-02, -5.0608800e+00],[ 3.4555400e-04, 9.2512200e-05, 4.6255300e-01]]) #h pour eth\n",
    "if dataset_name =='hotel':        \n",
    "    h = np.array([[1.1048200e-02, 6.6958900e-04, -3.3295300e+00],[-1.5966000e-03, 1.1632400e-02, -5.3951400e+00],[1.1190700e-04, 1.3617400e-05, 5.4276600e-01]]) #h pour hotel\n",
    "if dataset_name =='zara01': \n",
    "    h = np.array([[0.02174104, 0, -0.15],[0, -0.02461883, 13.77429807],[0, 0, 1]]) #h pour zara1\n",
    "if dataset_name =='zara02': \n",
    "    h = np.array([[0.02174104, 0, -0.4],[0, -0.02386598, 14.98401686],[0, 0, 1]]) #h pour zara2\n",
    "if dataset_name =='univ':\n",
    "    h = np.array([[0.02220407, 0, -0.48],[0, -0.02477289, 13.92551292],[0, 0, 1]]) #h pour univ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29766a",
   "metadata": {},
   "source": [
    "# Classe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae2f2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryPredictionDataset(torch.utils.data.Dataset): \n",
    "#Enc.cinématique reçoit la trajectoire observée de humain cible (input) de la forme T=(u1,u2-u1,u3-u2,..) qui consiste en les coordonnées de la position de départ et en les déplacements relatifs de l'humain entre les images consécutives.\n",
    "#Ce format a été choisi car il permet au modèle de mieux capturer les similarités entre des trajectoires presque identiques qui peuvent avoir des points de départ différents. \n",
    "    def __init__(self, ROOT_DIR, DB_PATH, cnx):\n",
    "        \n",
    "        self.pos_df    = pd.read_sql_query(\"SELECT * FROM \"+str(table), cnx)\n",
    "        self.root_dir  = ROOT_DIR+'/visual_data'\n",
    "        self.transform = torchvision.transforms.Compose([torchvision.transforms.Resize((image_size,image_size)), \\\n",
    "                                                         torchvision.transforms.ToTensor(), \\\n",
    "                                                         torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "        self.visual_data = []\n",
    "        # read sorted frames\n",
    "        for img in sorted(os.listdir(self.root_dir)): \n",
    "            self.visual_data.append(self.transform( Image.open(os.path.join(self.root_dir)+\"/\"+img) ))\n",
    "        self.visual_data = torch.stack(self.visual_data)  \n",
    "        #print(\"visual_data:\", self.visual_data) #tensor\n",
    "        #print(\"shapevisual_data:\", self.visual_data.shape) #torch.Size([1298, 3, 256, 256]) : 1298 blocs dans chaque bloc 3 sous blocs, et dans chaque sous bloc: 256 lignes et 256 colonnes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pos_df.data_id.max() #data_id maximum dans dataset\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist() \n",
    "        #print(\"idx :\", idx)\n",
    "        \n",
    "        extracted_df     = self.pos_df[ self.pos_df[\"data_id\"] == idx ] #table dont data_id=idx\n",
    "\n",
    "        tensor           = torch.tensor(extracted_df[['pos_x_delta','pos_y_delta']].values).reshape(-1,T_total,in_size) #juste pos_x_delta et pos_y_delta de extracted_df (tensor)\n",
    "        obs, pred        = torch.split(tensor,[T_obs,T_pred],dim=1) #obs de 8 et pred de 12 à partir de tensor construit\n",
    "\n",
    "        start_frames     = (extracted_df.groupby('data_id').frame_num.min().values/10).astype('int') #extracted_df dont data_id=idx, on prend minimum frame_num et aprés on divise par 10, cela represente start_frame\n",
    "        extracted_frames = []\n",
    "        for i in start_frames:            \n",
    "            extracted_frames.append(self.visual_data[i:i+T_obs])\n",
    "        frames = torch.stack(extracted_frames) #stack concatenates a sequence of tensors along a new dimension.\n",
    "        start_frames = torch.tensor(start_frames) #tensor([start_frames])\n",
    "        return obs, pred, frames, start_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771d406",
   "metadata": {},
   "source": [
    "# Initialize random weights for NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1b23f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c232eb",
   "metadata": {},
   "source": [
    "# Regularizer loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "974cccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_sigma_distance  = torch.zeros(1) #tensor[(0.)]\n",
    "\n",
    "def distance_from_line_regularizer(input_tensor,prediction):\n",
    "    global sum_sigma_distance\n",
    "    #the regularization is defined as the sum of Euclidean distances between each step of the predicted trajectory Tf , and a line fitted to the observed trajectory To.\n",
    "    # Fit a line to observation points over batch \n",
    "    input_tensor    = input_tensor.double()\n",
    "    prediction      = prediction.double()\n",
    "    input_tensor    = input_tensor.cumsum(dim=1).double()\n",
    "    X               = torch.ones_like(input_tensor).double().to('cuda', non_blocking=True)\n",
    "    X[:,:,0]        = input_tensor[:,:,0]\n",
    "    Y               = (input_tensor[:,:,1]).unsqueeze(-1).double()\n",
    "    try:\n",
    "        try:\n",
    "            XTX_1           = torch.matmul( X.transpose(-1,-2), X).double().inverse()\n",
    "        except:\n",
    "            XTX_1           = torch.matmul( X.transpose(-1,-2), X).double().pinverse()\n",
    "        XTY             = torch.matmul( X.transpose(-1,-2), Y)\n",
    "        theta           = torch.matmul( XTX_1.double(), XTY.double())\n",
    "        # Calculate real values of prediction instead of delta\n",
    "        prediction[:,:,0] = prediction[:,:,0] + input_tensor[:,-1,0].unsqueeze(-1) \n",
    "        prediction[:,:,1] = prediction[:,:,1] + input_tensor[:,-1,1].unsqueeze(-1)\n",
    "        \n",
    "        # Calculate distance ( predicted_points , observation_fitted_line ) over batch\n",
    "        theta0x0        = theta[:,0,:].double() * prediction[:,:,0].double()\n",
    "        denominator     = torch.sqrt( theta[:,0,:].double() * theta[:,0,:].double() + 1 )\n",
    "        nominator       = theta0x0 + theta[:,1,:] - prediction[:,:,1].double()\n",
    "        distance        = nominator.abs() / denominator\n",
    "        if regularization_mode =='weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().float(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(-1,T_pred)\n",
    "            weighted_distance   = weight * distance\n",
    "\n",
    "        elif regularization_mode =='e_weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().float(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(distance.size(0),T_pred)\n",
    "            weight              = torch.exp(weight)\n",
    "            weighted_distance   = weight*distance\n",
    "\n",
    "        else:\n",
    "            weighted_distance = distance\n",
    "        sigma_distance  = torch.mean(weighted_distance,1)\n",
    "        sum_sigma_distance  = torch.mean(sigma_distance)\n",
    "        return sum_sigma_distance\n",
    "    except:\n",
    "        print(\"SINGULAR VALUE\")\n",
    "        sum_sigma_distance = torch.zeros(1).to('cuda', non_blocking=True) + 20\n",
    "        return sum_sigma_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a4d6b",
   "metadata": {},
   "source": [
    "# Encoder Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a109d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderSelfAttentionNew(nn.Module):\n",
    "    def __init__(self,device,d_model, d_k, d_v, n_head, dff=2048, dropout_transformer=.1, n_module=6):\n",
    "        super(EncoderSelfAttentionNew, self).__init__()\n",
    "        \n",
    "        \n",
    "        self.encoder = nn.ModuleList([MultiHeadAttention(d_model, d_k, d_v, n_head, dff, dropout_transformer)\n",
    "                                      for _ in range(n_module)])\n",
    "        self.device = device\n",
    "    def forward(self, x):\n",
    "    \n",
    "        \n",
    "        in_encoder = x + sinusoid_encoding_table(x.shape[1], x.shape[2]).expand(x.shape).to(self.device)\n",
    "        for l in self.encoder:\n",
    "            in_encoder = l(in_encoder, in_encoder, in_encoder)\n",
    "        return in_encoder\n",
    "\n",
    "class EncoderTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Transformer adapted from the MTN Paper.\n",
    "    It only takes as input the (x,y) coordinates as in our case the camera is static (the MTN was used for a dynamic \"ego-car\" context)\n",
    "    \"\"\"\n",
    "    def __init__(self,device,d_model,d_k, d_v,n_head=8,n_module=6,ff_size=1024,dropout1d=0.5, batch_size=1):\n",
    "        super(EncoderTransformer, self).__init__()\n",
    "        self.device = device\n",
    "        self.fc = nn.Linear(2,d_model)\n",
    "        self.self_attention = EncoderSelfAttentionNew(device,d_model,d_k,d_v,n_head=n_head,dff=ff_size,dropout_transformer=dropout1d,n_module=n_module)\n",
    "        #self.ffn = nn.Linear(ff_size,d_model)\n",
    "        self.relu =  nn.ReLU()\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8,d_model)) #final pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        #print(x.size())\n",
    "        #x = x.to(device)\n",
    "\n",
    "        x = x.view(shape[0],shape[1],-1)\n",
    "        x = self.fc(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.self_attention(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.squeeze(dim=1) #final pooling\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e05673",
   "metadata": {},
   "source": [
    "# Encoder Transformer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bafea31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CoordinatesTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self,device,d_model,d_k, d_v,n_head=8,n_module=6,ff_size=1024,dropout1d=0.5, batch_size=1):\n",
    "        super(CoordinatesTransformer, self).__init__()\n",
    "\n",
    "        \n",
    "        # self.conv_model.to(device)\n",
    "        self.linear_mapper= torch.nn.Sequential(\n",
    "                              torch.nn.Linear(2, 32),\n",
    "                              torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(32, 64),\n",
    "                              torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(64, 128),\n",
    "                              torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(128, d_model),\n",
    "                              torch.nn.ReLU(),)\n",
    "        \n",
    "        \n",
    "        self.self_attention = EncoderSelfAttentionNew(device,d_model,d_k,d_v,n_head=n_head,dff=ff_size,dropout_transformer=dropout1d,n_module=n_module)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8,d_model)) #final pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        # bs = shape[0] # To know the batch size\n",
    "        # x = self.features(x)\n",
    "        # print(x.shape)\n",
    "        x = self.linear_mapper(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(shape[0],shape[1],-1)\n",
    "        # print(x.shape)\n",
    "        x = self.self_attention(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = self.pool(x).squeeze(dim=1) #final pooling\n",
    "        # print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46a0ae",
   "metadata": {},
   "source": [
    "# Resnet_Transformer_Vision Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8404554b",
   "metadata": {},
   "source": [
    "### Spatial features extraction (Resnet 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "037ff97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet(pretrain=True,layers_to_unfreeze=8,layers_to_delete=2,in_planes=3):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        pretrain: Define if we load a pretrained model from ImageNet\n",
    "        layers_to_unfreeze: Define the number of layers that we want to train at the end of the Resnet\n",
    "        layers_to_delete: Define the numbers of layers that we want to delete\n",
    "        in_planes: Define the numbers of input channels of images (supported values: 1,2 or 3)\n",
    "    return: The Resnet model\n",
    "    \"\"\"\n",
    "    resnet = torchvision.models.resnet18(pretrained=pretrain)\n",
    "    # Create a new model cause we don't want the pooling operation at the end and the classifier\n",
    "    model = nn.Sequential()\n",
    "    number_of_layers = len(list(resnet.children())) - layers_to_delete # In practice it remove the pooling operation and the classifier\n",
    "\n",
    "    if number_of_layers<layers_to_unfreeze:\n",
    "        layers_to_unfreeze = number_of_layers\n",
    "    layers_to_freeze = number_of_layers - layers_to_unfreeze\n",
    "    i=0\n",
    "    for child in resnet.children():\n",
    "        # For the first layers we create a new weight if in_planes is not 3 cause ResNet is pretrain on image with 3 channels there is no version for 1 channel\n",
    "        if i==0 and in_planes<3:\n",
    "            if i<layers_to_freeze: # Define if we freeze this layer or no\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False # Freeze the layers by passing requires_grad attribute to False\n",
    "            w = child._parameters['weight'].data # Get the weight for 3 channels data\n",
    "            child._modules['0'] = nn.Conv2d(in_planes, 64, kernel_size=3, padding=1) # Define the new conv layer\n",
    "            if in_planes == 1:\n",
    "                child._parameters['weight'].data = w.mean(dim=1, keepdim=True) # If the number of channels is 1 we made the mean of channels to set the new weight\n",
    "            else:\n",
    "                child._parameters['weight'].data = w[:, :-1] * 1.5\n",
    "\n",
    "        if i<layers_to_freeze: # Define if we freeze this layer or no\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False # Freeze the layers by passing requires_grad attribute to False\n",
    "        if i<number_of_layers: # To define if we keep this layer or not\n",
    "            model.append(child) \n",
    "        i+=1\n",
    "    return model\n",
    "\n",
    "\n",
    "class features_extraction(nn.Module):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    conv_model: The convolution model used before capsules for the moment only ResNet is supported\n",
    "    in_planes: Numbers of channels for the image\n",
    "    \"\"\"\n",
    "    def __init__(self,conv_model,in_planes: int):\n",
    "        super().__init__()\n",
    "        self.conv_model = conv_model\n",
    "        self.in_planes = in_planes\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "    def forward(self,input):\n",
    "        shape = input.size()\n",
    "        x = input.view(-1,self.in_planes,shape[-2],shape[-1])\n",
    "        x = self.conv_model(x)\n",
    "        x = self.pooling(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b6e3b",
   "metadata": {},
   "source": [
    "### Encoder Transformer (Vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37e868f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_embedding(input, d_model):\n",
    "    input = input.view(-1, 1)\n",
    "    dim = torch.arange(d_model // 2, dtype=torch.float32, device=input.device).view(1, -1)\n",
    "    sin = torch.sin(input / 10000 ** (2 * dim / d_model))\n",
    "    cos = torch.cos(input / 10000 ** (2 * dim / d_model))\n",
    "\n",
    "    out = torch.zeros((input.shape[0], d_model), device=input.device)\n",
    "    out[:, ::2] = sin\n",
    "    out[:, 1::2] = cos\n",
    "    return out\n",
    "\n",
    "def sinusoid_encoding_table(max_len, d_model):\n",
    "    pos = torch.arange(max_len, dtype=torch.float32)\n",
    "    out = position_embedding(pos, d_model)\n",
    "    return out\n",
    "\n",
    "#Scaled dot-product attention\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h):\n",
    "        \"\"\"\n",
    "        param:\n",
    "        d_model: Output dimensionality of the model\n",
    "        d_k: Dimensionality of queries and keys\n",
    "        d_v: Dimensionality of values\n",
    "        h: Number of heads\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "        self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.init_weights(gain=1.0)\n",
    "\n",
    "    def init_weights(self, gain=1.0):\n",
    "        nn.init.xavier_normal_(self.fc_q.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_k.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_v.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_o.weight, gain=gain)\n",
    "        nn.init.constant_(self.fc_q.bias, 0)\n",
    "        nn.init.constant_(self.fc_k.bias, 0)\n",
    "        nn.init.constant_(self.fc_v.bias, 0)\n",
    "        nn.init.constant_(self.fc_o.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_model)\n",
    "        :param keys: Keys (b_s, nk, d_model)\n",
    "        :param values: Values (b_s, nk, d_model)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "\n",
    "        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "\n",
    "        att = torch.softmax(att, -1)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "        return out\n",
    "\n",
    "    \n",
    "#Multi-head attention layer with Dropout and Layer Normalization\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h, dff=2048, dropout=.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(d_model=d_model, d_k=d_k, d_v=d_v, h=h)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Sequential(*[nn.Linear(d_model, dff), nn.ReLU(inplace=False), nn.Dropout(p=dropout),nn.Linear(dff, d_model)])\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        att = self.attention(queries, keys, values)\n",
    "        att = self.dropout(att)\n",
    "        att = self.fc(att)\n",
    "        att = self.dropout(att)\n",
    "        return self.layer_norm(queries + att)\n",
    "\n",
    "class EncoderSelfAttention(nn.Module):\n",
    "    def __init__(self,device,d_model, d_k, d_v, n_head, dff=2048, dropout_transformer=.1, n_module=6):\n",
    "        super(EncoderSelfAttention, self).__init__()\n",
    "        self.encoder = nn.ModuleList([MultiHeadAttention(d_model, d_k, d_v, n_head, dff, dropout_transformer)\n",
    "                                      for _ in range(n_module)])\n",
    "        self.device = device\n",
    "    def forward(self, x):\n",
    "        in_encoder = x + sinusoid_encoding_table(x.shape[1], x.shape[2]).expand(x.shape).to(self.device)\n",
    "        for l in self.encoder:\n",
    "            in_encoder = l(in_encoder, in_encoder, in_encoder)\n",
    "        return in_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc7e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "266dafd0",
   "metadata": {},
   "source": [
    "### Resnet + Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d8bddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _GestureTransformer(nn.Module):\n",
    "    \"\"\"Multi-Modal model on 3 or 1 channel\"\"\"\n",
    "    def __init__(self,device,backbone=\"resnet\",in_planes=3,pretrained= True,input_dim=512,layers_to_unfreeze=8,layers_to_delete=2,n_head=8,n_module=6,ff_size=1024,dropout1d=0.5):\n",
    "        super(_GestureTransformer, self).__init__()\n",
    "\n",
    "        self.in_planes = in_planes\n",
    "        self.device = device\n",
    "        self.conv_name = backbone\n",
    "        self.conv_model = None\n",
    "        \n",
    "        if self.conv_name.lower()==\"resnet\":\n",
    "            self.conv_model = Resnet(pretrained,layers_to_unfreeze,layers_to_delete,in_planes)\n",
    "        else:\n",
    "            raise NotImplementedError(\"The model {} is not supported!\".format(self.conv_name))\n",
    "        self.conv_model.to(device)\n",
    "        self.features = features_extraction(self.conv_model,in_planes)\n",
    "\n",
    "        self.self_attention = EncoderSelfAttention(device,input_dim,64,64,n_head=n_head,dff=ff_size,dropout_transformer=dropout1d,n_module=n_module)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8,input_dim)) #final pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        x = self.features(x)\n",
    "        x = x.view(shape[0],shape[1],-1)\n",
    "        x = self.self_attention(x)\n",
    "\n",
    "        x = self.pool(x).squeeze(dim=1) #final pooling\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058102f7",
   "metadata": {},
   "source": [
    "## Création du modèle en entier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4442ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,device, embed_size, code_size=512, target_size = 12, dropout_val=dropout_val, batch_size=1):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        self.encoder = EncoderTransformer(device,code_size,64,64,dropout1d=dropout_val, batch_size=batch_size) #EncoderTransformer\n",
    "        #self.encoder.apply(init_weights)\n",
    "        #self.encoder = CoordinatesTransformer(device,code_size,64,64,dropout1d=dropout_val, batch_size=batch_size) #EncoderTransformer\n",
    "        \n",
    "        embed_dim = 2*code_size\n",
    "        self.decoder = TransformerDecoder(target_size, embed_dim=embed_dim, seq_len=12, num_layers=2, expansion_factor=4, n_heads=8) \n",
    "        #self.decoder.apply(init_weights)\n",
    "        \n",
    "        self.vsn_module = _GestureTransformer(device,input_dim=code_size,dropout1d=dropout_val) #_GestureTransformer(partie vision)                   \n",
    "        self.vsn_module.apply(init_weights)\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool1d((code_size)) # add a pooling (to have the same shape)\n",
    "        \n",
    "        self.code_pooling = nn.AdaptiveAvgPool2d((target_size,embed_dim))\n",
    "        \n",
    "        if device.type=='cuda':\n",
    "            self.encoder.cuda()\n",
    "            self.decoder.cuda()\n",
    "            self.vsn_module.cuda()   \n",
    "            \n",
    "    def forward(self,input_tensor, target_tensor, visual_input_tensor, batch_size, train_mode):\n",
    "        batch_size      = int(input_tensor.size(0))\n",
    "        \n",
    "        encoder_output =  self.encoder(input_tensor) #(bs,8,code_size)\n",
    "\n",
    "        #start_point\n",
    "#         start_point     = (input_tensor[:,0,:]).to(device).clone().detach()\n",
    "        \n",
    "#         if startpoint_mode==\"on\":\n",
    "#             input_tensor[:,0,:]    = 0\n",
    "            \n",
    "        visual_initial_vsn          = self.vsn_module(visual_input_tensor)\n",
    "        visual_initial_vsn          = self.pooling(visual_initial_vsn) #pooling qu'on a ajouté\n",
    "        \n",
    "#         print(\"#######\")\n",
    "#         print(f\"encoder_output size : {encoder_output.size()}\")\n",
    "#         print(f\"visual_initial_vsn size : {visual_initial_vsn.size()}\")\n",
    "#         print(\"#######\")\n",
    "        \n",
    "        #Creattion du code (concatenation du transformer coordonnées et transformer resnet)\n",
    "        code = torch.cat((encoder_output,visual_initial_vsn),-1).to(device).double()\n",
    "        \n",
    "        code = self.code_pooling(code) #pooling qu'on a rajouté pour que la taille du code soit la meme que la target\n",
    "#         print(\"#######\")\n",
    "#         print(code.size())\n",
    "#         print(\"#######\")\n",
    "        trg_mask = self.make_trg_mask(target_tensor)\n",
    "        #print(f\"target_tensor : {target_tensor.size()}\")\n",
    "        decoder_output = self.decoder(target_tensor,code,trg_mask)\n",
    "\n",
    "        return decoder_output\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg: target sequence\n",
    "        Returns:\n",
    "            trg_mask: target mask\n",
    "        \"\"\"\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = 12\n",
    "        #batch_size, trg_len = trg.shape\n",
    "        # returns the lower triangular part of matrix filled with ones\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            batch_size, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092aeaf",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d5560a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing train dataset\n",
      "Batch_size : 4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects a non-empty TensorList",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch_size : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(batch_size))\n\u001b[1;32m      6\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m TrajectoryPredictionDataset(image_folder_path, DB_PATH_train, cnx_train)\n\u001b[0;32m----> 7\u001b[0m train_loader  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\u001b[43mdataset_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m]\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m validation_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m, in \u001b[0;36mTrajectoryPredictionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m start_frames:            \n\u001b[1;32m     35\u001b[0m     extracted_frames\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual_data[i:i\u001b[38;5;241m+\u001b[39mT_obs])\n\u001b[0;32m---> 36\u001b[0m frames \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextracted_frames\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#stack concatenates a sequence of tensors along a new dimension.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m start_frames \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(start_frames) \u001b[38;5;66;03m#tensor([start_frames])\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obs, pred, frames, start_frames\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects a non-empty TensorList"
     ]
    }
   ],
   "source": [
    "#train dataset and loader\n",
    "\n",
    "batch_size = 4\n",
    "print(\"Initializing train dataset\")\n",
    "print(\"Batch_size : {}\".format(batch_size))\n",
    "dataset_train = TrajectoryPredictionDataset(image_folder_path, DB_PATH_train, cnx_train)\n",
    "train_loader  = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=True, pin_memory=True)\n",
    "validation_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c87cc8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2Seq(device,512,dropout_val=dropout_val)\n",
    "model = model.to(device).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5c350434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c530e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision\n",
    "# from torchview import draw_graph\n",
    "\n",
    "# g = draw_graph(model, input_size())\n",
    "# g.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c64afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, pred, visual_obs, frame_tensor   = dataset_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3405963e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 3, 256, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual_obs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "79f3299e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 2])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c97d63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = obs.cuda().double()\n",
    "pred = pred.cuda().double()\n",
    "visual_obs = visual_obs.cuda().double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54987a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be42cb32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([1, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([1, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([1, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([1, 12, 2])\n",
      "torch.Size([1, 12, 8, 128])\n",
      "torch.Size([1, 12, 8, 128])\n",
      "torch.Size([1, 12, 8, 128])\n",
      "torch.Size([1, 12, 8, 128])\n",
      "torch.Size([1, 12, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 2])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(obs, pred, visual_obs, batch_size=4, train_mode=0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a206643e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n",
      "torch.Size([1, 8, 2])\n",
      "torch.Size([1, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([1, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([1, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([1, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([1, 12, 2])\n",
      "torch.Size([1, 12, 8, 128])\n",
      "torch.Size([1, 12, 8, 128])\n",
      "torch.Size([1, 12, 8, 128])\n",
      "torch.Size([1, 12, 8, 128])\n",
      "torch.Size([1, 12, 2])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'epoch_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m#print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m#print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\u001b[39;00m\n\u001b[1;32m     68\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epoch_loss' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "learning_step               = 40\n",
    "initial_learning_rate       = 0.01\n",
    "clip                        = 1\n",
    "\n",
    "# MSE loss\n",
    "criterion                   = nn.MSELoss(reduction='mean')\n",
    "criterion_vision            = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# SGD optimizer\n",
    "optimizer                   = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9, weight_decay=0.01) #SGD\n",
    "scheduler                   = torch.optim.lr_scheduler.StepLR(optimizer, step_size=learning_step, gamma=0.1)\n",
    "five_fold_cross_validation  = 0\n",
    "\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    ADE = 0\n",
    "    FDE = 0\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        obs, pred, visual_obs, frame_tensor              = data\n",
    "\n",
    "        #         obs = obs.view((obs.shape[0],obs.shape[2],obs.shape[3]))\n",
    "        #         pred = pred.view((pred.shape[0],pred.shape[2],pred.shape[3]))\n",
    "        #         visual_obs = visual_obs.view((visual_obs.shape[0],visual_obs.shape[2],visual_obs.shape[3]))\n",
    "        #         obs = obs.view((obs.shape[0],obs.shape[2],obs.shape[3]))\n",
    "\n",
    "        input_tensor, output_tensor                      = obs.double().squeeze(dim=1).to('cuda', non_blocking=True), pred.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "        visual_input_tensor                              = visual_obs.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "\n",
    "        print('####')\n",
    "        print(input_tensor.size())\n",
    "\n",
    "        prediction = model(input_tensor, output_tensor, visual_input_tensor, batch_size=batch_size, train_mode=0)\n",
    "        calculated_prediction = prediction.cumsum(axis=1) #calculated_prediction\n",
    "\n",
    "        loss_line_regularizer = distance_from_line_regularizer(input_tensor.clone(),calculated_prediction.clone())\n",
    "\n",
    "        if biased_loss_mode:\n",
    "            weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "            weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "            loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor.clone(),dim=-2)*weight)\n",
    "        else:\n",
    "            loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2)) #mean squared error (lmse)\n",
    "        \n",
    "        out_x       = output_tensor[:,:,0].cumsum(axis=1)\n",
    "        out_y       = output_tensor[:,:,1].cumsum(axis=1)\n",
    "        pred_x      = calculated_prediction[:,:,0]\n",
    "        pred_y      = calculated_prediction[:,:,1]\n",
    "        ADE         = ADE + ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)       \n",
    "\n",
    "\n",
    "        #Backward Propagation\n",
    "        total_loss      = loss.double() + torch.tensor(regularization_factor).to('cuda', non_blocking=True) * loss_line_regularizer.double() #total loss\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        #print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "        epoch_loss += total_loss.item()\n",
    "        #print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "        start_time = time.time()\n",
    "        torch.cuda.empty_cache()\n",
    "        writer.close()\n",
    "        count_div=i\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "        \n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16eb4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    torch.save(state, os.path.join(save_path,filename))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(save_path,filename), os.path.join(save_path,'model_best.pth'))\n",
    "        \n",
    "def train(model, optimizer, scheduler, criterion, criterion_vision, clip,train_loader, validation_loader):\n",
    "    global batch_size\n",
    "    i               = None\n",
    "    checked_frame   = 0\n",
    "\n",
    "    losses = []\n",
    "    print(\"Data Size \",data_size,\"\\tChunk Size \",chunk_size)\n",
    "    global teacher_forcing_ratio\n",
    "    counter =0\n",
    "    best_val = float(\"inf\")\n",
    "    save_path = './save_models'\n",
    "    for j in range(epoch_num):\n",
    "        model.train()\n",
    "        epoch_loss=0\n",
    "        if j%7 == 6:\n",
    "            teacher_forcing_ratio = (teacher_forcing_ratio - 0.2) if teacher_forcing_ratio>=0.1 else 0.0\n",
    "\n",
    "        # Update TeachForce ratio to gradually change during training\n",
    "        # global teacher_forcing_ratio\n",
    "        # teacher_forcing_ratio-= 1/epoch_num\n",
    "        print(\"TEACHER FORCE RATIO\\t\",teacher_forcing_ratio)\n",
    "        #print(\"Learning Rate\\t\", scheduler.get_last_lr())\n",
    "\n",
    "        \n",
    "        if(j>=from_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            start_time = time.time()\n",
    "            ADE = 0\n",
    "            FDE = 0\n",
    "            i   = 0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                #print(\"\\n--------------- Batch %d/ %d ---------------\"%(j,i)) #(epoch/i)\n",
    "                # Forward\n",
    "                obs, pred, visual_obs, frame_tensor = data\n",
    "                \n",
    "                input_tensor, output_tensor         = obs.double().squeeze(dim=1).to('cuda', non_blocking=True), pred.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "                    \n",
    "                visual_input_tensor                 = visual_obs.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "                \n",
    "                prediction = model(input_tensor, output_tensor, visual_input_tensor, batch_size=batch_size, train_mode=1)\n",
    "                \n",
    "                calculated_prediction = prediction.cumsum(axis=1) #calculated_prediction\n",
    "                \n",
    "                loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction) #loss (regularisation term Lreg)\n",
    "                \n",
    "                if biased_loss_mode:\n",
    "                    weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "                    weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "                    loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "                else:\n",
    "                    loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2)) #mean squared error (lmse)\n",
    "                    \n",
    "                out_x       = output_tensor[:,:,0].cumsum(axis=1)\n",
    "                out_y       = output_tensor[:,:,1].cumsum(axis=1)\n",
    "                pred_x      = calculated_prediction[:,:,0]\n",
    "                pred_y      = calculated_prediction[:,:,1]\n",
    "                ADE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "                # FDE      += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "                \n",
    "                # Backward Propagation\n",
    "                total_loss      = loss.double() + torch.tensor(regularization_factor).to('cuda', non_blocking=True) * loss_line_regularizer.double() #total loss\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "                #print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "                epoch_loss += total_loss.item()\n",
    "                #print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                torch.cuda.empty_cache()\n",
    "                writer.close()\n",
    "                count_div=i\n",
    "            \n",
    "            # tensorboard log\n",
    "            writer.add_scalar('ADE/train', ADE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('FDE/train', FDE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('LOSS/train', epoch_loss/(count_div+1), counter)\n",
    "            counter += 1\n",
    "\n",
    "        if scheduler.get_last_lr()[0]>0.001:\n",
    "            scheduler.step()\n",
    "        # validation(model, optimizer, criterion, criterion_vision, clip, validation_loader, j) \n",
    "        epoch_loss = epoch_loss / (int(data_size/chunk_size))\n",
    "        losses.append(epoch_loss)\n",
    "        display.clear_output(wait=True)\n",
    "        plt.plot(losses, '--ro', label='train loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'epoch {j}')\n",
    "        plt.show()\n",
    "        print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "        print(\"EPOCH \", j, \"\\tLOSS \", epoch_loss)\n",
    "        writer.add_scalar('epoch_loss/train', epoch_loss/ (int(data_size/chunk_size)), j ) #see how model performs on the training dataset\n",
    "        #torch.save( model.state_dict(), model_path+\"_current\")\n",
    "        print(\"-----------------------------------------------\\n\"+\"-----------------------------------------------\")\n",
    "\n",
    "        # save checkpoint for each epoch and a fine called best_model so far \n",
    "        print(np.argmin(losses))\n",
    "        is_best = epoch_loss < best_val\n",
    "        best_val = min(epoch_loss, best_val)\n",
    "        print(\"bestvaleur\", best_val)\n",
    "        save_checkpoint({'epoch': j+1,'state_dict': model.module.state_dict(),'optimizer': optimizer.state_dict(),'scheduler': scheduler.state_dict(),'best_loss': best_val}, is_best, save_path, 'epoch_{}.pth'.format(j+1))\n",
    "        \n",
    "    return epoch_loss / (int(data_size/chunk_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2af4b261",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "path mode\t top5\n",
      "Data Size  4900 \tChunk Size  80\n",
      "TEACHER FORCE RATIO\t 0.7\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n",
      "torch.Size([4, 8, 2])\n",
      "##########\n",
      "#######\n",
      "encoder_output size : torch.Size([4, 8, 512])\n",
      "visual_initial_vsn size : torch.Size([4, 8, 512])\n",
      "#######\n",
      "#######\n",
      "torch.Size([4, 12, 1024])\n",
      "#######\n",
      "target_tensor : torch.Size([4, 12, 2])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 8, 128])\n",
      "torch.Size([4, 12, 2])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath mode\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,path_mode)\n\u001b[0;32m----> 4\u001b[0m loss  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_vision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOSS \u001b[39m\u001b[38;5;124m\"\u001b[39m,loss)\n",
      "Cell \u001b[0;32mIn[51], line 69\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, criterion, criterion_vision, clip, train_loader, validation_loader)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# FDE      += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Backward Propagation\u001b[39;00m\n\u001b[1;32m     68\u001b[0m total_loss      \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdouble() \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(regularization_factor)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m*\u001b[39m loss_line_regularizer\u001b[38;5;241m.\u001b[39mdouble() \u001b[38;5;66;03m#total loss\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"TRAIN\")\n",
    "model.train()\n",
    "print(\"path mode\\t\",path_mode)\n",
    "loss  = train(model, optimizer, scheduler, criterion, criterion_vision, clip, train_loader, validation_loader)\n",
    "print(\"LOSS \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e16f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderTransformer(device,512,64,64,dropout1d=dropout_val)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52606fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model(dataset_train[0][0].cuda().float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a1f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b5d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b06bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train[0][0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d586be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((1,1,8,2))\n",
    "a.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352f355",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.view((a.shape[-2],a.shape[-1])).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1e5da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((1, 8, 1024))\n",
    "p = nn.AdaptiveAvgPool2d((12,1024))\n",
    "\n",
    "p(a).size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
