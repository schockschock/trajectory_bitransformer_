{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f123968",
   "metadata": {},
   "source": [
    "## Dans ce notebook :\n",
    "\n",
    "on test un encoder multimodal qui prend en entrée les coordonnées \n",
    "et les features extraites d'un Resnet. On utilise le principe de Cross-Attention du papier \"Multi-modal \n",
    "Transformers\", on utilise ensuite le décoder classique (transformer aussi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116854f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7febfbfe6890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader \n",
    "import torchvision\n",
    "from torchvision import datasets \n",
    "from torch.nn.functional import softmax\n",
    "from torchvision.transforms import ToTensor \n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "import datetime\n",
    "import cv2\n",
    "from skimage.util import img_as_ubyte\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import argparse\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "# Log\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#Transformer Decoder\n",
    "from models.scratch_transformer.decoder import TransformerDecoder\n",
    "from models.cross_attention_transformer.encoder import Encoder\n",
    "from models.cross_attention_transformer.encoder_layer import EncoderLayer\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd3ca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebook_data/github_repo\n"
     ]
    }
   ],
   "source": [
    "__file__=os.getcwd()\n",
    "print(__file__)\n",
    "\n",
    "#Dataset name\n",
    "dataset_name = \"eth\" # dataset options: 'university', 'zara_01', 'zara_02', 'eth', 'hotel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d26b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now() # current date and time\n",
    "current_time_date = now.strftime(\"%d_%m_%y_%H_%M_%S\")\n",
    "run_folder  = \"Outputs/traj_pred_\"+ dataset_name + \"_\" + str(os.path.basename(__file__)) + str(current_time_date)\n",
    "os.makedirs(run_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185db074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make log folder for tensorboard\n",
    "#SummaryWriter_path = run_folder + \"/log\"\n",
    "#os.makedirs(SummaryWriter_path) \n",
    "SummaryWriter_path = '/notebook_data/work_dirs/cross_attention_test_1'\n",
    "  \n",
    "writer = SummaryWriter(SummaryWriter_path,comment=\"ADE_FDE_Train\")\n",
    "\n",
    "# Make image folder to save outputs\n",
    "image_path  = run_folder + \"/Visual_Prediction\"\n",
    "os.makedirs(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71b993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataBase Variables\n",
    "image_folder_path       = '/notebook_data/data/data_trajpred/'+dataset_name\n",
    "DB_PATH_train     = '/notebook_data/data/data_trajpred/'+dataset_name+\"/pos_data_train.db\"\n",
    "cnx_train         = sqlite3.connect(DB_PATH_train)\n",
    "DB_PATH_val     = '/notebook_data/data/data_trajpred/'+dataset_name+\"/pos_data_val.db\"\n",
    "cnx_val         = sqlite3.connect(DB_PATH_val)\n",
    "DB_DIR      = run_folder + '/database'\n",
    "os.makedirs( DB_DIR )\n",
    "DB_PATH2    = DB_DIR+'/db_one_ped_delta_coordinates_results.db'\n",
    "cnx2        = sqlite3.connect(DB_PATH2)\n",
    "\n",
    "#Other variables\n",
    "T_obs                   = 8\n",
    "T_pred                  = 12\n",
    "T_total                 = T_obs + T_pred #8+12=20\n",
    "data_id                 = 0 \n",
    "batch_size              = 4 #10#100 #15 #2\n",
    "chunk_size              = batch_size * T_total # Chunksize should be multiple of T_total\n",
    "in_size                 = 2\n",
    "stochastic_out_size     = in_size * 2\n",
    "hidden_size             = 256 #!64\n",
    "embed_size              = 64 #16 #!64\n",
    "global dropout_val\n",
    "dropout_val             = 0.2 #0.5\n",
    "teacher_forcing_ratio   = 0.7 # 0.9\n",
    "regularization_factor   = 0.5 # 0.001\n",
    "avg_n_path_eval         = 20\n",
    "bst_n_path_eval         = 20\n",
    "path_mode               = \"top5\" #\"avg\",\"bst\",\"single\",\"top5\"\n",
    "regularization_mode     = \"regular\" #\"weighted\",\"e_weighted\", \"regular\"\n",
    "startpoint_mode         = \"on\" #\"on\",\"off\"\n",
    "enc_out                 = \"on\" #\"on\",\"off\"\n",
    "biased_loss_mode        = 0 # 0 , 1\n",
    "\n",
    "\n",
    "table_out   = \"results_delta\"\n",
    "table       = \"dataset_T_length_20delta_coordinates\" #\"dataset_T_length_\"+str(T_total)+\"delta_coordinates\"\n",
    "df_id       = pd.read_sql_query(\"SELECT data_id FROM \"+table, cnx_train)\n",
    "data_size   = df_id.data_id.max() * T_total\n",
    "epoch_num   = 100\n",
    "from_epoch  = 0\n",
    "\n",
    "#Visual Variables\n",
    "image_size              = 256  \n",
    "image_dimension         = 3\n",
    "mask_size               = 16\n",
    "visual_features_size    = 128 \n",
    "visual_embed_size       = 64  #128 #256 #64\n",
    "vsn_module_out_size    = 256\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "\n",
    "#Model Path\n",
    "model_path = run_folder + \"/NNmodel\" \n",
    "os.makedirs(model_path)   \n",
    "model_path = model_path + str(\"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d938d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_mode         = 1 #output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c822952d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b87ee99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Matrice Standard\n",
    "if dataset_name == 'eth':  \n",
    "    h = np.array([[2.8128700e-02, 2.0091900e-03, -4.6693600e+00],[8.0625700e-04, 2.5195500e-02, -5.0608800e+00],[ 3.4555400e-04, 9.2512200e-05, 4.6255300e-01]]) #h pour eth\n",
    "if dataset_name =='hotel':        \n",
    "    h = np.array([[1.1048200e-02, 6.6958900e-04, -3.3295300e+00],[-1.5966000e-03, 1.1632400e-02, -5.3951400e+00],[1.1190700e-04, 1.3617400e-05, 5.4276600e-01]]) #h pour hotel\n",
    "if dataset_name =='zara01': \n",
    "    h = np.array([[0.02174104, 0, -0.15],[0, -0.02461883, 13.77429807],[0, 0, 1]]) #h pour zara1\n",
    "if dataset_name =='zara02': \n",
    "    h = np.array([[0.02174104, 0, -0.4],[0, -0.02386598, 14.98401686],[0, 0, 1]]) #h pour zara2\n",
    "if dataset_name =='univ':\n",
    "    h = np.array([[0.02220407, 0, -0.48],[0, -0.02477289, 13.92551292],[0, 0, 1]]) #h pour univ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29766a",
   "metadata": {},
   "source": [
    "# Classe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae2f2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.TrajectoryDataset import TrajectoryPredictionDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771d406",
   "metadata": {},
   "source": [
    "# Initialize random weights for NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1b23f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c232eb",
   "metadata": {},
   "source": [
    "# Regularizer loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "974cccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_sigma_distance  = torch.zeros(1) #tensor[(0.)]\n",
    "\n",
    "def distance_from_line_regularizer(input_tensor,prediction):\n",
    "    global sum_sigma_distance\n",
    "    #the regularization is defined as the sum of Euclidean distances between each step of the predicted trajectory Tf , and a line fitted to the observed trajectory To.\n",
    "    # Fit a line to observation points over batch \n",
    "    input_tensor    = input_tensor.double()\n",
    "    prediction      = prediction.double()\n",
    "    input_tensor    = input_tensor.cumsum(dim=1).double()\n",
    "    X               = torch.ones_like(input_tensor).double().to('cuda', non_blocking=True)\n",
    "    X[:,:,0]        = input_tensor[:,:,0]\n",
    "    Y               = (input_tensor[:,:,1]).unsqueeze(-1).double()\n",
    "    try:\n",
    "        try:\n",
    "            XTX_1           = torch.matmul( X.transpose(-1,-2), X).double().inverse()\n",
    "        except:\n",
    "            XTX_1           = torch.matmul( X.transpose(-1,-2), X).double().pinverse()\n",
    "        XTY             = torch.matmul( X.transpose(-1,-2), Y)\n",
    "        theta           = torch.matmul( XTX_1.double(), XTY.double())\n",
    "        # Calculate real values of prediction instead of delta\n",
    "        prediction[:,:,0] = prediction[:,:,0] + input_tensor[:,-1,0].unsqueeze(-1) \n",
    "        prediction[:,:,1] = prediction[:,:,1] + input_tensor[:,-1,1].unsqueeze(-1)\n",
    "        \n",
    "        # Calculate distance ( predicted_points , observation_fitted_line ) over batch\n",
    "        theta0x0        = theta[:,0,:].double() * prediction[:,:,0].double()\n",
    "        denominator     = torch.sqrt( theta[:,0,:].double() * theta[:,0,:].double() + 1 )\n",
    "        nominator       = theta0x0 + theta[:,1,:] - prediction[:,:,1].double()\n",
    "        distance        = nominator.abs() / denominator\n",
    "        if regularization_mode =='weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().float(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(-1,T_pred)\n",
    "            weighted_distance   = weight * distance\n",
    "\n",
    "        elif regularization_mode =='e_weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().float(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(distance.size(0),T_pred)\n",
    "            weight              = torch.exp(weight)\n",
    "            weighted_distance   = weight*distance\n",
    "\n",
    "        else:\n",
    "            weighted_distance = distance\n",
    "        sigma_distance  = torch.mean(weighted_distance,1)\n",
    "        sum_sigma_distance  = torch.mean(sigma_distance)\n",
    "        return sum_sigma_distance\n",
    "    except:\n",
    "        print(\"SINGULAR VALUE\")\n",
    "        sum_sigma_distance = torch.zeros(1).to('cuda', non_blocking=True) + 20\n",
    "        return sum_sigma_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a4d6b",
   "metadata": {},
   "source": [
    "# Encoder Transformer (Cross-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a109d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderSelfAttentionNew(nn.Module):\n",
    "#     def __init__(self,device,d_model, d_k, d_v, n_head, dff=2048, dropout_transformer=.1, n_module=6):\n",
    "#         super(EncoderSelfAttentionNew, self).__init__()\n",
    "        \n",
    "        \n",
    "#         self.encoder = nn.ModuleList([MultiHeadAttention(d_model, d_k, d_v, n_head, dff, dropout_transformer)\n",
    "#                                       for _ in range(n_module)])\n",
    "#         self.device = device\n",
    "#     def forward(self, x):\n",
    "    \n",
    "        \n",
    "        \n",
    "#         for l in self.encoder:\n",
    "#             in_encoder = l(in_encoder, in_encoder, in_encoder)\n",
    "#         return in_encoder\n",
    "    \n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Layer du papier \"Multimodal Transformer\", cf fig 2 du papier\n",
    "    \"\"\"\n",
    "    def __init__(self,device,d_model, d_k, d_v, n_head, code_size=1024, dff=2048, dropout_transformer=.1, n_module=3):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = ScaledDotProductAttention(d_model, d_k, d_v, n_head)\n",
    "        \n",
    "        #Celle du haut dans le papier\n",
    "        self.cross_attention_features = ScaledDotProductAttention(d_model, d_k, d_v, n_head)\n",
    "        #Celle du bas dans le papier\n",
    "        self.cross_attention_coords = ScaledDotProductAttention(d_model, d_k, d_v, n_head)\n",
    "        \n",
    "        self.ffn_coords = nn.Linear(d_model, code_size)\n",
    "        self.ffn_features = nn.Linear(d_model, code_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_transformer)\n",
    "        self.layer_norm = nn.LayerNorm(code_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, in_encoder_coords, in_encoder_features):\n",
    "        \"\"\"\n",
    "        Encoder Layer return 2 output\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        coords_self_att = self.self_attention(in_encoder_coords,in_encoder_coords,in_encoder_coords)\n",
    "        \n",
    "        cross_attention_features = self.cross_attention_features(in_encoder_features,coords_self_att,coords_self_att)\n",
    "        \n",
    "        cross_attention_coords = self.cross_attention_coords(coords_self_att,in_encoder_features,in_encoder_features)\n",
    "        \n",
    "        out_coords = self.relu(self.ffn_coords(cross_attention_coords))\n",
    "        \n",
    "        out_features = self.relu(self.ffn_features(cross_attention_features))\n",
    "        \n",
    "        out_coords = self.dropout(out_coords)\n",
    "        out_features = self.dropout(out_features) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.layer_norm(out_coords), self.layer_norm(out_features)\n",
    "        \n",
    "        \n",
    "class EncoderTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Transformer adapted from the MTN Paper.\n",
    "    It only takes as input the (x,y) coordinates as in our case the camera is static (the MTN was used for a dynamic \"ego-car\" context)\n",
    "    \"\"\"\n",
    "    def __init__(self,device,code_size,d_k, d_v,n_head=8,n_module=3,ff_size=2048,dropout1d=0.5, feature_size=512):\n",
    "        super(EncoderTransformer, self).__init__()\n",
    "        self.device = device\n",
    "        self.fc_coords = nn.Linear(2,code_size)\n",
    "        self.fc_features = nn.Linear(feature_size,code_size)\n",
    "        \n",
    "        self.encoder = nn.ModuleList([EncoderLayer(device,code_size, d_k, d_v, n_head, dff=ff_size, dropout_transformer=dropout1d, code_size=code_size)\n",
    "                                      for _ in range(n_module)])\n",
    "        \n",
    "        self.relu =  nn.ReLU()\n",
    "        \n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((8,code_size)) #final pooling\n",
    "        self.dropout = nn.Dropout(dropout1d)\n",
    "        \n",
    "    def forward(self, coords, features):\n",
    "        \"\"\"\n",
    "        coords : sequence of coordinates\n",
    "        features : sequence of features extracted from a ResNet\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = coords.shape[0]\n",
    "        \n",
    "        coords = self.relu(self.fc_coords(coords))\n",
    "        \n",
    "        in_encoder_coords = coords + sinusoid_encoding_table(coords.shape[1], coords.shape[2]).expand(coords.shape).to(self.device)\n",
    "        \n",
    "#         print(f\"Shape of features{features.size()}\")\n",
    "#         print(f\"Shape of features{self.fc_features(features).size()}\")\n",
    "        \n",
    "        in_encoder_features = self.relu(self.fc_features(features))\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            out_enc_coords, out_enc_features = layer(in_encoder_coords, in_encoder_features)\n",
    "        #out_enc_coords, out_enc_features = self.encoder(in_encoder_coords, in_encoder_features)\n",
    "        \n",
    "        code = (out_enc_coords+out_enc_features).to(device).double()\n",
    "        \n",
    "        return code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46a0ae",
   "metadata": {},
   "source": [
    "# Resnet_Transformer_Vision Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8404554b",
   "metadata": {},
   "source": [
    "### Spatial features extraction (Resnet 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "037ff97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet(pretrain=True,layers_to_unfreeze=8,layers_to_delete=2,in_planes=3):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        pretrain: Define if we load a pretrained model from ImageNet\n",
    "        layers_to_unfreeze: Define the number of layers that we want to train at the end of the Resnet\n",
    "        layers_to_delete: Define the numbers of layers that we want to delete\n",
    "        in_planes: Define the numbers of input channels of images (supported values: 1,2 or 3)\n",
    "    return: The Resnet model\n",
    "    \"\"\"\n",
    "    resnet = torchvision.models.resnet18(pretrained=pretrain)\n",
    "    # Create a new model cause we don't want the pooling operation at the end and the classifier\n",
    "    model = nn.Sequential()\n",
    "    number_of_layers = len(list(resnet.children())) - layers_to_delete # In practice it remove the pooling operation and the classifier\n",
    "\n",
    "    if number_of_layers<layers_to_unfreeze:\n",
    "        layers_to_unfreeze = number_of_layers\n",
    "    layers_to_freeze = number_of_layers - layers_to_unfreeze\n",
    "    i=0\n",
    "    for child in resnet.children():\n",
    "        # For the first layers we create a new weight if in_planes is not 3 cause ResNet is pretrain on image with 3 channels there is no version for 1 channel\n",
    "        if i==0 and in_planes<3:\n",
    "            if i<layers_to_freeze: # Define if we freeze this layer or no\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False # Freeze the layers by passing requires_grad attribute to False\n",
    "            w = child._parameters['weight'].data # Get the weight for 3 channels data\n",
    "            child._modules['0'] = nn.Conv2d(in_planes, 64, kernel_size=3, padding=1) # Define the new conv layer\n",
    "            if in_planes == 1:\n",
    "                child._parameters['weight'].data = w.mean(dim=1, keepdim=True) # If the number of channels is 1 we made the mean of channels to set the new weight\n",
    "            else:\n",
    "                child._parameters['weight'].data = w[:, :-1] * 1.5\n",
    "\n",
    "        if i<layers_to_freeze: # Define if we freeze this layer or no\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False # Freeze the layers by passing requires_grad attribute to False\n",
    "        if i<number_of_layers: # To define if we keep this layer or not\n",
    "            model.append(child) \n",
    "        i+=1\n",
    "    return model\n",
    "\n",
    "\n",
    "class features_extraction(nn.Module):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    conv_model: The convolution model used before capsules for the moment only ResNet is supported\n",
    "    in_planes: Numbers of channels for the image\n",
    "    \"\"\"\n",
    "    def __init__(self,conv_model,in_planes: int):\n",
    "        super().__init__()\n",
    "        self.conv_model = conv_model\n",
    "        self.in_planes = in_planes\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "    def forward(self,input):\n",
    "        shape = input.size()\n",
    "        x = input.view(-1,self.in_planes,shape[-2],shape[-1])\n",
    "        x = self.conv_model(x)\n",
    "        x = self.pooling(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b6e3b",
   "metadata": {},
   "source": [
    "### Encoder Transformer (Vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37e868f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_embedding(input, d_model):\n",
    "    input = input.view(-1, 1)\n",
    "    dim = torch.arange(d_model // 2, dtype=torch.float32, device=input.device).view(1, -1)\n",
    "    sin = torch.sin(input / 10000 ** (2 * dim / d_model))\n",
    "    cos = torch.cos(input / 10000 ** (2 * dim / d_model))\n",
    "\n",
    "    out = torch.zeros((input.shape[0], d_model), device=input.device)\n",
    "    out[:, ::2] = sin\n",
    "    out[:, 1::2] = cos\n",
    "    return out\n",
    "\n",
    "def sinusoid_encoding_table(max_len, d_model):\n",
    "    pos = torch.arange(max_len, dtype=torch.float32)\n",
    "    out = position_embedding(pos, d_model)\n",
    "    return out\n",
    "\n",
    "#Scaled dot-product attention\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h):\n",
    "        \"\"\"\n",
    "        param:\n",
    "        d_model: Output dimensionality of the model\n",
    "        d_k: Dimensionality of queries and keys\n",
    "        d_v: Dimensionality of values\n",
    "        h: Number of heads\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "        self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.init_weights(gain=1.0)\n",
    "\n",
    "    def init_weights(self, gain=1.0):\n",
    "        nn.init.xavier_normal_(self.fc_q.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_k.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_v.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_o.weight, gain=gain)\n",
    "        nn.init.constant_(self.fc_q.bias, 0)\n",
    "        nn.init.constant_(self.fc_k.bias, 0)\n",
    "        nn.init.constant_(self.fc_v.bias, 0)\n",
    "        nn.init.constant_(self.fc_o.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_model)\n",
    "        :param keys: Keys (b_s, nk, d_model)\n",
    "        :param values: Values (b_s, nk, d_model)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "\n",
    "        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "\n",
    "        att = torch.softmax(att, -1)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "        return out\n",
    "\n",
    "    \n",
    "#Multi-head attention layer with Dropout and Layer Normalization\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h, dff=2048, dropout=.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(d_model=d_model, d_k=d_k, d_v=d_v, h=h)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Sequential(*[nn.Linear(d_model, dff), nn.ReLU(inplace=False), nn.Dropout(p=dropout),nn.Linear(dff, d_model)])\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        att = self.attention(queries, keys, values)\n",
    "        att = self.dropout(att)\n",
    "        att = self.fc(att)\n",
    "        att = self.dropout(att)\n",
    "        return self.layer_norm(queries + att)\n",
    "\n",
    "class EncoderSelfAttention(nn.Module):\n",
    "    def __init__(self,device,d_model, d_k, d_v, n_head, dff=2048, dropout_transformer=.1, n_module=6):\n",
    "        super(EncoderSelfAttention, self).__init__()\n",
    "        self.encoder = nn.ModuleList([MultiHeadAttention(d_model, d_k, d_v, n_head, dff, dropout_transformer)\n",
    "                                      for _ in range(n_module)])\n",
    "        self.device = device\n",
    "    def forward(self, x):\n",
    "        in_encoder = x + sinusoid_encoding_table(x.shape[1], x.shape[2]).expand(x.shape).to(self.device)\n",
    "        for l in self.encoder:\n",
    "            in_encoder = l(in_encoder, in_encoder, in_encoder)\n",
    "        return in_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc7e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "266dafd0",
   "metadata": {},
   "source": [
    "### Resnet + Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d8bddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _GestureTransformer(nn.Module):\n",
    "    \"\"\"Multi-Modal model on 3 or 1 channel\"\"\"\n",
    "    def __init__(self,device,backbone=\"resnet\",in_planes=3,pretrained= True,input_dim=512,layers_to_unfreeze=8,layers_to_delete=2,n_head=8,n_module=6,ff_size=1024,dropout1d=0.5):\n",
    "        super(_GestureTransformer, self).__init__()\n",
    "\n",
    "        self.in_planes = in_planes\n",
    "        self.device = device\n",
    "        self.conv_name = backbone\n",
    "        self.conv_model = None\n",
    "        \n",
    "        if self.conv_name.lower()==\"resnet\":\n",
    "            self.conv_model = Resnet(pretrained,layers_to_unfreeze,layers_to_delete,in_planes)\n",
    "        else:\n",
    "            raise NotImplementedError(\"The model {} is not supported!\".format(self.conv_name))\n",
    "        self.conv_model.to(device)\n",
    "        self.features = features_extraction(self.conv_model,in_planes)\n",
    "\n",
    "        self.self_attention = EncoderSelfAttention(device,input_dim,64,64,n_head=n_head,dff=ff_size,dropout_transformer=dropout1d,n_module=n_module)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8,input_dim)) #final pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        x = self.features(x)\n",
    "        x = x.view(shape[0],shape[1],-1)\n",
    "        x = self.self_attention(x)\n",
    "\n",
    "        x = self.pool(x).squeeze(dim=1) #final pooling\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058102f7",
   "metadata": {},
   "source": [
    "## Création du modèle en entier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4442ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqCA(nn.Module):\n",
    "    \"\"\"\n",
    "    SEQ2SEQ MODEL USING THE CROSS ATTENTION MECANISM TO ENCODE BOTH COORDS AND RESNET FEATURES AT THE SAME TIME\n",
    "    \"\"\"\n",
    "    def __init__(self,device, embed_size, code_size=512, target_size = 12, dropout_val=dropout_val, batch_size=1):\n",
    "        super(Seq2SeqCA, self).__init__()\n",
    "        \n",
    "        self.feature_size = 512\n",
    "        torch.cuda.empty_cache()\n",
    "        self.encoder = EncoderTransformer(device,code_size,64,64,dropout1d=dropout_val) #EncoderTransformer\n",
    "        #self.encoder.apply(init_weights)\n",
    "        #self.encoder = CoordinatesTransformer(device,code_size,64,64,dropout1d=dropout_val, batch_size=batch_size) #EncoderTransformer\n",
    "        \n",
    "        \n",
    "        self.decoder = TransformerDecoder(target_size, embed_dim=code_size, seq_len=12, num_layers=2, expansion_factor=4, n_heads=8) \n",
    "        #self.decoder.apply(init_weights)\n",
    "        \n",
    "#         self.vsn_module = _GestureTransformer(device,input_dim=code_size,dropout1d=dropout_val) #_GestureTransformer(partie vision)                   \n",
    "#         self.vsn_module.apply(init_weights)\n",
    "\n",
    "        self.feature_size = 512\n",
    "        self.features_ex = features_extraction(conv_model,in_planes=3)\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool1d((code_size)) # add a pooling (to have the same shape)\n",
    "        \n",
    "        self.code_pooling = nn.AdaptiveAvgPool2d((target_size,code_size))\n",
    "        \n",
    "        if device.type=='cuda':\n",
    "            self.encoder.cuda()\n",
    "            self.decoder.cuda()  \n",
    "            \n",
    "    def forward(self,input_tensor, target_tensor, visual_input_tensor):\n",
    "        batch_size      = int(input_tensor.size(0))\n",
    "\n",
    "        features = self.features_ex(visual_input_tensor)\n",
    "\n",
    "        features = features.view((batch_size,8,-1)) #(bs, 8, 512)\n",
    "\n",
    "        encoder_output =  self.encoder(input_tensor, features) #(bs,8,code_size)\n",
    "\n",
    "        #start_point\n",
    "#         start_point     = (input_tensor[:,0,:]).to(device).clone().detach()\n",
    "        \n",
    "#         if startpoint_mode==\"on\":\n",
    "#             input_tensor[:,0,:]    = 0\n",
    "            \n",
    "#         visual_initial_vsn          = self.vsn_module(visual_input_tensor)\n",
    "#         visual_initial_vsn          = self.pooling(visual_initial_vsn) #pooling qu'on a ajouté\n",
    "        \n",
    "#         print(\"#######\")\n",
    "#         print(f\"encoder_output size : {encoder_output.size()}\")\n",
    "#         print(f\"visual_initial_vsn size : {visual_initial_vsn.size()}\")\n",
    "#         print(\"#######\")\n",
    "        \n",
    "        trg_mask = self.make_trg_mask(target_tensor)\n",
    "        #print(f\"target_tensor : {target_tensor.size()}\")\n",
    "        \n",
    "        code_seq_12 = self.code_pooling(encoder_output)\n",
    "        decoder_output = self.decoder(target_tensor,code_seq_12,trg_mask)\n",
    "\n",
    "        return decoder_output\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg: target sequence\n",
    "        Returns:\n",
    "            trg_mask: target mask\n",
    "        \"\"\"\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = 12\n",
    "        #batch_size, trg_len = trg.shape\n",
    "        # returns the lower triangular part of matrix filled with ones\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            batch_size, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092aeaf",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5560a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing train dataset\n",
      "Batch_size : 4\n"
     ]
    }
   ],
   "source": [
    "#train dataset and loader\n",
    "\n",
    "batch_size = 4\n",
    "print(\"Initializing train dataset\")\n",
    "print(\"Batch_size : {}\".format(batch_size))\n",
    "dataset_train = TrajectoryPredictionDataset(image_folder_path, cnx_train)\n",
    "train_loader  = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True, pin_memory=True)\n",
    "validation_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c64c2300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 512, 1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = dataset_train[0][2]\n",
    "conv_model = Resnet()\n",
    "featur_ex = features_extraction(conv_model,in_planes=3)\n",
    "featur_ex(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c87cc8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqCA(device,embed_size=512,code_size=512,dropout_val=dropout_val,batch_size=batch_size)\n",
    "model = model.to(device).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e656564",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_step               = 40\n",
    "initial_learning_rate       = 0.01\n",
    "clip                        = 1\n",
    "\n",
    "# MSE loss\n",
    "criterion                   = nn.MSELoss(reduction='mean')\n",
    "criterion_vision            = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# SGD optimizer\n",
    "optimizer                   = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9, weight_decay=0.01) #SGD\n",
    "scheduler                   = torch.optim.lr_scheduler.StepLR(optimizer, step_size=learning_step, gamma=0.1)\n",
    "five_fold_cross_validation  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16eb4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    torch.save(state, os.path.join(save_path,filename))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(save_path,filename), os.path.join(save_path,'model_best.pth'))\n",
    "        \n",
    "def train(model, optimizer, scheduler, criterion, criterion_vision, clip,train_loader, validation_loader, save_path=None):\n",
    "    global batch_size\n",
    "    i               = None\n",
    "    checked_frame   = 0\n",
    "\n",
    "    losses = []\n",
    "    print(\"Data Size \",data_size,\"\\tChunk Size \",chunk_size)\n",
    "    global teacher_forcing_ratio\n",
    "    counter =0\n",
    "    best_val = float(\"inf\")\n",
    "    for j in range(epoch_num):\n",
    "        model.train()\n",
    "        epoch_loss=0\n",
    "        if j%7 == 6:\n",
    "            teacher_forcing_ratio = (teacher_forcing_ratio - 0.2) if teacher_forcing_ratio>=0.1 else 0.0\n",
    "\n",
    "        # Update TeachForce ratio to gradually change during training\n",
    "        # global teacher_forcing_ratio\n",
    "        # teacher_forcing_ratio-= 1/epoch_num\n",
    "        print(\"TEACHER FORCE RATIO\\t\",teacher_forcing_ratio)\n",
    "        #print(\"Learning Rate\\t\", scheduler.get_last_lr())\n",
    "\n",
    "        \n",
    "        if(j>=from_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            start_time = time.time()\n",
    "            ADE = 0\n",
    "            FDE = 0\n",
    "            i   = 0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                #print(\"\\n--------------- Batch %d/ %d ---------------\"%(j,i)) #(epoch/i)\n",
    "                # Forward\n",
    "                obs, pred, visual_obs, frame_tensor = data\n",
    "                \n",
    "                input_tensor, output_tensor         = obs.double().squeeze(dim=1).to('cuda', non_blocking=True), pred.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "                    \n",
    "                visual_input_tensor                 = visual_obs.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "\n",
    "                prediction = model(input_tensor, output_tensor, visual_input_tensor)\n",
    "                \n",
    "                calculated_prediction = prediction.cumsum(axis=1) #calculated_prediction\n",
    "                \n",
    "                loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction) #loss (regularisation term Lreg)\n",
    "                \n",
    "                if biased_loss_mode:\n",
    "                    weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "                    weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "                    loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "                else:\n",
    "                    loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2)) #mean squared error (lmse)\n",
    "                    \n",
    "                out_x       = output_tensor[:,:,0].cumsum(axis=1)\n",
    "                out_y       = output_tensor[:,:,1].cumsum(axis=1)\n",
    "                pred_x      = calculated_prediction[:,:,0]\n",
    "                pred_y      = calculated_prediction[:,:,1]\n",
    "                ADE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "                # FDE      += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "                \n",
    "                # Backward Propagation\n",
    "                total_loss      = loss.double() + torch.tensor(regularization_factor).to('cuda', non_blocking=True) * loss_line_regularizer.double() #total loss\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "                #print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "                epoch_loss += total_loss.item()\n",
    "                #print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                torch.cuda.empty_cache()\n",
    "                writer.close()\n",
    "                count_div=i\n",
    "            \n",
    "            # tensorboard log\n",
    "            writer.add_scalar('ADE/train', ADE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('FDE/train', FDE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('LOSS/train', epoch_loss/(count_div+1), counter)\n",
    "            counter += 1\n",
    "\n",
    "        if scheduler.get_last_lr()[0]>0.001:\n",
    "            scheduler.step()\n",
    "        # validation(model, optimizer, criterion, criterion_vision, clip, validation_loader, j) \n",
    "        epoch_loss = epoch_loss / (int(data_size/chunk_size))\n",
    "        losses.append(epoch_loss)\n",
    "        display.clear_output(wait=True)\n",
    "        plt.plot(losses, '--ro', label='train loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'epoch {j}')\n",
    "        plt.show()\n",
    "        print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "        print(\"EPOCH \", j, \"\\tLOSS \", epoch_loss)\n",
    "        writer.add_scalar('epoch_loss/train', epoch_loss/ (int(data_size/chunk_size)), j ) #see how model performs on the training dataset\n",
    "        #torch.save( model.state_dict(), model_path+\"_current\")\n",
    "        print(\"-----------------------------------------------\\n\"+\"-----------------------------------------------\")\n",
    "\n",
    "        # save checkpoint for each epoch and a fine called best_model so far \n",
    "        print(np.argmin(losses))\n",
    "        is_best = epoch_loss < best_val\n",
    "        best_val = min(epoch_loss, best_val)\n",
    "        print(\"bestvaleur\", best_val)\n",
    "        if save_path and (j+1)%5==0:\n",
    "            save_checkpoint({'epoch': j+1,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'scheduler': scheduler.state_dict(),'best_loss': best_val}, is_best, save_path, 'epoch_{}.pth'.format(j+1))\n",
    "        \n",
    "    return epoch_loss / (int(data_size/chunk_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4b261",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time\t\t0.18 sec \n",
      "\n",
      "EPOCH  88 \tLOSS  13.417029272541614\n",
      "-----------------------------------------------\n",
      "-----------------------------------------------\n",
      "41\n",
      "bestvaleur 13.154850235428976\n",
      "TEACHER FORCE RATIO\t 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN\")\n",
    "model.train()\n",
    "print(\"path mode\\t\",path_mode)\n",
    "\n",
    "save_path = \"/notebook_data/Introvert_ResnetTransf/save_models/cross_attention/\"\n",
    "loss  = train(model, optimizer, scheduler, criterion, criterion_vision, clip, train_loader, validation_loader, save_path)\n",
    "print(\"LOSS \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8824e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
