{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f123968",
   "metadata": {},
   "source": [
    "## Dans ce notebook :\n",
    "\n",
    "on test un encoder multimodal qui prend en entrée les coordonnées \n",
    "et les features extraites d'un Resnet. On utilise le principe de Cross-Attention du papier \"Multi-modal \n",
    "Transformers\", on utilise ensuite le décoder classique (transformer aussi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "116854f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7fbfa80bd480>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader \n",
    "import torchvision\n",
    "from torchvision import datasets \n",
    "from torch.nn.functional import softmax\n",
    "from torchvision.transforms import ToTensor \n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "import datetime\n",
    "import cv2\n",
    "from skimage.util import img_as_ubyte\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import argparse\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "# Log\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#Transformer Decoder\n",
    "from models.scratch_transformer.decoder import TransformerDecoder\n",
    "from models.cross_attention_transformer.encoder import Encoder\n",
    "from models.cross_attention_transformer.encoder_layer import EncoderLayer\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fd3ca10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebook_data/github_repo\n"
     ]
    }
   ],
   "source": [
    "__file__=os.getcwd()\n",
    "print(__file__)\n",
    "\n",
    "#Dataset name\n",
    "dataset_name = \"eth\" # dataset options: 'university', 'zara_01', 'zara_02', 'eth', 'hotel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03d26b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now() # current date and time\n",
    "current_time_date = now.strftime(\"%d_%m_%y_%H_%M_%S\")\n",
    "run_folder  = \"Outputs/traj_pred_\"+ dataset_name + \"_\" + str(os.path.basename(__file__)) + str(current_time_date)\n",
    "os.makedirs(run_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "185db074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make log folder for tensorboard\n",
    "#SummaryWriter_path = run_folder + \"/log\"\n",
    "#os.makedirs(SummaryWriter_path) \n",
    "SummaryWriter_path = '/notebook_data/work_dirs/cross_attention_test_2'\n",
    "  \n",
    "writer = SummaryWriter(SummaryWriter_path,comment=\"ADE_FDE_Train\")\n",
    "\n",
    "# Make image folder to save outputs\n",
    "image_path  = run_folder + \"/Visual_Prediction\"\n",
    "os.makedirs(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71b993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataBase Variables\n",
    "image_folder_path       = '/notebook_data/data/data_trajpred/'+dataset_name\n",
    "DB_PATH_train     = '/notebook_data/data/data_trajpred/'+dataset_name+\"/pos_data_train.db\"\n",
    "cnx_train         = sqlite3.connect(DB_PATH_train)\n",
    "DB_PATH_val     = '/notebook_data/data/data_trajpred/'+dataset_name+\"/pos_data_val.db\"\n",
    "cnx_val         = sqlite3.connect(DB_PATH_val)\n",
    "DB_DIR      = run_folder + '/database'\n",
    "os.makedirs( DB_DIR )\n",
    "DB_PATH2    = DB_DIR+'/db_one_ped_delta_coordinates_results.db'\n",
    "cnx2        = sqlite3.connect(DB_PATH2)\n",
    "\n",
    "#Other variables\n",
    "T_obs                   = 8\n",
    "T_pred                  = 12\n",
    "T_total                 = T_obs + T_pred #8+12=20\n",
    "data_id                 = 0 \n",
    "batch_size              = 4 #10#100 #15 #2\n",
    "chunk_size              = batch_size * T_total # Chunksize should be multiple of T_total\n",
    "in_size                 = 2\n",
    "stochastic_out_size     = in_size * 2\n",
    "hidden_size             = 256 #!64\n",
    "embed_size              = 64 #16 #!64\n",
    "global dropout_val\n",
    "dropout_val             = 0.2 #0.5\n",
    "teacher_forcing_ratio   = 0.7 # 0.9\n",
    "regularization_factor   = 0.5 # 0.001\n",
    "avg_n_path_eval         = 20\n",
    "bst_n_path_eval         = 20\n",
    "path_mode               = \"top5\" #\"avg\",\"bst\",\"single\",\"top5\"\n",
    "regularization_mode     = \"regular\" #\"weighted\",\"e_weighted\", \"regular\"\n",
    "startpoint_mode         = \"on\" #\"on\",\"off\"\n",
    "enc_out                 = \"on\" #\"on\",\"off\"\n",
    "biased_loss_mode        = 0 # 0 , 1\n",
    "\n",
    "\n",
    "table_out   = \"results_delta\"\n",
    "table       = \"dataset_T_length_20delta_coordinates\" #\"dataset_T_length_\"+str(T_total)+\"delta_coordinates\"\n",
    "df_id       = pd.read_sql_query(\"SELECT data_id FROM \"+table, cnx_train)\n",
    "data_size   = df_id.data_id.max() * T_total\n",
    "epoch_num   = 100\n",
    "from_epoch  = 0\n",
    "\n",
    "#Visual Variables\n",
    "image_size              = 256  \n",
    "image_dimension         = 3\n",
    "mask_size               = 16\n",
    "visual_features_size    = 128 \n",
    "visual_embed_size       = 64  #128 #256 #64\n",
    "vsn_module_out_size    = 256\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "\n",
    "#Model Path\n",
    "model_path = run_folder + \"/NNmodel\" \n",
    "os.makedirs(model_path)   \n",
    "model_path = model_path + str(\"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d938d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_mode         = 1 #output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c822952d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b87ee99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Matrice Standard\n",
    "if dataset_name == 'eth':  \n",
    "    h = np.array([[2.8128700e-02, 2.0091900e-03, -4.6693600e+00],[8.0625700e-04, 2.5195500e-02, -5.0608800e+00],[ 3.4555400e-04, 9.2512200e-05, 4.6255300e-01]]) #h pour eth\n",
    "if dataset_name =='hotel':        \n",
    "    h = np.array([[1.1048200e-02, 6.6958900e-04, -3.3295300e+00],[-1.5966000e-03, 1.1632400e-02, -5.3951400e+00],[1.1190700e-04, 1.3617400e-05, 5.4276600e-01]]) #h pour hotel\n",
    "if dataset_name =='zara01': \n",
    "    h = np.array([[0.02174104, 0, -0.15],[0, -0.02461883, 13.77429807],[0, 0, 1]]) #h pour zara1\n",
    "if dataset_name =='zara02': \n",
    "    h = np.array([[0.02174104, 0, -0.4],[0, -0.02386598, 14.98401686],[0, 0, 1]]) #h pour zara2\n",
    "if dataset_name =='univ':\n",
    "    h = np.array([[0.02220407, 0, -0.48],[0, -0.02477289, 13.92551292],[0, 0, 1]]) #h pour univ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29766a",
   "metadata": {},
   "source": [
    "# Classe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ae2f2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Other variables\n",
    "T_obs = 8\n",
    "T_pred = 12\n",
    "T_total = T_obs + T_pred  # 8+12=20\n",
    "image_size = 256\n",
    "in_size = 2\n",
    "# \"dataset_T_length_\"+str(T_total)+\"delta_coordinates\"\n",
    "table = \"dataset_T_length_20delta_coordinates\"\n",
    "\n",
    "\n",
    "class TrajectoryPredictionDataset(torch.utils.data.Dataset):\n",
    "    # Enc.cinématique reçoit la trajectoire observée de humain cible (input) de la forme T=(u1,u2-u1,u3-u2,..) qui consiste en les coordonnées de la position de départ et en les déplacements relatifs de l'humain entre les images consécutives.\n",
    "    # Ce format a été choisi car il permet au modèle de mieux capturer les similarités entre des trajectoires presque identiques qui peuvent avoir des points de départ différents.\n",
    "    def __init__(self, ROOT_DIR, cnx, conv_model = None, load_features = None, return_image = False):\n",
    "\n",
    "        self.return_image = return_image\n",
    "        self.pos_df = pd.read_sql_query(\"SELECT * FROM \"+str(table), cnx)\n",
    "        self.root_dir = ROOT_DIR+'/visual_data'\n",
    "        self.transform = torchvision.transforms.Compose([torchvision.transforms.Resize((image_size, image_size)),\n",
    "                                                         torchvision.transforms.ToTensor(),\n",
    "                                                         torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "        self.visual_data = []\n",
    "        # read sorted frames\n",
    "        for img in sorted(os.listdir(self.root_dir)):\n",
    "            self.visual_data.append(self.transform(\n",
    "                Image.open(os.path.join(self.root_dir)+\"/\"+img)))\n",
    "        self.visual_data = torch.stack(self.visual_data)\n",
    "        \n",
    "        \n",
    "        self.return_features = False\n",
    "        if conv_model:\n",
    "            self.return_features = True\n",
    "            self.feature_extractor = features_extraction(conv_model, in_planes=3)\n",
    "            if load_features:\n",
    "                print(\"Loading features from file\")\n",
    "                self.features = np.load(load_features)\n",
    "                self.features = torch.from_numpy(self.features)\n",
    "            else :\n",
    "                self.features = self.extract_features(self.feature_extractor)\n",
    "            print(self.features.size())\n",
    "            \n",
    "            \n",
    "    def extract_features(self,features_extractor):\n",
    "        print(\"Extracting features of all dataset\")\n",
    "        features = np.zeros((len(self.visual_data),8,512,1,1))\n",
    "        \n",
    "        for i in tqdm(range(len(self.visual_data)-8)):\n",
    "            \n",
    "            features[i]=features_extractor(self.visual_data[i:i+T_obs]).detach().numpy()\n",
    "            \n",
    "        #np.save('features_resnet.npy',features)    \n",
    "        print(\"End\")\n",
    "        return torch.from_numpy(features)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pos_df.data_id.max()  # data_id maximum dans dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        #print(\"idx :\", idx)\n",
    "\n",
    "        # table dont data_id=idx\n",
    "        extracted_df = self.pos_df[self.pos_df[\"data_id\"] == idx]\n",
    "\n",
    "        tensor = torch.tensor(extracted_df[['pos_x_delta', 'pos_y_delta']].values).reshape(\n",
    "            -1, T_total, in_size)  # juste pos_x_delta et pos_y_delta de extracted_df (tensor)\n",
    "        # obs de 8 et pred de 12 à partir de tensor construit\n",
    "        obs, pred = torch.split(tensor, [T_obs, T_pred], dim=1)\n",
    "\n",
    "        # extracted_df dont data_id=idx, on prend minimum frame_num et aprés on divise par 10, cela represente start_frame\n",
    "        start_frames = (extracted_df.groupby(\n",
    "            'data_id').frame_num.min().values/10).astype('int')\n",
    "        extracted_frames = []\n",
    "        extracted_features = []\n",
    "        for i in start_frames:\n",
    "            img = self.visual_data[i:i+T_obs]\n",
    "            extracted_frames.append(img)\n",
    "            if conv_model is not None:\n",
    "                extracted_features.append(self.features[i])\n",
    "\n",
    "        # stack concatenates a sequence of tensors along a new dimension.\n",
    "        frames = torch.stack(extracted_frames)\n",
    "        features_out = torch.stack(extracted_features)\n",
    "\n",
    "        start_frames = torch.tensor(start_frames)  # tensor([start_frames])\n",
    "        \n",
    "        if self.return_features:\n",
    "            \n",
    "            if self.return_image:\n",
    "                return obs, pred, frames, features_out, start_frames #Ou on peut return les frames aussi mais dataset plus lourd\n",
    "            else:\n",
    "                return obs, pred, features_out, start_frames\n",
    "        else:\n",
    "            return obs, pred, frames, start_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771d406",
   "metadata": {},
   "source": [
    "# Initialize random weights for NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b1b23f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c232eb",
   "metadata": {},
   "source": [
    "# Regularizer loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "974cccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_sigma_distance  = torch.zeros(1) #tensor[(0.)]\n",
    "\n",
    "def distance_from_line_regularizer(input_tensor,prediction):\n",
    "    global sum_sigma_distance\n",
    "    #the regularization is defined as the sum of Euclidean distances between each step of the predicted trajectory Tf , and a line fitted to the observed trajectory To.\n",
    "    # Fit a line to observation points over batch \n",
    "    input_tensor    = input_tensor.double()\n",
    "    prediction      = prediction.double()\n",
    "    input_tensor    = input_tensor.cumsum(dim=1).double()\n",
    "    X               = torch.ones_like(input_tensor).double().to('cuda', non_blocking=True)\n",
    "    X[:,:,0]        = input_tensor[:,:,0]\n",
    "    Y               = (input_tensor[:,:,1]).unsqueeze(-1).double()\n",
    "    try:\n",
    "        try:\n",
    "            XTX_1           = torch.matmul( X.transpose(-1,-2), X).double().inverse()\n",
    "        except:\n",
    "            XTX_1           = torch.matmul( X.transpose(-1,-2), X).double().pinverse()\n",
    "        XTY             = torch.matmul( X.transpose(-1,-2), Y)\n",
    "        theta           = torch.matmul( XTX_1.double(), XTY.double())\n",
    "        # Calculate real values of prediction instead of delta\n",
    "        prediction[:,:,0] = prediction[:,:,0] + input_tensor[:,-1,0].unsqueeze(-1) \n",
    "        prediction[:,:,1] = prediction[:,:,1] + input_tensor[:,-1,1].unsqueeze(-1)\n",
    "        \n",
    "        # Calculate distance ( predicted_points , observation_fitted_line ) over batch\n",
    "        theta0x0        = theta[:,0,:].double() * prediction[:,:,0].double()\n",
    "        denominator     = torch.sqrt( theta[:,0,:].double() * theta[:,0,:].double() + 1 )\n",
    "        nominator       = theta0x0 + theta[:,1,:] - prediction[:,:,1].double()\n",
    "        distance        = nominator.abs() / denominator\n",
    "        if regularization_mode =='weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().float(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(-1,T_pred)\n",
    "            weighted_distance   = weight * distance\n",
    "\n",
    "        elif regularization_mode =='e_weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().float(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(distance.size(0),T_pred)\n",
    "            weight              = torch.exp(weight)\n",
    "            weighted_distance   = weight*distance\n",
    "\n",
    "        else:\n",
    "            weighted_distance = distance\n",
    "        sigma_distance  = torch.mean(weighted_distance,1)\n",
    "        sum_sigma_distance  = torch.mean(sigma_distance)\n",
    "        return sum_sigma_distance\n",
    "    except:\n",
    "        print(\"SINGULAR VALUE\")\n",
    "        sum_sigma_distance = torch.zeros(1).to('cuda', non_blocking=True) + 20\n",
    "        return sum_sigma_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a4d6b",
   "metadata": {},
   "source": [
    "# Encoder Transformer (Cross-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3a109d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderSelfAttentionNew(nn.Module):\n",
    "#     def __init__(self,device,d_model, d_k, d_v, n_head, dff=2048, dropout_transformer=.1, n_module=6):\n",
    "#         super(EncoderSelfAttentionNew, self).__init__()\n",
    "        \n",
    "        \n",
    "#         self.encoder = nn.ModuleList([MultiHeadAttention(d_model, d_k, d_v, n_head, dff, dropout_transformer)\n",
    "#                                       for _ in range(n_module)])\n",
    "#         self.device = device\n",
    "#     def forward(self, x):\n",
    "    \n",
    "        \n",
    "        \n",
    "#         for l in self.encoder:\n",
    "#             in_encoder = l(in_encoder, in_encoder, in_encoder)\n",
    "#         return in_encoder\n",
    "    \n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Layer du papier \"Multimodal Transformer\", cf fig 2 du papier\n",
    "    \"\"\"\n",
    "    def __init__(self,device,d_model, d_k, d_v, n_head, code_size=1024, dff=2048, dropout_transformer=.1, n_module=3):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = ScaledDotProductAttention(d_model, d_k, d_v, n_head)\n",
    "        \n",
    "        #Celle du haut dans le papier\n",
    "        self.cross_attention_features = ScaledDotProductAttention(d_model, d_k, d_v, n_head)\n",
    "        #Celle du bas dans le papier\n",
    "        self.cross_attention_coords = ScaledDotProductAttention(d_model, d_k, d_v, n_head)\n",
    "        \n",
    "        self.ffn_coords = nn.Sequential(*[nn.Linear(d_model, dff), nn.ReLU(inplace=False), nn.Dropout(p=dropout_transformer),nn.Linear(dff, code_size)])\n",
    "        \n",
    "        \n",
    "        self.ffn_features = nn.Sequential(*[nn.Linear(d_model, dff), nn.ReLU(inplace=False), nn.Dropout(p=dropout_transformer),nn.Linear(dff, code_size)])\n",
    "        \n",
    "#         self.ffn_coords = nn.Linear(d_model, code_size)\n",
    "#         self.ffn_features = nn.Linear(d_model, code_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_transformer)\n",
    "        self.layer_norm = nn.LayerNorm(code_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, in_encoder_coords, in_encoder_features):\n",
    "        \"\"\"\n",
    "        Encoder Layer return 2 output\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        coords_self_att = self.self_attention(in_encoder_coords,in_encoder_coords,in_encoder_coords)\n",
    "        \n",
    "        cross_attention_features = self.cross_attention_features(in_encoder_features,coords_self_att,coords_self_att)\n",
    "        \n",
    "        cross_attention_coords = self.cross_attention_coords(coords_self_att,in_encoder_features,in_encoder_features)\n",
    "        \n",
    "        out_coords = self.relu(self.ffn_coords(cross_attention_coords))\n",
    "        \n",
    "        out_features = self.relu(self.ffn_features(cross_attention_features))\n",
    "        \n",
    "        out_coords = self.dropout(out_coords)\n",
    "        out_features = self.dropout(out_features) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.layer_norm(out_coords), self.layer_norm(out_features)\n",
    "        \n",
    "        \n",
    "class EncoderTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Transformer adapted from the MTN Paper.\n",
    "    It only takes as input the (x,y) coordinates as in our case the camera is static (the MTN was used for a dynamic \"ego-car\" context)\n",
    "    \"\"\"\n",
    "    def __init__(self,device,code_size,d_k, d_v,n_head=8,n_module=3,ff_size=2048,dropout1d=0.5, feature_size=512):\n",
    "        super(EncoderTransformer, self).__init__()\n",
    "        self.device = device\n",
    "        self.fc_coords = nn.Linear(2,code_size)\n",
    "        self.fc_features = nn.Linear(feature_size,code_size)\n",
    "        \n",
    "        self.encoder = nn.ModuleList([EncoderLayer(device,code_size, d_k, d_v, n_head, dff=ff_size, dropout_transformer=dropout1d, code_size=code_size)\n",
    "                                      for _ in range(n_module)])\n",
    "        \n",
    "        self.relu =  nn.ReLU()\n",
    "        \n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((8,code_size)) #final pooling\n",
    "        self.dropout = nn.Dropout(dropout1d)\n",
    "        \n",
    "    def forward(self, coords, features):\n",
    "        \"\"\"\n",
    "        coords : sequence of coordinates\n",
    "        features : sequence of features extracted from a ResNet\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = coords.shape[0]\n",
    "        \n",
    "        coords = self.relu(self.fc_coords(coords))\n",
    "        \n",
    "        in_encoder_coords = coords + sinusoid_encoding_table(coords.shape[1], coords.shape[2]).expand(coords.shape).to(self.device)\n",
    "        \n",
    "#         print(f\"Shape of features{features.size()}\")\n",
    "#         print(f\"Shape of features{self.fc_features(features).size()}\")\n",
    "        \n",
    "        in_encoder_features = self.relu(self.fc_features(features))\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            out_enc_coords, out_enc_features = layer(in_encoder_coords, in_encoder_features)\n",
    "        #out_enc_coords, out_enc_features = self.encoder(in_encoder_coords, in_encoder_features)\n",
    "        \n",
    "        code = (out_enc_coords+out_enc_features).to(device).double()\n",
    "        \n",
    "        return code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46a0ae",
   "metadata": {},
   "source": [
    "# Resnet_Transformer_Vision Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8404554b",
   "metadata": {},
   "source": [
    "### Spatial features extraction (Resnet 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "037ff97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet(pretrain=True,layers_to_unfreeze=8,layers_to_delete=2,in_planes=3):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        pretrain: Define if we load a pretrained model from ImageNet\n",
    "        layers_to_unfreeze: Define the number of layers that we want to train at the end of the Resnet\n",
    "        layers_to_delete: Define the numbers of layers that we want to delete\n",
    "        in_planes: Define the numbers of input channels of images (supported values: 1,2 or 3)\n",
    "    return: The Resnet model\n",
    "    \"\"\"\n",
    "    resnet = torchvision.models.resnet18(pretrained=pretrain)\n",
    "    # Create a new model cause we don't want the pooling operation at the end and the classifier\n",
    "    model = nn.Sequential()\n",
    "    number_of_layers = len(list(resnet.children())) - layers_to_delete # In practice it remove the pooling operation and the classifier\n",
    "\n",
    "    if number_of_layers<layers_to_unfreeze:\n",
    "        layers_to_unfreeze = number_of_layers\n",
    "    layers_to_freeze = number_of_layers - layers_to_unfreeze\n",
    "    i=0\n",
    "    for child in resnet.children():\n",
    "        # For the first layers we create a new weight if in_planes is not 3 cause ResNet is pretrain on image with 3 channels there is no version for 1 channel\n",
    "        if i==0 and in_planes<3:\n",
    "            if i<layers_to_freeze: # Define if we freeze this layer or no\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False # Freeze the layers by passing requires_grad attribute to False\n",
    "            w = child._parameters['weight'].data # Get the weight for 3 channels data\n",
    "            child._modules['0'] = nn.Conv2d(in_planes, 64, kernel_size=3, padding=1) # Define the new conv layer\n",
    "            if in_planes == 1:\n",
    "                child._parameters['weight'].data = w.mean(dim=1, keepdim=True) # If the number of channels is 1 we made the mean of channels to set the new weight\n",
    "            else:\n",
    "                child._parameters['weight'].data = w[:, :-1] * 1.5\n",
    "\n",
    "        if i<layers_to_freeze: # Define if we freeze this layer or no\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False # Freeze the layers by passing requires_grad attribute to False\n",
    "        if i<number_of_layers: # To define if we keep this layer or not\n",
    "            model.append(child) \n",
    "        i+=1\n",
    "    return model\n",
    "\n",
    "\n",
    "class features_extraction(nn.Module):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    conv_model: The convolution model used before capsules for the moment only ResNet is supported\n",
    "    in_planes: Numbers of channels for the image\n",
    "    \"\"\"\n",
    "    def __init__(self,conv_model,in_planes: int):\n",
    "        super().__init__()\n",
    "        self.conv_model = conv_model\n",
    "        self.in_planes = in_planes\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "    def forward(self,input):\n",
    "        shape = input.size()\n",
    "        x = input.view(-1,self.in_planes,shape[-2],shape[-1])\n",
    "        x = self.conv_model(x)\n",
    "        x = self.pooling(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b6e3b",
   "metadata": {},
   "source": [
    "### Encoder Transformer (Vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37e868f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_embedding(input, d_model):\n",
    "    input = input.view(-1, 1)\n",
    "    dim = torch.arange(d_model // 2, dtype=torch.float32, device=input.device).view(1, -1)\n",
    "    sin = torch.sin(input / 10000 ** (2 * dim / d_model))\n",
    "    cos = torch.cos(input / 10000 ** (2 * dim / d_model))\n",
    "\n",
    "    out = torch.zeros((input.shape[0], d_model), device=input.device)\n",
    "    out[:, ::2] = sin\n",
    "    out[:, 1::2] = cos\n",
    "    return out\n",
    "\n",
    "def sinusoid_encoding_table(max_len, d_model):\n",
    "    pos = torch.arange(max_len, dtype=torch.float32)\n",
    "    out = position_embedding(pos, d_model)\n",
    "    return out\n",
    "\n",
    "#Scaled dot-product attention\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h):\n",
    "        \"\"\"\n",
    "        param:\n",
    "        d_model: Output dimensionality of the model\n",
    "        d_k: Dimensionality of queries and keys\n",
    "        d_v: Dimensionality of values\n",
    "        h: Number of heads\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "        self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.init_weights(gain=1.0)\n",
    "\n",
    "    def init_weights(self, gain=1.0):\n",
    "        nn.init.xavier_normal_(self.fc_q.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_k.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_v.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_o.weight, gain=gain)\n",
    "        nn.init.constant_(self.fc_q.bias, 0)\n",
    "        nn.init.constant_(self.fc_k.bias, 0)\n",
    "        nn.init.constant_(self.fc_v.bias, 0)\n",
    "        nn.init.constant_(self.fc_o.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_model)\n",
    "        :param keys: Keys (b_s, nk, d_model)\n",
    "        :param values: Values (b_s, nk, d_model)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "\n",
    "        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "\n",
    "        att = torch.softmax(att, -1)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "        return out\n",
    "\n",
    "    \n",
    "#Multi-head attention layer with Dropout and Layer Normalization\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h, dff=2048, dropout=.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(d_model=d_model, d_k=d_k, d_v=d_v, h=h)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Sequential(*[nn.Linear(d_model, dff), nn.ReLU(inplace=False), nn.Dropout(p=dropout),nn.Linear(dff, d_model)])\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        att = self.attention(queries, keys, values)\n",
    "        att = self.dropout(att)\n",
    "        att = self.fc(att)\n",
    "        att = self.dropout(att)\n",
    "        return self.layer_norm(queries + att)\n",
    "\n",
    "class EncoderSelfAttention(nn.Module):\n",
    "    def __init__(self,device,d_model, d_k, d_v, n_head, dff=2048, dropout_transformer=.1, n_module=6):\n",
    "        super(EncoderSelfAttention, self).__init__()\n",
    "        self.encoder = nn.ModuleList([MultiHeadAttention(d_model, d_k, d_v, n_head, dff, dropout_transformer)\n",
    "                                      for _ in range(n_module)])\n",
    "        self.device = device\n",
    "    def forward(self, x):\n",
    "        in_encoder = x + sinusoid_encoding_table(x.shape[1], x.shape[2]).expand(x.shape).to(self.device)\n",
    "        for l in self.encoder:\n",
    "            in_encoder = l(in_encoder, in_encoder, in_encoder)\n",
    "        return in_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc7e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "266dafd0",
   "metadata": {},
   "source": [
    "### Resnet + Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5d8bddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _GestureTransformer(nn.Module):\n",
    "    \"\"\"Multi-Modal model on 3 or 1 channel\"\"\"\n",
    "    def __init__(self,device,backbone=\"resnet\",in_planes=3,pretrained= True,input_dim=512,layers_to_unfreeze=8,layers_to_delete=2,n_head=8,n_module=6,ff_size=1024,dropout1d=0.5):\n",
    "        super(_GestureTransformer, self).__init__()\n",
    "\n",
    "        self.in_planes = in_planes\n",
    "        self.device = device\n",
    "        self.conv_name = backbone\n",
    "        self.conv_model = None\n",
    "        \n",
    "        if self.conv_name.lower()==\"resnet\":\n",
    "            self.conv_model = Resnet(pretrained,layers_to_unfreeze,layers_to_delete,in_planes)\n",
    "        else:\n",
    "            raise NotImplementedError(\"The model {} is not supported!\".format(self.conv_name))\n",
    "        self.conv_model.to(device)\n",
    "        self.features = features_extraction(self.conv_model,in_planes)\n",
    "\n",
    "        self.self_attention = EncoderSelfAttention(device,input_dim,64,64,n_head=n_head,dff=ff_size,dropout_transformer=dropout1d,n_module=n_module)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8,input_dim)) #final pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        x = self.features(x)\n",
    "        x = x.view(shape[0],shape[1],-1)\n",
    "        x = self.self_attention(x)\n",
    "\n",
    "        x = self.pool(x).squeeze(dim=1) #final pooling\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058102f7",
   "metadata": {},
   "source": [
    "## Création du modèle en entier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4442ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqCA(nn.Module):\n",
    "    \"\"\"\n",
    "    SEQ2SEQ MODEL USING THE CROSS ATTENTION MECANISM TO ENCODE BOTH COORDS AND RESNET FEATURES AT THE SAME TIME\n",
    "    \"\"\"\n",
    "    def __init__(self,device, embed_size, code_size=512, target_size = 12, dropout_val=dropout_val, batch_size=1, conv_model=Resnet()):\n",
    "        super(Seq2SeqCA, self).__init__()\n",
    "        \n",
    "        self.feature_size = 512\n",
    "        torch.cuda.empty_cache()\n",
    "        self.encoder = EncoderTransformer(device,code_size,64,64,dropout1d=dropout_val) #EncoderTransformer\n",
    "        #self.encoder.apply(init_weights)\n",
    "        #self.encoder = CoordinatesTransformer(device,code_size,64,64,dropout1d=dropout_val, batch_size=batch_size) #EncoderTransformer\n",
    "        \n",
    "        \n",
    "        self.decoder = TransformerDecoder(target_size, embed_dim=code_size, seq_len=12, num_layers=2, expansion_factor=4, n_heads=8) \n",
    "        #self.decoder.apply(init_weights)\n",
    "        \n",
    "#         self.vsn_module = _GestureTransformer(device,input_dim=code_size,dropout1d=dropout_val) #_GestureTransformer(partie vision)                   \n",
    "#         self.vsn_module.apply(init_weights)\n",
    "\n",
    "        self.feature_size = 512\n",
    "        self.features_ex = features_extraction(conv_model,in_planes=3)\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool1d((code_size)) # add a pooling (to have the same shape)\n",
    "        \n",
    "        self.code_pooling = nn.AdaptiveAvgPool2d((target_size,code_size))\n",
    "        \n",
    "        if device.type=='cuda':\n",
    "            self.encoder.cuda()\n",
    "            self.decoder.cuda()  \n",
    "            \n",
    "    def forward(self,input_tensor, target_tensor, features):\n",
    "        batch_size      = int(input_tensor.size(0))\n",
    "\n",
    "        #Les features ont été introduites directement dans le dataset\n",
    "        #features = self.features_ex(visual_input_tensor)\n",
    "\n",
    "        features = features.view((batch_size,8,-1)) #(bs, 8, 512)\n",
    "\n",
    "        encoder_output =  self.encoder(input_tensor, features) #(bs,8,code_size)\n",
    "\n",
    "        #start_point\n",
    "#         start_point     = (input_tensor[:,0,:]).to(device).clone().detach()\n",
    "        \n",
    "#         if startpoint_mode==\"on\":\n",
    "#             input_tensor[:,0,:]    = 0\n",
    "            \n",
    "#         visual_initial_vsn          = self.vsn_module(visual_input_tensor)\n",
    "#         visual_initial_vsn          = self.pooling(visual_initial_vsn) #pooling qu'on a ajouté\n",
    "        \n",
    "#         print(\"#######\")\n",
    "#         print(f\"encoder_output size : {encoder_output.size()}\")\n",
    "#         print(f\"visual_initial_vsn size : {visual_initial_vsn.size()}\")\n",
    "#         print(\"#######\")\n",
    "        \n",
    "        trg_mask = self.make_trg_mask(target_tensor)\n",
    "        #print(f\"target_tensor : {target_tensor.size()}\")\n",
    "        \n",
    "        code_seq_12 = self.code_pooling(encoder_output)\n",
    "        decoder_output = self.decoder(target_tensor,code_seq_12,trg_mask)\n",
    "\n",
    "        return decoder_output\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg: target sequence\n",
    "        Returns:\n",
    "            trg_mask: target mask\n",
    "        \"\"\"\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = 12\n",
    "        #batch_size, trg_len = trg.shape\n",
    "        # returns the lower triangular part of matrix filled with ones\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            batch_size, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092aeaf",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d5560a39",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing train dataset\n",
      "Batch_size : 32\n",
      "Loading features from file\n",
      "torch.Size([1298, 8, 512, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "#train dataset and loader\n",
    "\n",
    "batch_size = 32\n",
    "print(\"Initializing train dataset\")\n",
    "print(\"Batch_size : {}\".format(batch_size))\n",
    "\n",
    "conv_model = Resnet()\n",
    "\n",
    "dataset_train = TrajectoryPredictionDataset(image_folder_path, cnx_train, conv_model = conv_model, load_features = \"features_resnet.npy\")\n",
    "train_loader  = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=True, pin_memory=True)\n",
    "validation_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f9424071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1298, 8, 512, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1298, 3, 256, 256])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset_train.features.size())\n",
    "dataset_train.visual_data.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c87cc8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqCA(device,embed_size=512,code_size=512,dropout_val=dropout_val,batch_size=batch_size)\n",
    "model = model.to(device).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e656564",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_step               = 40\n",
    "initial_learning_rate       = 0.01\n",
    "clip                        = 1\n",
    "\n",
    "# MSE loss\n",
    "criterion                   = nn.MSELoss(reduction='mean')\n",
    "criterion_vision            = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# SGD optimizer\n",
    "optimizer                   = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9, weight_decay=0.01) #SGD\n",
    "scheduler                   = torch.optim.lr_scheduler.StepLR(optimizer, step_size=learning_step, gamma=0.1)\n",
    "five_fold_cross_validation  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "16eb4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    torch.save(state, os.path.join(save_path,filename))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(save_path,filename), os.path.join(save_path,'model_best.pth'))\n",
    "        \n",
    "def train(model, optimizer, scheduler, criterion, criterion_vision, clip,train_loader, validation_loader, save_path=None):\n",
    "    global batch_size\n",
    "    i               = None\n",
    "    checked_frame   = 0\n",
    "\n",
    "    losses = []\n",
    "    print(\"Data Size \",data_size,\"\\tChunk Size \",chunk_size)\n",
    "    global teacher_forcing_ratio\n",
    "    counter =0\n",
    "    best_val = float(\"inf\")\n",
    "    for j in range(epoch_num):\n",
    "        model.train()\n",
    "        epoch_loss=0\n",
    "        if j%7 == 6:\n",
    "            teacher_forcing_ratio = (teacher_forcing_ratio - 0.2) if teacher_forcing_ratio>=0.1 else 0.0\n",
    "\n",
    "        # Update TeachForce ratio to gradually change during training\n",
    "        # global teacher_forcing_ratio\n",
    "        # teacher_forcing_ratio-= 1/epoch_num\n",
    "        print(\"TEACHER FORCE RATIO\\t\",teacher_forcing_ratio)\n",
    "        #print(\"Learning Rate\\t\", scheduler.get_last_lr())\n",
    "\n",
    "        \n",
    "        if(j>=from_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            start_time = time.time()\n",
    "            ADE = 0\n",
    "            FDE = 0\n",
    "            i   = 0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                #print(\"\\n--------------- Batch %d/ %d ---------------\"%(j,i)) #(epoch/i)\n",
    "                # Forward\n",
    "                obs, pred,features, frame_tensor = data\n",
    "                \n",
    "                input_tensor, output_tensor         = obs.double().squeeze(dim=1).to('cuda', non_blocking=True), pred.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "                    \n",
    "                features                 = features.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "                \n",
    "                prediction = model(input_tensor, output_tensor, features)\n",
    "                \n",
    "                calculated_prediction = prediction.cumsum(axis=1) #calculated_prediction\n",
    "                \n",
    "                loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction) #loss (regularisation term Lreg)\n",
    "                \n",
    "                if biased_loss_mode:\n",
    "                    weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "                    weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "                    loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "                else:\n",
    "                    loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2)) #mean squared error (lmse)\n",
    "                    \n",
    "                out_x       = output_tensor[:,:,0].cumsum(axis=1)\n",
    "                out_y       = output_tensor[:,:,1].cumsum(axis=1)\n",
    "                pred_x      = calculated_prediction[:,:,0]\n",
    "                pred_y      = calculated_prediction[:,:,1]\n",
    "                ADE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "                # FDE      += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "                \n",
    "                # Backward Propagation\n",
    "                total_loss      = loss.double() + torch.tensor(regularization_factor).to('cuda', non_blocking=True) * loss_line_regularizer.double() #total loss\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "                #print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "                epoch_loss += total_loss.item()\n",
    "                #print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                torch.cuda.empty_cache()\n",
    "                writer.close()\n",
    "                count_div=i\n",
    "            \n",
    "            # tensorboard log\n",
    "            writer.add_scalar('ADE/train', ADE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('FDE/train', FDE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('LOSS/train', epoch_loss/(count_div+1), counter)\n",
    "            counter += 1\n",
    "\n",
    "        if scheduler.get_last_lr()[0]>0.001:\n",
    "            scheduler.step()\n",
    "        # validation(model, optimizer, criterion, criterion_vision, clip, validation_loader, j) \n",
    "        epoch_loss = epoch_loss / (int(data_size/chunk_size))\n",
    "        losses.append(epoch_loss)\n",
    "        display.clear_output(wait=True)\n",
    "        plt.plot(losses, '--ro', label='train loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'epoch {j}')\n",
    "        plt.show()\n",
    "        print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "        print(\"EPOCH \", j, \"\\tLOSS \", epoch_loss)\n",
    "        writer.add_scalar('epoch_loss/train', epoch_loss/ (int(data_size/chunk_size)), j ) #see how model performs on the training dataset\n",
    "        #torch.save( model.state_dict(), model_path+\"_current\")\n",
    "        print(\"-----------------------------------------------\\n\"+\"-----------------------------------------------\")\n",
    "\n",
    "        # save checkpoint for each epoch and a fine called best_model so far \n",
    "        print(np.argmin(losses))\n",
    "        is_best = epoch_loss < best_val\n",
    "        best_val = min(epoch_loss, best_val)\n",
    "        print(\"bestvaleur\", best_val)\n",
    "        if save_path and (j+1)%20==0:\n",
    "            save_checkpoint({'epoch': j+1,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'scheduler': scheduler.state_dict(),'best_loss': best_val}, is_best, save_path, 'epoch_{}.pth'.format(j+1))\n",
    "        \n",
    "    return epoch_loss / (int(data_size/chunk_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4b261",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time\t\t0.20 sec \n",
      "\n",
      "EPOCH  17 \tLOSS  1.9438120946687723\n",
      "-----------------------------------------------\n",
      "-----------------------------------------------\n",
      "16\n",
      "bestvaleur 1.7291823524836647\n",
      "TEACHER FORCE RATIO\t 0.29999999999999993\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN\")\n",
    "model.train()\n",
    "print(\"path mode\\t\",path_mode)\n",
    "\n",
    "save_path = \"/notebook_data/Introvert_ResnetTransf/save_models/cross_attention_2/\"\n",
    "loss  = train(model, optimizer, scheduler, criterion, criterion_vision, clip, train_loader, validation_loader, save_path)\n",
    "print(\"LOSS \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da083c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train.features[90] - dataset_train[11][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac5086",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, optimizer, criterion, criterion_vision, clip, validation_loader, counter):\n",
    "    global batch_size\n",
    "    model.eval()\n",
    "    i           = None\n",
    "    ADEs        = 0\n",
    "    FDEs        = 0\n",
    "    epoch_loss  = 0\n",
    "    loss_line_regularizer = 0\n",
    "    loss = 0 \n",
    "    total_loss = 0\n",
    "    ADE  = 0\n",
    "    FDE  = 0\n",
    "    for i,data in enumerate(test_loader):\n",
    "        # Forward\n",
    "        obs, pred, features, frame_tensor = data\n",
    "\n",
    "        input_tensor, output_tensor         = obs.double().squeeze(dim=1).to('cuda', non_blocking=True), pred.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "\n",
    "        features                 = features.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "\n",
    "        prediction = model(input_tensor, output_tensor, features)\n",
    "        \n",
    "        calculated_prediction = prediction.cumsum(axis=1) \n",
    "\n",
    "        loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction)\n",
    "        \n",
    "        if biased_loss_mode:\n",
    "            weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "            weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "            loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "        else:\n",
    "            loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2))\n",
    "            \n",
    "        out_x       = output_tensor[:,:,0].cumsum(axis=1)\n",
    "        out_y       = output_tensor[:,:,1].cumsum(axis=1)\n",
    "        pred_x      = calculated_prediction[:,:,0]\n",
    "        pred_y      = calculated_prediction[:,:,1]\n",
    "        ADE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "        FDE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "        total_loss  += loss.double() + regularization_factor * loss_line_regularizer.double() \n",
    "        print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "\n",
    "    writer.add_scalar('ADE/val_'+path_mode,             ADE.item()/(i+1),             counter)\n",
    "    writer.add_scalar('FDE/val_'+path_mode,             FDE.item()/(i+1),             counter)\n",
    "    writer.add_scalar('LOSS/val_'+path_mode,            total_loss.item()/(i+1)   ,   counter)\n",
    "    writer.add_scalar('LOSS_c/val_'+path_mode,          loss.item()/(i+1)        ,    counter)\n",
    "    writer.add_scalar('L-REGULARIZER/val_'+path_mode,   loss_line_regularizer.item()/(i+1), counter)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c37562f",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_eval(model, optimizer, criterion, criterion_vision, clip, five_fold_cross_validation):\n",
    "    global batch_size\n",
    "    model.eval()\n",
    "    i           = None\n",
    "    ADEs        = 0\n",
    "    FDEs        = 0\n",
    "    epoch_loss  = 0\n",
    "    list_x_obs          = ['x_obs_'+str(i)              for i in range(0,T_obs)] #x_obs_0 --> x_obs_7\n",
    "    list_y_obs          = ['y_obs_'+str(i)              for i in range(0,T_obs)] #y_obs_0 --> y_obs_7\n",
    "    list_c_context      = ['context_c_'+str(i)          for i in range(0,hidden_size)] #context_c_0 --> context_c_255\n",
    "    list_h_context      = ['context_h_'+str(i)          for i in range(0,hidden_size)] #context_h_0 --> context_h_255\n",
    "    list_x_pred         = ['x_pred_'+str(i)             for i in range(0,T_pred)] #x_pred_0 --> x_pred_11\n",
    "    list_y_pred         = ['y_pred_'+str(i)             for i in range(0,T_pred)] #y_pred_0 --> y_pred_11\n",
    "    list_x_stoch_pred_m = ['x_stoch_pred_m_'+str(i)     for i in range(0,T_pred)] #x_stoch_pred_m_0 --> x_stoch_pred_m_11\n",
    "    list_y_stoch_pred_m = ['y_stoch_pred_m_'+str(i)     for i in range(0,T_pred)] #y_stoch_pred_m_0 --> y_stoch_pred_m_11\n",
    "    list_x_stoch_pred_s = ['x_stoch_pred_s_'+str(i)     for i in range(0,T_pred)] #x_stoch_pred_s_0 --> x_stoch_pred_s_11\n",
    "    list_y_stoch_pred_s = ['y_stoch_pred_s_'+str(i)     for i in range(0,T_pred)] #y_stoch_pred_s_0 --> y_stoch_pred_s_11\n",
    "    list_x_out          = ['x_out_'+str(i)              for i in range(0,T_pred)] #x_out_0 --> x_out_11\n",
    "    list_y_out          = ['y_out_'+str(i)              for i in range(0,T_pred)] #y_out_0 --> y_out_11\n",
    "    list_vsn            = ['vsn_'+str(i)               for i in range(0,hidden_size)] #vsn_0 --> vsn_255\n",
    "    df_out              = pd.DataFrame(columns=list_x_obs + list_y_obs + list_x_out + list_y_out + list_x_pred + list_y_pred + list_x_stoch_pred_m + list_y_stoch_pred_m + list_x_stoch_pred_s + list_y_stoch_pred_s + list_c_context + list_h_context + list_vsn)# + list_vsn_visual)\n",
    "\n",
    "    for i,data in enumerate(test_loader):\n",
    "        start_time = time.time()\n",
    "        # Forward\n",
    "        obs, pred, features, frame_tensor = data\n",
    "\n",
    "        input_tensor, output_tensor         = obs.double().squeeze(dim=1).to('cuda', non_blocking=True), pred.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "\n",
    "        features                 = features.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "\n",
    "        prediction = model(input_tensor, output_tensor, features)\n",
    "    \n",
    "        calculated_prediction =  prediction.cumsum(axis=1) \n",
    "\n",
    "        loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction) #lreg\n",
    "\n",
    "        if biased_loss_mode:\n",
    "            weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "            weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "            loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "        else:\n",
    "            loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2)) #lmse\n",
    "        out_x           = output_tensor[:,:,0].cumsum(axis=1)\n",
    "        out_y           = output_tensor[:,:,1].cumsum(axis=1)\n",
    "        pred_x          = calculated_prediction[:,:,0]\n",
    "        pred_y          = calculated_prediction[:,:,1]\n",
    "        ADE             = ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "        FDE             = ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "        total_loss      = loss.double() + regularization_factor * loss_line_regularizer.double() #loss\n",
    "        print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "        epoch_loss += total_loss.item()\n",
    "        ADEs    += ADE.item()\n",
    "        FDEs    += FDE.item()\n",
    "        input_x_lin                 = input_tensor[:,:,0].view(-1, T_obs).cpu() #x_obs\n",
    "        input_y_lin                 = input_tensor[:,:,1].view(-1, T_obs).cpu() #y_obs\n",
    "        output_x_lin                = output_tensor[:,:,0].view(-1, T_pred).cpu() #x_out\n",
    "        output_y_lin                = output_tensor[:,:,1].view(-1, T_pred).cpu() #y_out\n",
    "        prediction_x_lin            = prediction[:,:,0].view(-1, T_pred).cpu() #x_pred\n",
    "        prediction_y_lin            = prediction[:,:,1].view(-1, T_pred).cpu() #y_pred\n",
    "#         stoch_prediction_x_m        = stochastic_prediction[:,:,0].view(-1, T_pred).cpu() #x_stoch_pred_m\n",
    "#         stoch_prediction_x_s        = stochastic_prediction[:,:,1].view(-1, T_pred).cpu() #x_stoch_pred_s\n",
    "#         stoch_prediction_y_m        = stochastic_prediction[:,:,2].view(-1, T_pred).cpu() #y_stoch_pred_m\n",
    "#         stoch_prediction_y_s        = stochastic_prediction[:,:,3].view(-1, T_pred).cpu() #y_stoch_pred_s\n",
    "#         context_h_lin               = encoder_hidden[0].view(-1, hidden_size).cpu() #context_h\n",
    "#         context_c_lin               = encoder_hidden[1].view(-1, hidden_size).cpu() #context_c\n",
    "#         visual_embedding_weights    = visual_embedding.view(-1, hidden_size).cpu() #vsn\n",
    "\n",
    "#         whole_data                  = torch.cat((input_x_lin, input_y_lin, output_x_lin, output_y_lin, prediction_x_lin, prediction_y_lin, stoch_prediction_x_m, stoch_prediction_y_m, stoch_prediction_x_s, stoch_prediction_y_s, context_c_lin, context_h_lin, visual_embedding_weights), 1)\n",
    "#         temp                        = pd.DataFrame(whole_data.detach().cpu().numpy(), columns=list_x_obs + list_y_obs + list_x_out + list_y_out + list_x_pred + list_y_pred + list_x_stoch_pred_m + list_y_stoch_pred_m + list_x_stoch_pred_s + list_y_stoch_pred_s + list_c_context + list_h_context + list_vsn)\n",
    "#         df_out                      = df_out.append(temp)\n",
    "#         df_out.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "\n",
    "    # ADE/FDE Report\n",
    "    out_x  = df_out[['x_out_' +str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    pred_x = df_out[['x_pred_'+str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    out_y  = df_out[['y_out_' +str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    pred_y = df_out[['y_pred_'+str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    ADE = (out_x.sub(pred_x.values)**2).add((out_y.sub(pred_y.values)**2).values, axis=1)**(1/2)\n",
    "    df_out['ADE'] = ADE.mean(axis=1)\n",
    "    FDE = ADE.x_out_11\n",
    "    df_out['FDE'] = FDE\n",
    "    Mean_ADE = df_out.ADE.mean()\n",
    "    Mean_FDE = df_out.FDE.mean()\n",
    "    print(\"MEAN ADE/FDE\\t\",Mean_ADE,Mean_FDE)\n",
    "    writer.add_scalar(\"Final_Test/ADE_\"+path_mode, Mean_ADE, global_step=0)\n",
    "    writer.add_scalar(\"Final_Test/FDE_\"+path_mode, Mean_FDE, global_step=0)\n",
    "\n",
    "    df_out.to_sql(table_out+'_'+path_mode, cnx2, if_exists=\"replace\", index=False)\n",
    "    writer.close()\n",
    "    return ADEs, FDEs, int(data_size/chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1249ac",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66884705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, scheduler, filename='checkpoint.pth.tar'):\n",
    "    start_epoch = 0\n",
    "    best_val=-1\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        try:\n",
    "            best_val=checkpoint['best_loss']\n",
    "        except:\n",
    "            best_val=-1\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\".format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return model, optimizer, scheduler, start_epoch, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/notebook_data/Introvert_ResnetTransf/save_models/cross_attention_2/model_best.pth\"\n",
    "\n",
    "print(\"LOAD MODEL\")\n",
    "# Change device to cpu\n",
    "#del model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "model = Seq2SeqCA(device,embed_size=512,code_size=512,dropout_val=dropout_val,batch_size=batch_size)\n",
    "model = model.to(device).double()\n",
    "model, optimizer, scheduler, start_epoch, best_val = load_checkpoint(model, optimizer, scheduler, filename= model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3256ae",
   "metadata": {},
   "source": [
    "# Val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca085db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset and loader\n",
    "print(\"Initializing val dataset\")\n",
    "dataset_val   = TrajectoryPredictionDataset(image_folder_path, cnx_val, conv_model)\n",
    "test_loader   = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1a3732",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EVALUATE bst\")\n",
    "model.eval()\n",
    "path_mode = 'bst'\n",
    "print(\"path mode\\t\",path_mode)\n",
    "evaluate_eval(model, optimizer, criterion, criterion_vision, clip, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca18275",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
