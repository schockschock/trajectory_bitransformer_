{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f123968",
   "metadata": {},
   "source": [
    "## Dans ce notebook :\n",
    "\n",
    "on test un encoder multimodal qui prend en entrée les coordonnées \n",
    "et les features extraites d'un Resnet. On utilise le principe de Cross-Attention du papier \"Multi-modal \n",
    "Transformers\", on utilise ensuite le décoder classique (transformer aussi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116854f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader \n",
    "import torchvision\n",
    "from torchvision import datasets \n",
    "from torch.nn.functional import softmax\n",
    "from torchvision.transforms import ToTensor \n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "import datetime\n",
    "import cv2\n",
    "from skimage.util import img_as_ubyte\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import argparse\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "# Log\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "#Transformer Decoder\n",
    "from models.scratch_transformer.decoder import TransformerDecoder\n",
    "from models.cross_attention_transformer.encoder import Encoder\n",
    "from models.cross_attention_transformer.encoder_layer import EncoderLayer\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd3ca10",
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__=os.getcwd()\n",
    "print(__file__)\n",
    "\n",
    "#Dataset name\n",
    "dataset_name = \"eth\" # dataset options: 'university', 'zara_01', 'zara_02', 'eth', 'hotel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d26b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now() # current date and time\n",
    "current_time_date = now.strftime(\"%d_%m_%y_%H_%M_%S\")\n",
    "run_folder  = \"Outputs/traj_pred_\"+ dataset_name + \"_\" + str(os.path.basename(__file__)) + str(current_time_date)\n",
    "os.makedirs(run_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185db074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make log folder for tensorboard\n",
    "#SummaryWriter_path = run_folder + \"/log\"\n",
    "#os.makedirs(SummaryWriter_path) \n",
    "SummaryWriter_path = '/notebook_data/work_dirs/cross_attention_test_2'\n",
    "  \n",
    "writer = SummaryWriter(SummaryWriter_path,comment=\"ADE_FDE_Train\")\n",
    "\n",
    "# Make image folder to save outputs\n",
    "image_path  = run_folder + \"/Visual_Prediction\"\n",
    "os.makedirs(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataBase Variables\n",
    "image_folder_path       = '/notebook_data/data/data_trajpred/'+dataset_name\n",
    "DB_PATH_train     = '/notebook_data/data/data_trajpred/'+dataset_name+\"/pos_data_train.db\"\n",
    "cnx_train         = sqlite3.connect(DB_PATH_train)\n",
    "DB_PATH_val     = '/notebook_data/data/data_trajpred/'+dataset_name+\"/pos_data_val.db\"\n",
    "cnx_val         = sqlite3.connect(DB_PATH_val)\n",
    "DB_DIR      = run_folder + '/database'\n",
    "os.makedirs( DB_DIR )\n",
    "DB_PATH2    = DB_DIR+'/db_one_ped_delta_coordinates_results.db'\n",
    "cnx2        = sqlite3.connect(DB_PATH2)\n",
    "\n",
    "#Other variables\n",
    "T_obs                   = 8\n",
    "T_pred                  = 12\n",
    "T_total                 = T_obs + T_pred #8+12=20\n",
    "data_id                 = 0 \n",
    "batch_size              = 4 #10#100 #15 #2\n",
    "chunk_size              = batch_size * T_total # Chunksize should be multiple of T_total\n",
    "in_size                 = 2\n",
    "stochastic_out_size     = in_size * 2\n",
    "hidden_size             = 256 #!64\n",
    "embed_size              = 64 #16 #!64\n",
    "global dropout_val\n",
    "dropout_val             = 0.2 #0.5\n",
    "teacher_forcing_ratio   = 0.7 # 0.9\n",
    "regularization_factor   = 0.5 # 0.001\n",
    "avg_n_path_eval         = 20\n",
    "bst_n_path_eval         = 20\n",
    "path_mode               = \"top5\" #\"avg\",\"bst\",\"single\",\"top5\"\n",
    "regularization_mode     = \"regular\" #\"weighted\",\"e_weighted\", \"regular\"\n",
    "startpoint_mode         = \"on\" #\"on\",\"off\"\n",
    "enc_out                 = \"on\" #\"on\",\"off\"\n",
    "biased_loss_mode        = 0 # 0 , 1\n",
    "\n",
    "\n",
    "table_out   = \"results_delta\"\n",
    "table       = \"dataset_T_length_20delta_coordinates\" #\"dataset_T_length_\"+str(T_total)+\"delta_coordinates\"\n",
    "df_id       = pd.read_sql_query(\"SELECT data_id FROM \"+table, cnx_train)\n",
    "data_size   = df_id.data_id.max() * T_total\n",
    "epoch_num   = 100\n",
    "from_epoch  = 0\n",
    "\n",
    "#Visual Variables\n",
    "image_size              = 256  \n",
    "image_dimension         = 3\n",
    "mask_size               = 16\n",
    "visual_features_size    = 128 \n",
    "visual_embed_size       = 64  #128 #256 #64\n",
    "vsn_module_out_size    = 256\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "\n",
    "#Model Path\n",
    "model_path = run_folder + \"/NNmodel\" \n",
    "os.makedirs(model_path)   \n",
    "model_path = model_path + str(\"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d938d79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_mode         = 1 #output  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c822952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87ee99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Matrice Standard\n",
    "if dataset_name == 'eth':  \n",
    "    h = np.array([[2.8128700e-02, 2.0091900e-03, -4.6693600e+00],[8.0625700e-04, 2.5195500e-02, -5.0608800e+00],[ 3.4555400e-04, 9.2512200e-05, 4.6255300e-01]]) #h pour eth\n",
    "if dataset_name =='hotel':        \n",
    "    h = np.array([[1.1048200e-02, 6.6958900e-04, -3.3295300e+00],[-1.5966000e-03, 1.1632400e-02, -5.3951400e+00],[1.1190700e-04, 1.3617400e-05, 5.4276600e-01]]) #h pour hotel\n",
    "if dataset_name =='zara01': \n",
    "    h = np.array([[0.02174104, 0, -0.15],[0, -0.02461883, 13.77429807],[0, 0, 1]]) #h pour zara1\n",
    "if dataset_name =='zara02': \n",
    "    h = np.array([[0.02174104, 0, -0.4],[0, -0.02386598, 14.98401686],[0, 0, 1]]) #h pour zara2\n",
    "if dataset_name =='univ':\n",
    "    h = np.array([[0.02220407, 0, -0.48],[0, -0.02477289, 13.92551292],[0, 0, 1]]) #h pour univ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd29766a",
   "metadata": {},
   "source": [
    "# Classe Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2f2094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other variables\n",
    "T_obs = 8\n",
    "T_pred = 12\n",
    "T_total = T_obs + T_pred  # 8+12=20\n",
    "image_size = 256\n",
    "in_size = 2\n",
    "# \"dataset_T_length_\"+str(T_total)+\"delta_coordinates\"\n",
    "table = \"dataset_T_length_20delta_coordinates\"\n",
    "\n",
    "\n",
    "class TrajectoryPredictionDataset(torch.utils.data.Dataset):\n",
    "    # Enc.cinématique reçoit la trajectoire observée de humain cible (input) de la forme T=(u1,u2-u1,u3-u2,..) qui consiste en les coordonnées de la position de départ et en les déplacements relatifs de l'humain entre les images consécutives.\n",
    "    # Ce format a été choisi car il permet au modèle de mieux capturer les similarités entre des trajectoires presque identiques qui peuvent avoir des points de départ différents.\n",
    "    def __init__(self, ROOT_DIR, cnx, conv_model = None):\n",
    "\n",
    "        self.pos_df = pd.read_sql_query(\"SELECT * FROM \"+str(table), cnx)\n",
    "        self.root_dir = ROOT_DIR+'/visual_data'\n",
    "        self.transform = torchvision.transforms.Compose([torchvision.transforms.Resize((image_size, image_size)),\n",
    "                                                         torchvision.transforms.ToTensor(),\n",
    "                                                         torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "        self.visual_data = []\n",
    "        # read sorted frames\n",
    "        for img in sorted(os.listdir(self.root_dir)):\n",
    "            self.visual_data.append(self.transform(\n",
    "                Image.open(os.path.join(self.root_dir)+\"/\"+img)))\n",
    "        self.visual_data = torch.stack(self.visual_data)\n",
    "        \n",
    "        self.return_features = False\n",
    "        if conv_model:\n",
    "            self.return_features = True\n",
    "            self.features_extractor = features_extraction(conv_model, in_planes = 3)\n",
    "            \n",
    "        self.features = ...\n",
    "    def extract_features(self, #put features ici)\n",
    "                         #Faire une fonction qui calcule les features directement en un bloc\n",
    "                                 #\n",
    "                         #DU STYLE\n",
    "#         if self.return_features:\n",
    "#             features = self.features_extractor(frames)\n",
    "#             features = features.view((1,8,-1))\n",
    "#             features_out = features.clone().detach()\n",
    "#             print(features_out.size())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pos_df.data_id.max()  # data_id maximum dans dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        #print(\"idx :\", idx)\n",
    "\n",
    "        # table dont data_id=idx\n",
    "        extracted_df = self.pos_df[self.pos_df[\"data_id\"] == idx]\n",
    "\n",
    "        tensor = torch.tensor(extracted_df[['pos_x_delta', 'pos_y_delta']].values).reshape(\n",
    "            -1, T_total, in_size)  # juste pos_x_delta et pos_y_delta de extracted_df (tensor)\n",
    "        # obs de 8 et pred de 12 à partir de tensor construit\n",
    "        obs, pred = torch.split(tensor, [T_obs, T_pred], dim=1)\n",
    "\n",
    "        # extracted_df dont data_id=idx, on prend minimum frame_num et aprés on divise par 10, cela represente start_frame\n",
    "        start_frames = (extracted_df.groupby(\n",
    "            'data_id').frame_num.min().values/10).astype('int')\n",
    "        extracted_frames = []\n",
    "        extracted_features = []\n",
    "        for i in start_frames:\n",
    "            img = self.visual_data[i:i+T_obs]\n",
    "            extracted_frames.append(img)\n",
    "#             if conv_model is not None:\n",
    "#                 for i in start_frames:\n",
    "#                     extracted_features.append(self.features_extractor(img))\n",
    "            \n",
    "        \n",
    "            \n",
    "        # stack concatenates a sequence of tensors along a new dimension.\n",
    "        frames = torch.stack(extracted_frames)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        start_frames = torch.tensor(start_frames)  # tensor([start_frames])\n",
    "        \n",
    "        if self.return_features:\n",
    "            return obs, pred, features_out, start_frames\n",
    "            #return obs, pred, frames, features_out, start_frames #Ou on peut return les frames aussi mais dataset plus lourd\n",
    "        else:\n",
    "            return obs, pred, frames, start_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1771d406",
   "metadata": {},
   "source": [
    "# Initialize random weights for NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b23f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c232eb",
   "metadata": {},
   "source": [
    "# Regularizer loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974cccce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_sigma_distance  = torch.zeros(1) #tensor[(0.)]\n",
    "\n",
    "def distance_from_line_regularizer(input_tensor,prediction):\n",
    "    global sum_sigma_distance\n",
    "    #the regularization is defined as the sum of Euclidean distances between each step of the predicted trajectory Tf , and a line fitted to the observed trajectory To.\n",
    "    # Fit a line to observation points over batch \n",
    "    input_tensor    = input_tensor.double()\n",
    "    prediction      = prediction.double()\n",
    "    input_tensor    = input_tensor.cumsum(dim=1).double()\n",
    "    X               = torch.ones_like(input_tensor).double().to('cuda', non_blocking=True)\n",
    "    X[:,:,0]        = input_tensor[:,:,0]\n",
    "    Y               = (input_tensor[:,:,1]).unsqueeze(-1).double()\n",
    "    try:\n",
    "        try:\n",
    "            XTX_1           = torch.matmul( X.transpose(-1,-2), X).double().inverse()\n",
    "        except:\n",
    "            XTX_1           = torch.matmul( X.transpose(-1,-2), X).double().pinverse()\n",
    "        XTY             = torch.matmul( X.transpose(-1,-2), Y)\n",
    "        theta           = torch.matmul( XTX_1.double(), XTY.double())\n",
    "        # Calculate real values of prediction instead of delta\n",
    "        prediction[:,:,0] = prediction[:,:,0] + input_tensor[:,-1,0].unsqueeze(-1) \n",
    "        prediction[:,:,1] = prediction[:,:,1] + input_tensor[:,-1,1].unsqueeze(-1)\n",
    "        \n",
    "        # Calculate distance ( predicted_points , observation_fitted_line ) over batch\n",
    "        theta0x0        = theta[:,0,:].double() * prediction[:,:,0].double()\n",
    "        denominator     = torch.sqrt( theta[:,0,:].double() * theta[:,0,:].double() + 1 )\n",
    "        nominator       = theta0x0 + theta[:,1,:] - prediction[:,:,1].double()\n",
    "        distance        = nominator.abs() / denominator\n",
    "        if regularization_mode =='weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().float(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(-1,T_pred)\n",
    "            weighted_distance   = weight * distance\n",
    "\n",
    "        elif regularization_mode =='e_weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().float(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(distance.size(0),T_pred)\n",
    "            weight              = torch.exp(weight)\n",
    "            weighted_distance   = weight*distance\n",
    "\n",
    "        else:\n",
    "            weighted_distance = distance\n",
    "        sigma_distance  = torch.mean(weighted_distance,1)\n",
    "        sum_sigma_distance  = torch.mean(sigma_distance)\n",
    "        return sum_sigma_distance\n",
    "    except:\n",
    "        print(\"SINGULAR VALUE\")\n",
    "        sum_sigma_distance = torch.zeros(1).to('cuda', non_blocking=True) + 20\n",
    "        return sum_sigma_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a4d6b",
   "metadata": {},
   "source": [
    "# Encoder Transformer (Cross-Attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a109d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderSelfAttentionNew(nn.Module):\n",
    "#     def __init__(self,device,d_model, d_k, d_v, n_head, dff=2048, dropout_transformer=.1, n_module=6):\n",
    "#         super(EncoderSelfAttentionNew, self).__init__()\n",
    "        \n",
    "        \n",
    "#         self.encoder = nn.ModuleList([MultiHeadAttention(d_model, d_k, d_v, n_head, dff, dropout_transformer)\n",
    "#                                       for _ in range(n_module)])\n",
    "#         self.device = device\n",
    "#     def forward(self, x):\n",
    "    \n",
    "        \n",
    "        \n",
    "#         for l in self.encoder:\n",
    "#             in_encoder = l(in_encoder, in_encoder, in_encoder)\n",
    "#         return in_encoder\n",
    "    \n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Layer du papier \"Multimodal Transformer\", cf fig 2 du papier\n",
    "    \"\"\"\n",
    "    def __init__(self,device,d_model, d_k, d_v, n_head, code_size=1024, dff=2048, dropout_transformer=.1, n_module=3):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        self.self_attention = ScaledDotProductAttention(d_model, d_k, d_v, n_head)\n",
    "        \n",
    "        #Celle du haut dans le papier\n",
    "        self.cross_attention_features = ScaledDotProductAttention(d_model, d_k, d_v, n_head)\n",
    "        #Celle du bas dans le papier\n",
    "        self.cross_attention_coords = ScaledDotProductAttention(d_model, d_k, d_v, n_head)\n",
    "        \n",
    "        self.ffn_coords = nn.Sequential(*[nn.Linear(d_model, dff), nn.ReLU(inplace=False), nn.Dropout(p=dropout_transformer),nn.Linear(dff, code_size)])\n",
    "        \n",
    "        \n",
    "        self.ffn_features = nn.Sequential(*[nn.Linear(d_model, dff), nn.ReLU(inplace=False), nn.Dropout(p=dropout_transformer),nn.Linear(dff, code_size)])\n",
    "        \n",
    "#         self.ffn_coords = nn.Linear(d_model, code_size)\n",
    "#         self.ffn_features = nn.Linear(d_model, code_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_transformer)\n",
    "        self.layer_norm = nn.LayerNorm(code_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, in_encoder_coords, in_encoder_features):\n",
    "        \"\"\"\n",
    "        Encoder Layer return 2 output\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        coords_self_att = self.self_attention(in_encoder_coords,in_encoder_coords,in_encoder_coords)\n",
    "        \n",
    "        cross_attention_features = self.cross_attention_features(in_encoder_features,coords_self_att,coords_self_att)\n",
    "        \n",
    "        cross_attention_coords = self.cross_attention_coords(coords_self_att,in_encoder_features,in_encoder_features)\n",
    "        \n",
    "        out_coords = self.relu(self.ffn_coords(cross_attention_coords))\n",
    "        \n",
    "        out_features = self.relu(self.ffn_features(cross_attention_features))\n",
    "        \n",
    "        out_coords = self.dropout(out_coords)\n",
    "        out_features = self.dropout(out_features) \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.layer_norm(out_coords), self.layer_norm(out_features)\n",
    "        \n",
    "        \n",
    "class EncoderTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Transformer adapted from the MTN Paper.\n",
    "    It only takes as input the (x,y) coordinates as in our case the camera is static (the MTN was used for a dynamic \"ego-car\" context)\n",
    "    \"\"\"\n",
    "    def __init__(self,device,code_size,d_k, d_v,n_head=8,n_module=3,ff_size=2048,dropout1d=0.5, feature_size=512):\n",
    "        super(EncoderTransformer, self).__init__()\n",
    "        self.device = device\n",
    "        self.fc_coords = nn.Linear(2,code_size)\n",
    "        self.fc_features = nn.Linear(feature_size,code_size)\n",
    "        \n",
    "        self.encoder = nn.ModuleList([EncoderLayer(device,code_size, d_k, d_v, n_head, dff=ff_size, dropout_transformer=dropout1d, code_size=code_size)\n",
    "                                      for _ in range(n_module)])\n",
    "        \n",
    "        self.relu =  nn.ReLU()\n",
    "        \n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((8,code_size)) #final pooling\n",
    "        self.dropout = nn.Dropout(dropout1d)\n",
    "        \n",
    "    def forward(self, coords, features):\n",
    "        \"\"\"\n",
    "        coords : sequence of coordinates\n",
    "        features : sequence of features extracted from a ResNet\n",
    "        \n",
    "        \"\"\"\n",
    "        batch_size = coords.shape[0]\n",
    "        \n",
    "        coords = self.relu(self.fc_coords(coords))\n",
    "        \n",
    "        in_encoder_coords = coords + sinusoid_encoding_table(coords.shape[1], coords.shape[2]).expand(coords.shape).to(self.device)\n",
    "        \n",
    "#         print(f\"Shape of features{features.size()}\")\n",
    "#         print(f\"Shape of features{self.fc_features(features).size()}\")\n",
    "        \n",
    "        in_encoder_features = self.relu(self.fc_features(features))\n",
    "        \n",
    "        for layer in self.encoder:\n",
    "            out_enc_coords, out_enc_features = layer(in_encoder_coords, in_encoder_features)\n",
    "        #out_enc_coords, out_enc_features = self.encoder(in_encoder_coords, in_encoder_features)\n",
    "        \n",
    "        code = (out_enc_coords+out_enc_features).to(device).double()\n",
    "        \n",
    "        return code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b46a0ae",
   "metadata": {},
   "source": [
    "# Resnet_Transformer_Vision Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8404554b",
   "metadata": {},
   "source": [
    "### Spatial features extraction (Resnet 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037ff97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet(pretrain=True,layers_to_unfreeze=8,layers_to_delete=2,in_planes=3):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        pretrain: Define if we load a pretrained model from ImageNet\n",
    "        layers_to_unfreeze: Define the number of layers that we want to train at the end of the Resnet\n",
    "        layers_to_delete: Define the numbers of layers that we want to delete\n",
    "        in_planes: Define the numbers of input channels of images (supported values: 1,2 or 3)\n",
    "    return: The Resnet model\n",
    "    \"\"\"\n",
    "    resnet = torchvision.models.resnet18(pretrained=pretrain)\n",
    "    # Create a new model cause we don't want the pooling operation at the end and the classifier\n",
    "    model = nn.Sequential()\n",
    "    number_of_layers = len(list(resnet.children())) - layers_to_delete # In practice it remove the pooling operation and the classifier\n",
    "\n",
    "    if number_of_layers<layers_to_unfreeze:\n",
    "        layers_to_unfreeze = number_of_layers\n",
    "    layers_to_freeze = number_of_layers - layers_to_unfreeze\n",
    "    i=0\n",
    "    for child in resnet.children():\n",
    "        # For the first layers we create a new weight if in_planes is not 3 cause ResNet is pretrain on image with 3 channels there is no version for 1 channel\n",
    "        if i==0 and in_planes<3:\n",
    "            if i<layers_to_freeze: # Define if we freeze this layer or no\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False # Freeze the layers by passing requires_grad attribute to False\n",
    "            w = child._parameters['weight'].data # Get the weight for 3 channels data\n",
    "            child._modules['0'] = nn.Conv2d(in_planes, 64, kernel_size=3, padding=1) # Define the new conv layer\n",
    "            if in_planes == 1:\n",
    "                child._parameters['weight'].data = w.mean(dim=1, keepdim=True) # If the number of channels is 1 we made the mean of channels to set the new weight\n",
    "            else:\n",
    "                child._parameters['weight'].data = w[:, :-1] * 1.5\n",
    "\n",
    "        if i<layers_to_freeze: # Define if we freeze this layer or no\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False # Freeze the layers by passing requires_grad attribute to False\n",
    "        if i<number_of_layers: # To define if we keep this layer or not\n",
    "            model.append(child) \n",
    "        i+=1\n",
    "    return model\n",
    "\n",
    "\n",
    "class features_extraction(nn.Module):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    conv_model: The convolution model used before capsules for the moment only ResNet is supported\n",
    "    in_planes: Numbers of channels for the image\n",
    "    \"\"\"\n",
    "    def __init__(self,conv_model,in_planes: int):\n",
    "        super().__init__()\n",
    "        self.conv_model = conv_model\n",
    "        self.in_planes = in_planes\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "    def forward(self,input):\n",
    "        shape = input.size()\n",
    "        x = input.view(-1,self.in_planes,shape[-2],shape[-1])\n",
    "        x = self.conv_model(x)\n",
    "        x = self.pooling(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17b6e3b",
   "metadata": {},
   "source": [
    "### Encoder Transformer (Vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e868f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_embedding(input, d_model):\n",
    "    input = input.view(-1, 1)\n",
    "    dim = torch.arange(d_model // 2, dtype=torch.float32, device=input.device).view(1, -1)\n",
    "    sin = torch.sin(input / 10000 ** (2 * dim / d_model))\n",
    "    cos = torch.cos(input / 10000 ** (2 * dim / d_model))\n",
    "\n",
    "    out = torch.zeros((input.shape[0], d_model), device=input.device)\n",
    "    out[:, ::2] = sin\n",
    "    out[:, 1::2] = cos\n",
    "    return out\n",
    "\n",
    "def sinusoid_encoding_table(max_len, d_model):\n",
    "    pos = torch.arange(max_len, dtype=torch.float32)\n",
    "    out = position_embedding(pos, d_model)\n",
    "    return out\n",
    "\n",
    "#Scaled dot-product attention\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h):\n",
    "        \"\"\"\n",
    "        param:\n",
    "        d_model: Output dimensionality of the model\n",
    "        d_k: Dimensionality of queries and keys\n",
    "        d_v: Dimensionality of values\n",
    "        h: Number of heads\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "        self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.init_weights(gain=1.0)\n",
    "\n",
    "    def init_weights(self, gain=1.0):\n",
    "        nn.init.xavier_normal_(self.fc_q.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_k.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_v.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_o.weight, gain=gain)\n",
    "        nn.init.constant_(self.fc_q.bias, 0)\n",
    "        nn.init.constant_(self.fc_k.bias, 0)\n",
    "        nn.init.constant_(self.fc_v.bias, 0)\n",
    "        nn.init.constant_(self.fc_o.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_model)\n",
    "        :param keys: Keys (b_s, nk, d_model)\n",
    "        :param values: Values (b_s, nk, d_model)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "\n",
    "        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "\n",
    "        att = torch.softmax(att, -1)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "        return out\n",
    "\n",
    "    \n",
    "#Multi-head attention layer with Dropout and Layer Normalization\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h, dff=2048, dropout=.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(d_model=d_model, d_k=d_k, d_v=d_v, h=h)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Sequential(*[nn.Linear(d_model, dff), nn.ReLU(inplace=False), nn.Dropout(p=dropout),nn.Linear(dff, d_model)])\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        att = self.attention(queries, keys, values)\n",
    "        att = self.dropout(att)\n",
    "        att = self.fc(att)\n",
    "        att = self.dropout(att)\n",
    "        return self.layer_norm(queries + att)\n",
    "\n",
    "class EncoderSelfAttention(nn.Module):\n",
    "    def __init__(self,device,d_model, d_k, d_v, n_head, dff=2048, dropout_transformer=.1, n_module=6):\n",
    "        super(EncoderSelfAttention, self).__init__()\n",
    "        self.encoder = nn.ModuleList([MultiHeadAttention(d_model, d_k, d_v, n_head, dff, dropout_transformer)\n",
    "                                      for _ in range(n_module)])\n",
    "        self.device = device\n",
    "    def forward(self, x):\n",
    "        in_encoder = x + sinusoid_encoding_table(x.shape[1], x.shape[2]).expand(x.shape).to(self.device)\n",
    "        for l in self.encoder:\n",
    "            in_encoder = l(in_encoder, in_encoder, in_encoder)\n",
    "        return in_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc7e17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "266dafd0",
   "metadata": {},
   "source": [
    "### Resnet + Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8bddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _GestureTransformer(nn.Module):\n",
    "    \"\"\"Multi-Modal model on 3 or 1 channel\"\"\"\n",
    "    def __init__(self,device,backbone=\"resnet\",in_planes=3,pretrained= True,input_dim=512,layers_to_unfreeze=8,layers_to_delete=2,n_head=8,n_module=6,ff_size=1024,dropout1d=0.5):\n",
    "        super(_GestureTransformer, self).__init__()\n",
    "\n",
    "        self.in_planes = in_planes\n",
    "        self.device = device\n",
    "        self.conv_name = backbone\n",
    "        self.conv_model = None\n",
    "        \n",
    "        if self.conv_name.lower()==\"resnet\":\n",
    "            self.conv_model = Resnet(pretrained,layers_to_unfreeze,layers_to_delete,in_planes)\n",
    "        else:\n",
    "            raise NotImplementedError(\"The model {} is not supported!\".format(self.conv_name))\n",
    "        self.conv_model.to(device)\n",
    "        self.features = features_extraction(self.conv_model,in_planes)\n",
    "\n",
    "        self.self_attention = EncoderSelfAttention(device,input_dim,64,64,n_head=n_head,dff=ff_size,dropout_transformer=dropout1d,n_module=n_module)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((8,input_dim)) #final pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        x = self.features(x)\n",
    "        x = x.view(shape[0],shape[1],-1)\n",
    "        x = self.self_attention(x)\n",
    "\n",
    "        x = self.pool(x).squeeze(dim=1) #final pooling\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058102f7",
   "metadata": {},
   "source": [
    "## Création du modèle en entier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4442ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqCA(nn.Module):\n",
    "    \"\"\"\n",
    "    SEQ2SEQ MODEL USING THE CROSS ATTENTION MECANISM TO ENCODE BOTH COORDS AND RESNET FEATURES AT THE SAME TIME\n",
    "    \"\"\"\n",
    "    def __init__(self,device, embed_size, code_size=512, target_size = 12, dropout_val=dropout_val, batch_size=1, conv_model=Resnet()):\n",
    "        super(Seq2SeqCA, self).__init__()\n",
    "        \n",
    "        self.feature_size = 512\n",
    "        torch.cuda.empty_cache()\n",
    "        self.encoder = EncoderTransformer(device,code_size,64,64,dropout1d=dropout_val) #EncoderTransformer\n",
    "        #self.encoder.apply(init_weights)\n",
    "        #self.encoder = CoordinatesTransformer(device,code_size,64,64,dropout1d=dropout_val, batch_size=batch_size) #EncoderTransformer\n",
    "        \n",
    "        \n",
    "        self.decoder = TransformerDecoder(target_size, embed_dim=code_size, seq_len=12, num_layers=2, expansion_factor=4, n_heads=8) \n",
    "        #self.decoder.apply(init_weights)\n",
    "        \n",
    "#         self.vsn_module = _GestureTransformer(device,input_dim=code_size,dropout1d=dropout_val) #_GestureTransformer(partie vision)                   \n",
    "#         self.vsn_module.apply(init_weights)\n",
    "\n",
    "        self.feature_size = 512\n",
    "        self.features_ex = features_extraction(conv_model,in_planes=3)\n",
    "\n",
    "        self.pooling = nn.AdaptiveAvgPool1d((code_size)) # add a pooling (to have the same shape)\n",
    "        \n",
    "        self.code_pooling = nn.AdaptiveAvgPool2d((target_size,code_size))\n",
    "        \n",
    "        if device.type=='cuda':\n",
    "            self.encoder.cuda()\n",
    "            self.decoder.cuda()  \n",
    "            \n",
    "    def forward(self,input_tensor, target_tensor, features):\n",
    "        batch_size      = int(input_tensor.size(0))\n",
    "\n",
    "        #Les features ont été introduites directement dans le dataset\n",
    "        #features = self.features_ex(visual_input_tensor)\n",
    "\n",
    "        features = features.view((batch_size,8,-1)) #(bs, 8, 512)\n",
    "\n",
    "        encoder_output =  self.encoder(input_tensor, features) #(bs,8,code_size)\n",
    "\n",
    "        #start_point\n",
    "#         start_point     = (input_tensor[:,0,:]).to(device).clone().detach()\n",
    "        \n",
    "#         if startpoint_mode==\"on\":\n",
    "#             input_tensor[:,0,:]    = 0\n",
    "            \n",
    "#         visual_initial_vsn          = self.vsn_module(visual_input_tensor)\n",
    "#         visual_initial_vsn          = self.pooling(visual_initial_vsn) #pooling qu'on a ajouté\n",
    "        \n",
    "#         print(\"#######\")\n",
    "#         print(f\"encoder_output size : {encoder_output.size()}\")\n",
    "#         print(f\"visual_initial_vsn size : {visual_initial_vsn.size()}\")\n",
    "#         print(\"#######\")\n",
    "        \n",
    "        trg_mask = self.make_trg_mask(target_tensor)\n",
    "        #print(f\"target_tensor : {target_tensor.size()}\")\n",
    "        \n",
    "        code_seq_12 = self.code_pooling(encoder_output)\n",
    "        decoder_output = self.decoder(target_tensor,code_seq_12,trg_mask)\n",
    "\n",
    "        return decoder_output\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trg: target sequence\n",
    "        Returns:\n",
    "            trg_mask: target mask\n",
    "        \"\"\"\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = 12\n",
    "        #batch_size, trg_len = trg.shape\n",
    "        # returns the lower triangular part of matrix filled with ones\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            batch_size, 1, trg_len, trg_len\n",
    "        )\n",
    "        return trg_mask.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4092aeaf",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5560a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train dataset and loader\n",
    "\n",
    "batch_size = 4\n",
    "print(\"Initializing train dataset\")\n",
    "print(\"Batch_size : {}\".format(batch_size))\n",
    "\n",
    "conv_model = Resnet()\n",
    "\n",
    "dataset_train = TrajectoryPredictionDataset(image_folder_path, cnx_train, conv_model = conv_model)\n",
    "train_loader  = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True, pin_memory=True)\n",
    "validation_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18024a",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(train_loader))[2].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87cc8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2SeqCA(device,embed_size=512,code_size=512,dropout_val=dropout_val,batch_size=batch_size)\n",
    "model = model.to(device).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e656564",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_step               = 40\n",
    "initial_learning_rate       = 0.01\n",
    "clip                        = 1\n",
    "\n",
    "# MSE loss\n",
    "criterion                   = nn.MSELoss(reduction='mean')\n",
    "criterion_vision            = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# SGD optimizer\n",
    "optimizer                   = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9, weight_decay=0.01) #SGD\n",
    "scheduler                   = torch.optim.lr_scheduler.StepLR(optimizer, step_size=learning_step, gamma=0.1)\n",
    "five_fold_cross_validation  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eb4bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    torch.save(state, os.path.join(save_path,filename))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(save_path,filename), os.path.join(save_path,'model_best.pth'))\n",
    "        \n",
    "def train(model, optimizer, scheduler, criterion, criterion_vision, clip,train_loader, validation_loader, save_path=None):\n",
    "    global batch_size\n",
    "    i               = None\n",
    "    checked_frame   = 0\n",
    "\n",
    "    losses = []\n",
    "    print(\"Data Size \",data_size,\"\\tChunk Size \",chunk_size)\n",
    "    global teacher_forcing_ratio\n",
    "    counter =0\n",
    "    best_val = float(\"inf\")\n",
    "    for j in range(epoch_num):\n",
    "        model.train()\n",
    "        epoch_loss=0\n",
    "        if j%7 == 6:\n",
    "            teacher_forcing_ratio = (teacher_forcing_ratio - 0.2) if teacher_forcing_ratio>=0.1 else 0.0\n",
    "\n",
    "        # Update TeachForce ratio to gradually change during training\n",
    "        # global teacher_forcing_ratio\n",
    "        # teacher_forcing_ratio-= 1/epoch_num\n",
    "        print(\"TEACHER FORCE RATIO\\t\",teacher_forcing_ratio)\n",
    "        #print(\"Learning Rate\\t\", scheduler.get_last_lr())\n",
    "\n",
    "        \n",
    "        if(j>=from_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            start_time = time.time()\n",
    "            ADE = 0\n",
    "            FDE = 0\n",
    "            i   = 0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                #print(\"\\n--------------- Batch %d/ %d ---------------\"%(j,i)) #(epoch/i)\n",
    "                # Forward\n",
    "                obs, pred, visual_obs, frame_tensor = data\n",
    "                \n",
    "                input_tensor, output_tensor         = obs.double().squeeze(dim=1).to('cuda', non_blocking=True), pred.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "                    \n",
    "                visual_input_tensor                 = visual_obs.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "\n",
    "                prediction = model(input_tensor, output_tensor, visual_input_tensor)\n",
    "                \n",
    "                calculated_prediction = prediction.cumsum(axis=1) #calculated_prediction\n",
    "                \n",
    "                loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction) #loss (regularisation term Lreg)\n",
    "                \n",
    "                if biased_loss_mode:\n",
    "                    weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "                    weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "                    loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "                else:\n",
    "                    loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2)) #mean squared error (lmse)\n",
    "                    \n",
    "                out_x       = output_tensor[:,:,0].cumsum(axis=1)\n",
    "                out_y       = output_tensor[:,:,1].cumsum(axis=1)\n",
    "                pred_x      = calculated_prediction[:,:,0]\n",
    "                pred_y      = calculated_prediction[:,:,1]\n",
    "                ADE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "                # FDE      += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "                \n",
    "                # Backward Propagation\n",
    "                total_loss      = loss.double() + torch.tensor(regularization_factor).to('cuda', non_blocking=True) * loss_line_regularizer.double() #total loss\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "                #print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "                epoch_loss += total_loss.item()\n",
    "                #print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                torch.cuda.empty_cache()\n",
    "                writer.close()\n",
    "                count_div=i\n",
    "            \n",
    "            # tensorboard log\n",
    "            writer.add_scalar('ADE/train', ADE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('FDE/train', FDE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('LOSS/train', epoch_loss/(count_div+1), counter)\n",
    "            counter += 1\n",
    "\n",
    "        if scheduler.get_last_lr()[0]>0.001:\n",
    "            scheduler.step()\n",
    "        # validation(model, optimizer, criterion, criterion_vision, clip, validation_loader, j) \n",
    "        epoch_loss = epoch_loss / (int(data_size/chunk_size))\n",
    "        losses.append(epoch_loss)\n",
    "        display.clear_output(wait=True)\n",
    "        plt.plot(losses, '--ro', label='train loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'epoch {j}')\n",
    "        plt.show()\n",
    "        print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "        print(\"EPOCH \", j, \"\\tLOSS \", epoch_loss)\n",
    "        writer.add_scalar('epoch_loss/train', epoch_loss/ (int(data_size/chunk_size)), j ) #see how model performs on the training dataset\n",
    "        #torch.save( model.state_dict(), model_path+\"_current\")\n",
    "        print(\"-----------------------------------------------\\n\"+\"-----------------------------------------------\")\n",
    "\n",
    "        # save checkpoint for each epoch and a fine called best_model so far \n",
    "        print(np.argmin(losses))\n",
    "        is_best = epoch_loss < best_val\n",
    "        best_val = min(epoch_loss, best_val)\n",
    "        print(\"bestvaleur\", best_val)\n",
    "        if save_path and (j+1)%5==0:\n",
    "            save_checkpoint({'epoch': j+1,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'scheduler': scheduler.state_dict(),'best_loss': best_val}, is_best, save_path, 'epoch_{}.pth'.format(j+1))\n",
    "        \n",
    "    return epoch_loss / (int(data_size/chunk_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4b261",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"TRAIN\")\n",
    "model.train()\n",
    "print(\"path mode\\t\",path_mode)\n",
    "\n",
    "save_path = \"/notebook_data/Introvert_ResnetTransf/save_models/cross_attention/\"\n",
    "loss  = train(model, optimizer, scheduler, criterion, criterion_vision, clip, train_loader, validation_loader, save_path)\n",
    "print(\"LOSS \",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c747b0eb",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50ed3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, optimizer, criterion, criterion_vision, clip, validation_loader, counter):\n",
    "    global batch_size\n",
    "    model.eval()\n",
    "    i           = None\n",
    "    ADEs        = 0\n",
    "    FDEs        = 0\n",
    "    epoch_loss  = 0\n",
    "    loss_line_regularizer = 0\n",
    "    loss = 0 \n",
    "    total_loss = 0\n",
    "    ADE  = 0\n",
    "    FDE  = 0\n",
    "    for i,data in enumerate(test_loader):\n",
    "        # Forward\n",
    "        obs, pred, visual_obs, frame_tensor = data\n",
    "\n",
    "        input_tensor, output_tensor         = obs.double().squeeze(dim=1).to('cuda', non_blocking=True), pred.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "\n",
    "        visual_input_tensor                 = visual_obs.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "\n",
    "        prediction = model(input_tensor, output_tensor, visual_input_tensor) \n",
    "        \n",
    "        calculated_prediction = prediction.cumsum(axis=1) \n",
    "\n",
    "        loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction)\n",
    "        \n",
    "        if biased_loss_mode:\n",
    "            weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "            weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "            loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "        else:\n",
    "            loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2))\n",
    "            \n",
    "        out_x       = output_tensor[:,:,0].cumsum(axis=1)\n",
    "        out_y       = output_tensor[:,:,1].cumsum(axis=1)\n",
    "        pred_x      = calculated_prediction[:,:,0]\n",
    "        pred_y      = calculated_prediction[:,:,1]\n",
    "        ADE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "        FDE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "        total_loss  += loss.double() + regularization_factor * loss_line_regularizer.double() \n",
    "        print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "\n",
    "    writer.add_scalar('ADE/val_'+path_mode,             ADE.item()/(i+1),             counter)\n",
    "    writer.add_scalar('FDE/val_'+path_mode,             FDE.item()/(i+1),             counter)\n",
    "    writer.add_scalar('LOSS/val_'+path_mode,            total_loss.item()/(i+1)   ,   counter)\n",
    "    writer.add_scalar('LOSS_c/val_'+path_mode,          loss.item()/(i+1)        ,    counter)\n",
    "    writer.add_scalar('L-REGULARIZER/val_'+path_mode,   loss_line_regularizer.item()/(i+1), counter)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c6979f",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90521764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_eval(model, optimizer, criterion, criterion_vision, clip, five_fold_cross_validation):\n",
    "    global batch_size\n",
    "    model.eval()\n",
    "    i           = None\n",
    "    ADEs        = 0\n",
    "    FDEs        = 0\n",
    "    epoch_loss  = 0\n",
    "    list_x_obs          = ['x_obs_'+str(i)              for i in range(0,T_obs)] #x_obs_0 --> x_obs_7\n",
    "    list_y_obs          = ['y_obs_'+str(i)              for i in range(0,T_obs)] #y_obs_0 --> y_obs_7\n",
    "    list_c_context      = ['context_c_'+str(i)          for i in range(0,hidden_size)] #context_c_0 --> context_c_255\n",
    "    list_h_context      = ['context_h_'+str(i)          for i in range(0,hidden_size)] #context_h_0 --> context_h_255\n",
    "    list_x_pred         = ['x_pred_'+str(i)             for i in range(0,T_pred)] #x_pred_0 --> x_pred_11\n",
    "    list_y_pred         = ['y_pred_'+str(i)             for i in range(0,T_pred)] #y_pred_0 --> y_pred_11\n",
    "    list_x_stoch_pred_m = ['x_stoch_pred_m_'+str(i)     for i in range(0,T_pred)] #x_stoch_pred_m_0 --> x_stoch_pred_m_11\n",
    "    list_y_stoch_pred_m = ['y_stoch_pred_m_'+str(i)     for i in range(0,T_pred)] #y_stoch_pred_m_0 --> y_stoch_pred_m_11\n",
    "    list_x_stoch_pred_s = ['x_stoch_pred_s_'+str(i)     for i in range(0,T_pred)] #x_stoch_pred_s_0 --> x_stoch_pred_s_11\n",
    "    list_y_stoch_pred_s = ['y_stoch_pred_s_'+str(i)     for i in range(0,T_pred)] #y_stoch_pred_s_0 --> y_stoch_pred_s_11\n",
    "    list_x_out          = ['x_out_'+str(i)              for i in range(0,T_pred)] #x_out_0 --> x_out_11\n",
    "    list_y_out          = ['y_out_'+str(i)              for i in range(0,T_pred)] #y_out_0 --> y_out_11\n",
    "    list_vsn            = ['vsn_'+str(i)               for i in range(0,hidden_size)] #vsn_0 --> vsn_255\n",
    "    df_out              = pd.DataFrame(columns=list_x_obs + list_y_obs + list_x_out + list_y_out + list_x_pred + list_y_pred + list_x_stoch_pred_m + list_y_stoch_pred_m + list_x_stoch_pred_s + list_y_stoch_pred_s + list_c_context + list_h_context + list_vsn)# + list_vsn_visual)\n",
    "\n",
    "    for i,data in enumerate(test_loader):\n",
    "        start_time = time.time()\n",
    "        # Forward\n",
    "        obs, pred, visual_obs, frame_tensor = data\n",
    "\n",
    "        input_tensor, output_tensor         = obs.double().squeeze(dim=1).to('cuda', non_blocking=True), pred.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "\n",
    "        visual_input_tensor                 = visual_obs.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "\n",
    "        prediction = model(input_tensor, output_tensor, visual_input_tensor) \n",
    "    \n",
    "        calculated_prediction =  prediction.cumsum(axis=1) \n",
    "\n",
    "        loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction) #lreg\n",
    "\n",
    "        if biased_loss_mode:\n",
    "            weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "            weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "            loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "        else:\n",
    "            loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2)) #lmse\n",
    "        out_x           = output_tensor[:,:,0].cumsum(axis=1)\n",
    "        out_y           = output_tensor[:,:,1].cumsum(axis=1)\n",
    "        pred_x          = calculated_prediction[:,:,0]\n",
    "        pred_y          = calculated_prediction[:,:,1]\n",
    "        ADE             = ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "        FDE             = ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "        total_loss      = loss.double() + regularization_factor * loss_line_regularizer.double() #loss\n",
    "        print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "        epoch_loss += total_loss.item()\n",
    "        ADEs    += ADE.item()\n",
    "        FDEs    += FDE.item()\n",
    "        input_x_lin                 = input_tensor[:,:,0].view(-1, T_obs).cpu() #x_obs\n",
    "        input_y_lin                 = input_tensor[:,:,1].view(-1, T_obs).cpu() #y_obs\n",
    "        output_x_lin                = output_tensor[:,:,0].view(-1, T_pred).cpu() #x_out\n",
    "        output_y_lin                = output_tensor[:,:,1].view(-1, T_pred).cpu() #y_out\n",
    "        prediction_x_lin            = prediction[:,:,0].view(-1, T_pred).cpu() #x_pred\n",
    "        prediction_y_lin            = prediction[:,:,1].view(-1, T_pred).cpu() #y_pred\n",
    "#         stoch_prediction_x_m        = stochastic_prediction[:,:,0].view(-1, T_pred).cpu() #x_stoch_pred_m\n",
    "#         stoch_prediction_x_s        = stochastic_prediction[:,:,1].view(-1, T_pred).cpu() #x_stoch_pred_s\n",
    "#         stoch_prediction_y_m        = stochastic_prediction[:,:,2].view(-1, T_pred).cpu() #y_stoch_pred_m\n",
    "#         stoch_prediction_y_s        = stochastic_prediction[:,:,3].view(-1, T_pred).cpu() #y_stoch_pred_s\n",
    "#         context_h_lin               = encoder_hidden[0].view(-1, hidden_size).cpu() #context_h\n",
    "#         context_c_lin               = encoder_hidden[1].view(-1, hidden_size).cpu() #context_c\n",
    "#         visual_embedding_weights    = visual_embedding.view(-1, hidden_size).cpu() #vsn\n",
    "\n",
    "#         whole_data                  = torch.cat((input_x_lin, input_y_lin, output_x_lin, output_y_lin, prediction_x_lin, prediction_y_lin, stoch_prediction_x_m, stoch_prediction_y_m, stoch_prediction_x_s, stoch_prediction_y_s, context_c_lin, context_h_lin, visual_embedding_weights), 1)\n",
    "#         temp                        = pd.DataFrame(whole_data.detach().cpu().numpy(), columns=list_x_obs + list_y_obs + list_x_out + list_y_out + list_x_pred + list_y_pred + list_x_stoch_pred_m + list_y_stoch_pred_m + list_x_stoch_pred_s + list_y_stoch_pred_s + list_c_context + list_h_context + list_vsn)\n",
    "#         df_out                      = df_out.append(temp)\n",
    "#         df_out.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "\n",
    "    # ADE/FDE Report\n",
    "    out_x  = df_out[['x_out_' +str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    pred_x = df_out[['x_pred_'+str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    out_y  = df_out[['y_out_' +str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    pred_y = df_out[['y_pred_'+str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    ADE = (out_x.sub(pred_x.values)**2).add((out_y.sub(pred_y.values)**2).values, axis=1)**(1/2)\n",
    "    df_out['ADE'] = ADE.mean(axis=1)\n",
    "    FDE = ADE.x_out_11\n",
    "    df_out['FDE'] = FDE\n",
    "    Mean_ADE = df_out.ADE.mean()\n",
    "    Mean_FDE = df_out.FDE.mean()\n",
    "    print(\"MEAN ADE/FDE\\t\",Mean_ADE,Mean_FDE)\n",
    "    writer.add_scalar(\"Final_Test/ADE_\"+path_mode, Mean_ADE, global_step=0)\n",
    "    writer.add_scalar(\"Final_Test/FDE_\"+path_mode, Mean_FDE, global_step=0)\n",
    "\n",
    "    df_out.to_sql(table_out+'_'+path_mode, cnx2, if_exists=\"replace\", index=False)\n",
    "    writer.close()\n",
    "    return ADEs, FDEs, int(data_size/chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b140f93",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ca01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, scheduler, filename='checkpoint.pth.tar'):\n",
    "    start_epoch = 0\n",
    "    best_val=-1\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        try:\n",
    "            best_val=checkpoint['best_loss']\n",
    "        except:\n",
    "            best_val=-1\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\".format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return model, optimizer, scheduler, start_epoch, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a94998",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/notebook_data/Introvert_ResnetTransf/save_models/cross_attention/model_best.pth\"\n",
    "\n",
    "print(\"LOAD MODEL\")\n",
    "# Change device to cpu\n",
    "#del model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "model = Seq2SeqCA(device,embed_size=512,code_size=512,dropout_val=dropout_val,batch_size=batch_size)\n",
    "model = model.to(device).double()\n",
    "model, optimizer, scheduler, start_epoch, best_val = load_checkpoint(model, optimizer, scheduler, filename= model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6c5243",
   "metadata": {},
   "source": [
    "# Val dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76977a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset and loader\n",
    "print(\"Initializing val dataset\")\n",
    "dataset_val   = TrajectoryPredictionDataset(image_folder_path, cnx_val)\n",
    "test_loader   = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=2, drop_last=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42817965",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EVALUATE bst\")\n",
    "model.eval()\n",
    "path_mode = 'bst'\n",
    "print(\"path mode\\t\",path_mode)\n",
    "evaluate_eval(model, optimizer, criterion, criterion_vision, clip, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644d8c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
