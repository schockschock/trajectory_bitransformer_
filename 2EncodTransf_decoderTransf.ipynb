{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "import datetime\n",
    "from skimage.util import img_as_ubyte\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import argparse\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from models.indiv_crossAttention_seq2seq import crossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_mode         = 0#output  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make folder for outputs and logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset name\n",
    "dataset_name = \"eth\" # dataset options: 'university', 'zara_01', 'zara_02', 'eth', 'hotel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebook_data/trajectory_bitransformer_-main/trajectory_bitransformer_-main\n"
     ]
    }
   ],
   "source": [
    "__file__=os.getcwd()\n",
    "print(__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now() # current date and time\n",
    "current_time_date = now.strftime(\"%d_%m_%y_%H_%M_%S\")\n",
    "run_folder  = \"Outputs/traj_pred_\"+ dataset_name + \"_\" + str(os.path.basename(__file__)) + str(current_time_date)\n",
    "os.makedirs(run_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip to left side bar\n",
    ">\n",
    "/\n",
    "Name\n",
    "Last Modified\n",
    "\n",
    "# Make log folder for tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SummaryWriter_path = \"/notebook_data/work_dirs/dec_trans_test_new_decoder_1/\"\n",
    "#os.makedirs(SummaryWriter_path)   \n",
    "writer = SummaryWriter(SummaryWriter_path,comment=\"ADE_FDE_Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make image folder to save outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path  = run_folder + \"/Visual_Prediction\"\n",
    "os.makedirs(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataBase Variables\n",
    "\n",
    "image_folder_path       = 'data/data_trajpred/'+dataset_name\n",
    "DB_PATH_train     = \"./data/data_trajpred/\"+dataset_name+\"/pos_data_train.db\"\n",
    "cnx_train         = sqlite3.connect(DB_PATH_train)\n",
    "DB_PATH_val     = \"./data/data_trajpred/\"+dataset_name+\"/pos_data_val.db\"\n",
    "cnx_val         = sqlite3.connect(DB_PATH_val)\n",
    "DB_DIR      = run_folder + '/database'\n",
    "os.makedirs( DB_DIR )\n",
    "DB_PATH2    = DB_DIR+'/db_one_ped_delta_coordinates_results.db'\n",
    "cnx2        = sqlite3.connect(DB_PATH2)\n",
    "\n",
    "#Other variables\n",
    "T_obs                   = 8\n",
    "T_pred                  = 12\n",
    "T_total                 = T_obs + T_pred #8+12=20\n",
    "data_id                 = 0 \n",
    "batch_size              = 32\n",
    "chunk_size              = batch_size * T_total # Chunksize should be multiple of T_total\n",
    "in_size                 = 2\n",
    "stochastic_out_size     = in_size * 2\n",
    "hidden_size             = 256 #!64\n",
    "embed_size              = 64 #16 #!64\n",
    "global dropout_val\n",
    "dropout_val             = 0.2 #0.5\n",
    "teacher_forcing_ratio   = 0.7 # 0.9\n",
    "regularization_factor   = 0.5 # 0.001\n",
    "avg_n_path_eval         = 20\n",
    "bst_n_path_eval         = 20\n",
    "path_mode               = \"top5\" #\"avg\",\"bst\",\"single\",\"top5\"\n",
    "regularization_mode     = \"regular\" #\"weighted\",\"e_weighted\", \"regular\"\n",
    "startpoint_mode         = \"on\" #\"on\",\"off\"\n",
    "enc_out                 = \"on\" #\"on\",\"off\"\n",
    "biased_loss_mode        = 0 # 0 , 1\n",
    "\n",
    "\n",
    "table_out   = \"results_delta\"\n",
    "table       = \"dataset_T_length_20delta_coordinates\" #\"dataset_T_length_\"+str(T_total)+\"delta_coordinates\"\n",
    "df_id       = pd.read_sql_query(\"SELECT data_id FROM \"+table, cnx_train) #juste la colonne data_id de database\n",
    "#print(\"dfshape\",df_id.shape) #[4920 rows x 1 columns]\n",
    "data_size   = df_id.data_id.max() * T_total\n",
    "epoch_num   = 400\n",
    "from_epoch  = 0\n",
    "\n",
    "#Visual Variables\n",
    "image_size              = 256  \n",
    "image_dimension         = 3\n",
    "mask_size               = 16\n",
    "visual_features_size    = 128 \n",
    "visual_embed_size       = 64  #128 #256 #64\n",
    "vsn_module_out_size    = 256\n",
    "to_pil = torchvision.transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = run_folder + \"/NNmodel\" \n",
    "os.makedirs(model_path)   \n",
    "model_path = model_path + str(\"/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homography Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "# la matrice fournie par eux\n",
    "if dataset_name == 'eth' or dataset_name =='hotel':   # ETH dataset\n",
    "    h = np.array([[0.0110482,0.000669589,-3.32953],[-0.0015966,0.0116324,-5.39514],[0.000111907,0.0000136174,0.542766]])\n",
    "else:                                       # UCY dataset\n",
    "    h = np.array([[47.51,0,476],[0,41.9,117],[0,0,1]])\n",
    "\"\"\"\"\"\n",
    "# Matrice Standard\n",
    "if dataset_name == 'eth':  \n",
    "    h = np.array([[2.8128700e-02, 2.0091900e-03, -4.6693600e+00],[8.0625700e-04, 2.5195500e-02, -5.0608800e+00],[ 3.4555400e-04, 9.2512200e-05, 4.6255300e-01]]) #h pour eth\n",
    "if dataset_name =='hotel':        \n",
    "    h = np.array([[1.1048200e-02, 6.6958900e-04, -3.3295300e+00],[-1.5966000e-03, 1.1632400e-02, -5.3951400e+00],[1.1190700e-04, 1.3617400e-05, 5.4276600e-01]]) #h pour hotel\n",
    "if dataset_name =='zara01': \n",
    "    h = np.array([[0.02174104, 0, -0.15],[0, -0.02461883, 13.77429807],[0, 0, 1]]) #h pour zara1\n",
    "if dataset_name =='zara02': \n",
    "    h = np.array([[0.02174104, 0, -0.4],[0, -0.02386598, 14.98401686],[0, 0, 1]]) #h pour zara2\n",
    "if dataset_name =='univ':\n",
    "    h = np.array([[0.02220407, 0, -0.48],[0, -0.02477289, 13.92551292],[0, 0, 1]]) #h pour univ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Sequential Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryPredictionDataset(torch.utils.data.Dataset): \n",
    "#Enc.cinématique reçoit la trajectoire observée de humain cible (input) de la forme T=(u1,u2-u1,u3-u2,..) qui consiste en les coordonnées de la position de départ et en les déplacements relatifs de l'humain entre les images consécutives.\n",
    "#Ce format a été choisi car il permet au modèle de mieux capturer les similarités entre des trajectoires presque identiques qui peuvent avoir des points de départ différents. \n",
    "    def __init__(self, ROOT_DIR, DB_PATH, cnx):\n",
    "        \n",
    "        self.pos_df    = pd.read_sql_query(\"SELECT * FROM \"+str(table), cnx)\n",
    "        self.root_dir  = ROOT_DIR+'/visual_data'\n",
    "        self.transform = torchvision.transforms.Compose([torchvision.transforms.Resize((image_size,image_size)), \\\n",
    "                                                         torchvision.transforms.ToTensor(), \\\n",
    "                                                         torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "        self.visual_data = []\n",
    "        # read sorted frames\n",
    "        for img in sorted(os.listdir(self.root_dir)): \n",
    "            self.visual_data.append(self.transform( Image.open(os.path.join(self.root_dir)+\"/\"+img) ))\n",
    "        self.visual_data = torch.stack(self.visual_data)  \n",
    "        #print(\"visual_data:\", self.visual_data) #tensor\n",
    "        #print(\"shapevisual_data:\", self.visual_data.shape) #torch.Size([1298, 3, 256, 256]) : 1298 blocs dans chaque bloc 3 sous blocs, et dans chaque sous bloc: 256 lignes et 256 colonnes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pos_df.data_id.max() #data_id maximum dans dataset\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist() \n",
    "        #print(\"idx :\", idx)\n",
    "        \n",
    "        extracted_df     = self.pos_df[ self.pos_df[\"data_id\"] == idx ] #table dont data_id=idx\n",
    "\n",
    "        tensor           = torch.tensor(extracted_df[['pos_x_delta','pos_y_delta']].values).reshape(-1,T_total,in_size) #juste pos_x_delta et pos_y_delta de extracted_df (tensor)\n",
    "        #print(\"tensor: \", tensor)\n",
    "        obs, pred        = torch.split(tensor,[T_obs,T_pred],dim=1) #obs de 8 et pred de 12 à partir de tensor construit\n",
    "        #print(\"obs: \", obs)\n",
    "        #print(\"pred: \", pred)\n",
    "\n",
    "        start_frames     = (extracted_df.groupby('data_id').frame_num.min().values/10).astype('int') #extracted_df dont data_id=idx, on prend minimum frame_num et aprés on divise par 10, cela represente start_frame\n",
    "        extracted_frames = []\n",
    "        for i in start_frames:            \n",
    "            extracted_frames.append(self.visual_data[i:i+T_obs])\n",
    "        frames = torch.stack(extracted_frames) #stack concatenates a sequence of tensors along a new dimension.\n",
    "        start_frames = torch.tensor(start_frames) #tensor([start_frames])\n",
    "        return obs, pred, frames, start_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize random weights for NN models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizer loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance_from_line_regularizer function is implemented as a regularization method to enforce the predicted trajectory to stay close to the observed trajectory. The method is used to minimize the Euclidean distances between each step of the predicted trajectory and a line fitted to the observed trajectory.\n",
    "\n",
    "It takes 2 inputs:\n",
    "1-input_tensor : The observed trajectory in the form of a tensor of size (batch_size, T_obs, 2)\n",
    "2-prediction : The predicted trajectory in the form of a tensor of size (batch_size, T_pred, 2)\n",
    "\n",
    "The function first converts the input tensors to double precision, then it calculates the cumulative sum of the observed trajectory along the time axis. Next, it fits a line to the observed trajectory by calculating the slope and intercept of the line using the least square method. Then it calculates the real values of the predicted trajectory by adding the last point of the observed trajectory to the delta values predicted by the model. Next, it calculates the distance between each predicted point and the fitted line using the formula distance = |(theta0*x0+theta1) - y0| / sqrt(theta0^2 + 1) where x0, y0 are the coordinates of the predicted point, theta0, theta1 are the slope and intercept of the line respectively.\n",
    "\n",
    "Finally, it applies a weighting scheme to the distances based on the regularization_mode:\n",
    "weighted : The weights are linearly decreasing from T_pred to 1\n",
    "e_weighted : The weights are exponentially decreasing from T_pred to 1\n",
    "None : No weighting\n",
    "\n",
    "It then calculates the mean distance over all the predicted points and returns it as sum_sigma_distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_sigma_distance  = torch.zeros(1) #tensor[(0.)]\n",
    "\n",
    "def distance_from_line_regularizer(input_tensor,prediction):\n",
    "    global sum_sigma_distance\n",
    "    #the regularization is defined as the sum of Euclidean distances between each step of the predicted trajectory Tf , and a line fitted to the observed trajectory To.\n",
    "    # Fit a line to observation points over batch \n",
    "    input_tensor    = input_tensor.double()\n",
    "    prediction      = prediction.double()\n",
    "    input_tensor    = input_tensor.cumsum(dim=1).double()\n",
    "    X               = torch.ones_like(input_tensor).double().to('cuda', non_blocking=True)\n",
    "    X[:,:,0]        = input_tensor[:,:,0]\n",
    "    Y               = (input_tensor[:,:,1]).unsqueeze(-1).double()\n",
    "    try:\n",
    "        try:\n",
    "            XTX_1           = torch.matmul( X.transpose(-1,-2), X).double().inverse()\n",
    "        except:\n",
    "            XTX_1           = torch.matmul( X.transpose(-1,-2), X).double().pinverse()\n",
    "        XTY             = torch.matmul( X.transpose(-1,-2), Y)\n",
    "        theta           = torch.matmul( XTX_1.double(), XTY.double())\n",
    "        # Calculate real values of prediction instead of delta\n",
    "        prediction[:,:,0] = prediction[:,:,0] + input_tensor[:,-1,0].unsqueeze(-1) \n",
    "        prediction[:,:,1] = prediction[:,:,1] + input_tensor[:,-1,1].unsqueeze(-1)\n",
    "        \n",
    "        # Calculate distance ( predicted_points , observation_fitted_line ) over batch\n",
    "        theta0x0        = theta[:,0,:].double() * prediction[:,:,0].double()\n",
    "        denominator     = torch.sqrt( theta[:,0,:].double() * theta[:,0,:].double() + 1 )\n",
    "        nominator       = theta0x0 + theta[:,1,:] - prediction[:,:,1].double()\n",
    "        distance        = nominator.abs() / denominator\n",
    "        if regularization_mode =='weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().float(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(-1,T_pred)\n",
    "            weighted_distance   = weight * distance\n",
    "\n",
    "        elif regularization_mode =='e_weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().float(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(distance.size(0),T_pred)\n",
    "            weight              = torch.exp(weight)\n",
    "            weighted_distance   = weight*distance\n",
    "\n",
    "        else:\n",
    "            weighted_distance = distance\n",
    "        sigma_distance  = torch.mean(weighted_distance,1)\n",
    "        sum_sigma_distance  = torch.mean(sigma_distance)\n",
    "        return sum_sigma_distance\n",
    "    except:\n",
    "        print(\"SINGULAR VALUE\")\n",
    "        sum_sigma_distance = torch.zeros(1).to('cuda', non_blocking=True) + 20\n",
    "        return sum_sigma_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (partie vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Features Extraction (Resnet 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from torchvision import datasets \n",
    "from torchvision.transforms import ToTensor \n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet function creates a ResNet model with the option to use a pretrained model from ImageNet, and to select the number of layers to unfreeze and delete.\n",
    "The function also allows to define the number of input channels for the images (1, 2 or 3). It returns the ResNet model.\n",
    "\n",
    "The function takes 4 parameters as input:\n",
    "1-pretrain: a boolean value that determines whether to use a pretrained model from ImageNet or not. The default value is True, which means the model will be pretrained.\n",
    "2-layers_to_unfreeze: an integer value that defines the number of layers at the end of the ResNet model that will be unfrozen for training. The default value is 8.\n",
    "3-layers_to_delete: an integer value that defines the number of layers at the end of the ResNet model that will be deleted from the model. The default value is 2.\n",
    "4-in_planes: an integer value that defines the number of input channels for the images. The supported values are 1, 2, or 3. The default value is 3.\n",
    "\n",
    "The function starts by initializing the ResNet model with the specified pretrained setting, then creates a new nn.Sequential model to store the modified layers. It calculates the total number of layers in the ResNet model, and if this number is less than the number of layers to unfreeze, it sets the number of layers to unfreeze to the total number of layers. The function then loops through the children layers of the ResNet model and for the first layer it modifies the number of input channels to match the specified number of input channels, in case the input channel is 1 or 2, it modify the weight of the conv layer accordingly. Then the function applies freezing on some layers depending on the layers_to_freeze and layers_to_delete parameters. It appends the child layers to the new model if it's in the range of number_of_layers. Finally, it returns the new model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script defines a features_extraction class that inherits from the PyTorch nn.Module class. The class is used to extract features from an input image using a convolutional model (in this case, ResNet) and a pooling operation.\n",
    "The class takes the convolutional model and the number of input channels for the images as input. The class overrides the forward method to reshape the input image, pass it through the convolutional model, and then apply the pooling operation. The resulting feature map is then returned.\n",
    "\n",
    "The class takes 2 arguments in the constructor:\n",
    "1-conv_model: the convolutional model that will be used for feature extraction. Currently, only ResNet is supported.\n",
    "2-in_planes: the number of input channels for the images.\n",
    "The class also creates an AdaptiveAvgPool2d object with the shape (1,1) to be used for the pooling operation.\n",
    "\n",
    "The class overrides the forward method, which takes an input image, reshape the image to have the number of channels as defined by the in_planes parameter, pass it through the convolutional model, and then apply the pooling operation. The resulting feature map is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet(pretrain=True,layers_to_unfreeze=8,layers_to_delete=2,in_planes=3):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        pretrain: Define if we load a pretrained model from ImageNet\n",
    "        layers_to_unfreeze: Define the number of layers that we want to train at the end of the Resnet\n",
    "        layers_to_delete: Define the numbers of layers that we want to delete\n",
    "        in_planes: Define the numbers of input channels of images (supported values: 1,2 or 3)\n",
    "    return: The Resnet model\n",
    "    \"\"\"\n",
    "    resnet = torchvision.models.resnet18(pretrained=pretrain)\n",
    "    # Create a new model cause we don't want the pooling operation at the end and the classifier\n",
    "    model = nn.Sequential()\n",
    "    number_of_layers = len(list(resnet.children())) - layers_to_delete # In practice it remove the pooling operation and the classifier\n",
    "\n",
    "    if number_of_layers<layers_to_unfreeze:\n",
    "        layers_to_unfreeze = number_of_layers\n",
    "    layers_to_freeze = number_of_layers - layers_to_unfreeze\n",
    "    i=0\n",
    "    for child in resnet.children():\n",
    "        # For the first layers we create a new weight if in_planes is not 3 cause ResNet is pretrain on image with 3 channels there is no version for 1 channel\n",
    "        if i==0 and in_planes<3:\n",
    "            if i<layers_to_freeze: # Define if we freeze this layer or no\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False # Freeze the layers by passing requires_grad attribute to False\n",
    "            w = child._parameters['weight'].data # Get the weight for 3 channels data\n",
    "            child._modules['0'] = nn.Conv2d(in_planes, 64, kernel_size=3, padding=1) # Define the new conv layer\n",
    "            if in_planes == 1:\n",
    "                child._parameters['weight'].data = w.mean(dim=1, keepdim=True) # If the number of channels is 1 we made the mean of channels to set the new weight\n",
    "            else:\n",
    "                child._parameters['weight'].data = w[:, :-1] * 1.5\n",
    "\n",
    "        if i<layers_to_freeze: # Define if we freeze this layer or no\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False # Freeze the layers by passing requires_grad attribute to False\n",
    "        if i<number_of_layers: # To define if we keep this layer or not\n",
    "            model.append(child) \n",
    "        i+=1\n",
    "    return model\n",
    "\n",
    "\n",
    "class features_extraction(nn.Module):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    conv_model: The convolution model used before capsules for the moment only ResNet is supported\n",
    "    in_planes: Numbers of channels for the image\n",
    "    \"\"\"\n",
    "    def __init__(self,conv_model,in_planes: int):\n",
    "        super().__init__()\n",
    "        self.conv_model = conv_model\n",
    "        self.in_planes = in_planes\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "    def forward(self,input):\n",
    "        shape = input.size()\n",
    "        x = input.view(-1,self.in_planes,shape[-2],shape[-1])\n",
    "        x = self.conv_model(x)\n",
    "        x = self.pooling(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position_embedding function takes 2 arguments as input:\n",
    "1-input: a tensor representing the input of the function, it's a 1-D tensor with the shape of (batch_size,)\n",
    "2-d_model: an integer value representing the depth of the model, it's used to define the size of the output tensor.\n",
    "The function starts by reshaping the input tensor to have a shape of (-1, 1) and creating a 1-D tensor called dim of shape (d_model/2,). Then it creates two tensors, sin and cos, by applying sin and cos functions on the input tensor element-wise with a scaling factor of 2 * dim / d_model. The function then creates an output tensor of shape (batch_size, d_model) and assigns the values of sin and cos tensors to the even and odd indices respectively. Finally, it returns the output tensor.\n",
    "\n",
    "The sinusoid_encoding_table function takes 2 arguments as input:\n",
    "1-max_len: an integer value representing the maximum length of the input tensor that the function will be applied on.\n",
    "2-d_model: an integer value representing the depth of the model, it's used to define the size of the output tensor.\n",
    "The function starts by creating a 1-D tensor called pos of shape (max_len) with integer values ranging from 0 to max_len-1, then it applies the position_embedding function on the pos tensor with the d_model value passed to the function as an argument. Finally, it returns the output of the position_embedding function.\n",
    "\n",
    "These functions are used in the Transformer model to add position information to the input embeddings. The sinusoid_encoding_table function is used to create a lookup table of position embeddings, which is then used to add position information to the input embeddings in the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_embedding(input, d_model):\n",
    "    input = input.view(-1, 1)\n",
    "    dim = torch.arange(d_model // 2, dtype=torch.float32, device=input.device).view(1, -1)\n",
    "    sin = torch.sin(input / 10000 ** (2 * dim / d_model))\n",
    "    cos = torch.cos(input / 10000 ** (2 * dim / d_model))\n",
    "\n",
    "    out = torch.zeros((input.shape[0], d_model), device=input.device)\n",
    "    out[:, ::2] = sin\n",
    "    out[:, 1::2] = cos\n",
    "    return out\n",
    "\n",
    "def sinusoid_encoding_table(max_len, d_model):\n",
    "    pos = torch.arange(max_len, dtype=torch.float32)\n",
    "    out = position_embedding(pos, d_model)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScaledDotProductAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script defines a ScaledDotProductAttention class that inherits from the PyTorch nn.Module class. This class implements the scaled dot-product attention mechanism as described in the Transformer model.\n",
    "\n",
    "The class takes 4 parameters in the constructor:\n",
    "1-d_model: the output dimensionality of the model\n",
    "2-d_k: the dimensionality of queries and keys\n",
    "3-d_v: the dimensionality of values\n",
    "4-h: the number of heads\n",
    "\n",
    "It then initializes four nn.Linear modules, fc_q, fc_k, fc_v and fc_o, which are used to project the queries, keys, values, and the final output, respectively. It also defines an init_weights function which is used to initialize the weights of the linear layers using the Xavier initialization method.\n",
    "\n",
    "The forward function takes three inputs, queries, keys, and values, and applies the scaled dot-product attention mechanism on them. It starts by reshaping the inputs to have the shape (batch_size, num_queries, h, d_k/d_v) and permutes the dimensions to have the shape (batch_size, h, num_queries/d_k, d_k/d_v) for queries, keys, and values respectively. Then it computes the dot product between the queries and keys and scales it by 1/sqrt(d_k) and applies a softmax function on the resulting tensor to get the attention weights. Then, it takes a weighted sum of the values tensor with the attention weights to get the final output. Finally, it applies the final linear layer fc_o on the output and returns it.\n",
    "\n",
    "This class is used in the transformer model as one of the building blocks to compute the attention scores between the input and output sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h):\n",
    "        \"\"\"\n",
    "        param:\n",
    "        d_model: Output dimensionality of the model\n",
    "        d_k: Dimensionality of queries and keys\n",
    "        d_v: Dimensionality of values\n",
    "        h: Number of heads\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "        self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.init_weights(gain=1.0)\n",
    "\n",
    "    def init_weights(self, gain=1.0):\n",
    "        nn.init.xavier_normal_(self.fc_q.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_k.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_v.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_o.weight, gain=gain)\n",
    "        nn.init.constant_(self.fc_q.bias, 0)\n",
    "        nn.init.constant_(self.fc_k.bias, 0)\n",
    "        nn.init.constant_(self.fc_v.bias, 0)\n",
    "        nn.init.constant_(self.fc_o.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_model)\n",
    "        :param keys: Keys (b_s, nk, d_model)\n",
    "        :param values: Values (b_s, nk, d_model)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "\n",
    "        att = torch.softmax(att, -1)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script defines a MultiHeadAttention class that inherits from the PyTorch nn.Module class. This class implements the multi-head attention mechanism as described in the Transformer model.\n",
    "\n",
    "The class takes 6 parameters in the constructor:\n",
    "1-d_model: the output dimensionality of the model\n",
    "2-d_k: the dimensionality of queries and keys\n",
    "3-d_v: the dimensionality of values\n",
    "4-h: the number of heads\n",
    "5-dff: the dimensionality of the feed-forward layer\n",
    "6-dropout: the dropout rate for the dropout layers\n",
    "\n",
    "It then initializes an instance of the ScaledDotProductAttention class, a dropout layer, a layer normalization layer, and a feed-forward layer which is implemented using two linear layers with a ReLU activation in between them.\n",
    "\n",
    "The forward function takes three inputs, queries, keys, and values, and applies the multi-head attention mechanism on them. It starts by applying the attention mechanism on the inputs using the attention instance. Then it applies the dropout layer on the resulting tensor. Then it applies the feed-forward layer on the resulting tensor. Finally, it applies dropout layer again and then applies the layer normalization on the resulting tensor. It then sums the resulting tensor with the queries tensor and returns it.\n",
    "\n",
    "This class is used in the transformer model as one of the building blocks to compute the attention scores between the input and output sequences. By using multi-head attention, the transformer can attend to different parts of the input sequence in parallel for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Multi-head attention layer with Dropout and Layer Normalization\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h, dff=2048, dropout=.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(d_model=d_model, d_k=d_k, d_v=d_v, h=h)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Sequential(*[nn.Linear(d_model, dff), nn.ReLU(inplace=True), nn.Dropout(p=dropout),nn.Linear(dff, d_model)])\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        att = self.attention(queries, keys, values)\n",
    "        att = self.dropout(att)\n",
    "        att = self.fc(att)\n",
    "        att = self.dropout(att)\n",
    "        return self.layer_norm(queries + att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EncoderSelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above defines a class for the Encoder Self-Attention module. The main goal of this module is to perform self-attention on the input tensor and output the encoded tensor. The EncoderSelfAttention class takes in several parameters such as device, d_model, d_k, d_v, n_head, dff, dropout_transformer, and n_module.\n",
    "\n",
    "The device parameter is used to specify the device that the tensors will be allocated on (CPU or GPU). d_model, d_k, d_v, and n_head are hyperparameters used in the MultiHeadAttention module. dff is the dimensionality of the feedforward network and dropout_transformer is the dropout rate applied to the output of the MultiHeadAttention module. n_module is the number of MultiHeadAttention modules stacked in the encoder.\n",
    "\n",
    "The forward method takes in the input tensor x, and applies the sinusoid_encoding_table to it, which is used to add positional encoding to the input tensor. Then it applies the MultiHeadAttention module n_module times to the input tensor, and returns the encoded tensor.\n",
    "\n",
    "This is applying the MultiHeadAttention module to the input tensor \"in_encoder\" multiple times. The input tensor is passed as the queries, keys and values to the MultiHeadAttention module and the output of the MultiHeadAttention is then passed back as the input to the next MultiHeadAttention module in the nn.ModuleList \"self.encoder\" through the for loop. This process is repeated n_module times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderSelfAttention(nn.Module):\n",
    "    def __init__(self, device, d_model, d_k, d_v, n_head, dff=2048, dropout_transformer=.1, n_module=6):\n",
    "        super(EncoderSelfAttention, self).__init__()\n",
    "        self.encoder = nn.ModuleList([MultiHeadAttention(d_model, d_k, d_v, n_head, dff, dropout_transformer) for _ in range(n_module)])\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"x shape: {}\".format(x.shape)) #torch.Size([32, 8, 512])\n",
    "        \n",
    "        in_encoder = x + sinusoid_encoding_table(x.shape[1], x.shape[2]).expand(x.shape).to(self.device)\n",
    "        #print(\"in_encoder shape: {}\".format(in_encoder.shape)) #torch.Size([32, 8, 512])\n",
    "        \n",
    "        for l in self.encoder:\n",
    "            in_encoder = l(in_encoder, in_encoder, in_encoder)\n",
    "            #print(\"in_encoder shape: {}\".format(in_encoder.shape)) #torch.Size([32, 8, 512])\n",
    "            \n",
    "        return in_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie vision : Resnet + Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a class definition for the _GestureTransformer model, which is a multi-modal model that can take in either 3 or 1 channel images as input. \n",
    "The model uses a convolutional backbone, such as ResNet, to extract features from the input images, followed by a self-attention mechanism implemented using the EncoderSelfAttention module. The final output of the model is obtained by applying a global average pooling operation on the output of the self-attention mechanism. \n",
    "The constructor of the class takes in several parameters, such as the device to run the model on, the choice of convolutional backbone, the number of input channels, whether to use a pre-trained model, the input dimension, the number of layers to unfreeze, the number of layers to delete, the number of attention heads, the number of attention modules, the feed-forward size and the dropout rate for the 1D dropout. \n",
    "Overall, the _GestureTransformer model is designed to perform feature extraction and self-attention on the input images in order to identify gestures in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class _GestureTransformer(nn.Module):\n",
    "    \"\"\"Multi-Modal model on 3 or 1 channel\"\"\"\n",
    "    def __init__(self,device,backbone=\"resnet\",in_planes=3,pretrained= True,input_dim=512,layers_to_unfreeze=8,layers_to_delete=2,n_head=8,n_module=6,ff_size=1024,dropout1d=0.5):\n",
    "        super(_GestureTransformer, self).__init__()\n",
    "\n",
    "        self.in_planes = in_planes\n",
    "        self.device = device\n",
    "        self.conv_name = backbone\n",
    "        self.conv_model = None\n",
    "        \n",
    "        if self.conv_name.lower()==\"resnet\":\n",
    "            self.conv_model = Resnet(pretrained,layers_to_unfreeze,layers_to_delete,in_planes)\n",
    "        else:\n",
    "            raise NotImplementedError(\"The model {} is not supported!\".format(self.conv_name))\n",
    "            \n",
    "        self.conv_model.to(device)\n",
    "        self.features = features_extraction(self.conv_model,in_planes)\n",
    "\n",
    "        self.self_attention = EncoderSelfAttention(device,input_dim,64,64,n_head=n_head,dff=ff_size,dropout_transformer=dropout1d,n_module=n_module)\n",
    "\n",
    "        #self.pool = nn.AdaptiveAvgPool2d((,input_dim)) #final pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        shape = x.shape\n",
    "        #print(\"x avant encoder shape: {}\".format(x.shape)) # torch.Size([32, 8, 3, 256, 256])\n",
    "        \n",
    "        x = self.features(x)\n",
    "        #print(\"x avant encoder shape: {}\".format(x.shape)) #torch.Size([256, 512, 1, 1])\n",
    "        \n",
    "        x = x.view(shape[0],shape[1],-1)\n",
    "        #print(\"x avant encoder shape: {}\".format(x.shape)) #torch.Size([32, 8, 512])\n",
    "        \n",
    "        x = self.self_attention(x)\n",
    "        #print(\"x avant encoder shape: {}\".format(x.shape)) #torch.Size([32, 8, 512])\n",
    "\n",
    "        #x = self.pool(x).squeeze(dim=1) #final pooling\n",
    "        #print(\"x avant encoder shape: {}\".format(x.shape)) #torch.Size([32, 512])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Transformer (partie cinématique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CoordinatesTransformer(nn.Module):\n",
    "    \"\"\"Multi-Modal model on 3 or 1 channel\"\"\"\n",
    "    def __init__(self,device,pretrained= True,input_dim=512,layers_to_unfreeze=8,layers_to_delete=2,n_head=8,n_module=6,ff_size=1024,dropout1d=0.5):\n",
    "        super(CoordinatesTransformer, self).__init__()\n",
    "        \n",
    "        # self.conv_model.to(device)\n",
    "        self.linear_mapper= torch.nn.Sequential(\n",
    "                              torch.nn.Linear(2, 32),\n",
    "                              torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(32, 64),\n",
    "                              torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(64, 128),\n",
    "                              torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(128, 256),\n",
    "                              torch.nn.ReLU(),)\n",
    "        \n",
    "        \n",
    "        self.self_attention = EncoderSelfAttention(device,256,64,64,n_head=n_head,dff=ff_size,dropout_transformer=dropout1d,n_module=n_module)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,256)) #final pooling\n",
    "        \n",
    "        self.fc_out = nn.Sequential(nn.Linear(256,256),nn.ReLU(),nn.Dropout(p=dropout_val))\n",
    "        \n",
    "        self.embedder_out = nn.Sequential(nn.Linear(8*256, 256),nn.ReLU(),nn.Dropout(p=dropout_val),nn.Linear(256, 256),nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        #print(\"x avant:\", x.shape) #torch.Size([40, 1, 2])\n",
    "        x=self.linear_mapper(x)\n",
    "        #print(\"x apres linear:\", x.shape) #torch.Size([40, 1, 256])\n",
    "        x = x.view(shape[0],shape[1],-1)\n",
    "        #print(\"x apres view:\", x.shape) #torch.Size([40, 1, 256])\n",
    "        x = self.self_attention(x)\n",
    "        #print(\"x apres attention\", x.shape) #torch.Size([40, 1, 256])\n",
    "        x = self.fc_out(x)\n",
    "        #x = self.pool(x).squeeze(dim=1) #final pooling\n",
    "        #print(\"x apres pool\", x.shape) #torch.Size([40, 256])\n",
    "        return x\n",
    "    \n",
    "    def emb_out(self,input):\n",
    "        #print(\"input shape avant embedder out: {}\".format(input.shape)) #torch.Size([40, 2048])\n",
    "        out= self.embedder_out(input)\n",
    "        #print(\"out shape apres embedder out: {}\".format(out.shape)) #torch.Size([40, 256])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a decoder part of a transformer-based architecture for a gesture recognition task, where it takes in the current time step's input and the encoded outputs from the encoder, processes them through multi-head self-attention, feed-forward layers and linear layers, and produces an output. The input is first passed through an embedding layer to get a 64-dimensional embedding, and then processed through a stack of num_layers DecoderLayer modules, each consisting of multi-head self-attention and feed-forward layers. Finally, the output from the last layer is passed through a linear layer to produce the final output of the decoder. The final output's size is 514, which can be used for gesture recognition.\n",
    "\n",
    "This is the implementation of a single layer of the decoder in a transformer architecture. \n",
    "The decoder layer consists of 2 main components: multi-head self-attention and feed-forward neural network.\n",
    "\n",
    "The multi-head self-attention mechanism allows the decoder to consider different parts of the input sequence at different positions, and to weigh the importance of each part for the current position. This is done by computing the attention weights between the input sequence and itself, and using those weights to compute a weighted sum of the input sequence. The result is then added to the input to obtain the attention output.\n",
    "\n",
    "The feed-forward neural network is a simple linear neural network that transforms the input into a new representation. In this case, it consists of a linear layer followed by a ReLU activation function. The output of the feed-forward network is then added to the input, and the result is passed through two layer normalization layers. This final output is the output of the decoder layer.\n",
    "\n",
    "The x variable passed as input is the input to the decoder, and the e_output variable is the output of the encoder. This variable is used in the attention mechanism to compute the attention weights between the input and the encoder output.\n",
    "\n",
    "the DecoderTransformer class is a decoder for a transformer-based model. The class initializes various linear and non-linear layers, and defines the forward pass. The forward pass starts by taking the input x, which is the current position of the agent, and passing it through a linear layer self.embedder_rho to create an embedding. This embedding is then passed through a ReLU activation function and a dropout layer. The class also defines several other layers, including self.fC_mu, self.FC_dim_red, self.embedding, self.layers and self.output_layer. These layers are used in the forward pass to perform various computations, such as multihead self-attention and feed forward, on the input and the encoder outputs. Finally, the class uses a linear layer self.fC_mu to produce the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, e_output):\n",
    "        # Multihead self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = x + self.norm1(attn_output)\n",
    "        \n",
    "        # Feed forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.norm2(ff_output)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, in_size, embed_size, hidden_size, d_model=512, dropout_val=dropout_val, batch_size=1, nhead=8, num_layers=6):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "                \n",
    "        self.in_size                = in_size #2\n",
    "        self.stochastic_out_size    = stochastic_out_size #2*2=4\n",
    "        self.hidden_size            = hidden_size #256\n",
    "        self.batch_size             = batch_size\n",
    "        self.embed_size             = embed_size #64\n",
    "        self.seq_length             = T_pred #12\n",
    "        self.dropout_val            = dropout_val #0.2\n",
    "        self.visual_embed_size      = visual_embed_size #64\n",
    "        self.visual_embed_size      = visual_embed_size\n",
    "        self.visual_size            = image_dimension * image_size * image_size #3*256*256\n",
    "        \n",
    "        self.d_model=d_model\n",
    "        self.nhead=nhead\n",
    "        self.num_layers=num_layers\n",
    "        \n",
    "        self.embedder_rho = nn.Linear(self.in_size, self.embed_size) #(2,64)\n",
    "        self.fC_mu = nn.Sequential(nn.Linear(self.hidden_size + self.hidden_size + in_size, int(self.hidden_size/2), bias=True),nn.ReLU(),nn.Dropout(p=dropout_val),nn.Linear(int(self.hidden_size/2), self.stochastic_out_size, bias=True))\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.reducted_size = int((self.hidden_size-1)/3)+1\n",
    "        self.reducted_size2 = int((self.hidden_size+in_size-1)/3)+1\n",
    "        self.FC_dim_red = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=3, padding=1),nn.Flatten(start_dim=1, end_dim=-1),nn.Linear(self.reducted_size*self.reducted_size2, 2*self.hidden_size+in_size, bias=True),nn.ReLU())\n",
    "        \n",
    "        self.embedding = nn.Linear(64, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, nhead) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, 514)\n",
    "                \n",
    "    def forward(self, x, encoder_outputs):\n",
    "        \n",
    "        #print(\"x forward shape: {}\".format(x.shape)) #torch.Size([32, 2]) ok\n",
    "\n",
    "        # Coordination Embedding\n",
    "        embedding = self.embedder_rho(x.view(x.shape[0],-1,2)) #torch.Size([32, 64]) ok\n",
    "        #print(\"embedding1 forward shape: {}\".format(embedding.shape))\n",
    "        embedding = F.relu(self.dropout(embedding))\n",
    "        #print(\"embedding2 forward shape: {}\".format(embedding.shape)) #torch.Size([32, 64]) ok\n",
    "\n",
    "        # Embed the decoder input\n",
    "        x = self.embedding(embedding)\n",
    "        #print(\"embedding unsqueeze forward shape: {}\".format(embedding.shape)) #torch.Size([32, 1, 64]) ok\n",
    "\n",
    "        #x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_outputs)\n",
    "        output = self.output_layer(x)\n",
    "        #print(\"output forward shape: {}\".format(output.shape)) #torch.Size([32, 1, 514]) ok\n",
    "        \n",
    "        #print(\"output squeeze forward shape: {}\".format(output.squeeze(0).shape)) #output squeeze forward shape: torch.Size([32, 1, 514])\n",
    "        prediction = self.fC_mu(output.squeeze(0)) \n",
    "        #print(\"prediction forward shape: {}\".format(prediction.shape)) #torch.Size([32, 1, 4]) ok\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def dim_red(self, input):\n",
    "        #print(\"input DIM_RED shape: {}\".format(input.shape)) #torch.Size([32, 257, 257])\n",
    "        output = self.FC_dim_red(input)\n",
    "        #print(\"output DIM_RED shape: {}\".format(output.shape)) #torch.Size([32, 514])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a PyTorch implementation of a sequence-to-sequence model, which is composed of several neural network modules. \n",
    "\n",
    "The Seq2Seq class has several components:\n",
    "1-An encoder module (encoder) which is a Transformer encoder that takes in the input_tensor and encodes it to a fixed-size representation. It has multi-head self-attention mechanism, layer normalization, dropout, and feed-forward neural network as sub-layers.\n",
    "2-An encoder module (encoderrnn) which is a RNN encoder that takes in the input_tensor and encodes it to a fixed-size representation. It has coordination embedding, RNN, and Linear sub-layers.\n",
    "3-A decoder module (decoder) which is a transformer decoder that takes the encoded representation and generates the output_tensor.\n",
    "4-A visual module (vsn_module) that takes visual_input_tensor and encodes it to a fixed-size representation.\n",
    "5-A pooling layer (pooling) that applies pooling on the encoder_outputs.\n",
    "6-A linear layer (out) which is applied on the encoded representations to produce the final output.\n",
    "\n",
    "In the forward method, the input_tensor and visual_input_tensor are passed through the encoder, encoder_rnn, and visual module respectively. Then the pooled outputs are passed through the decoder to generate the output_tensor. Finally, the encoded representations are passed through the linear layer to produce the final output.\n",
    "\n",
    "the Seq2Seq class takes several inputs:\n",
    "1-in_size: The size of the input features.\n",
    "2-embed_size: The size of the embedding for the input features.\n",
    "3-hidden_size: The size of the hidden state for the encoder and decoder.\n",
    "4-batch_size: The batch size of the input data.\n",
    "5-d_model: The desired dimension of the input after passing through the linear layer in the transformer encoder.\n",
    "6-d_ff: The dimension of the feed-forward neural network in the transformer encoder.\n",
    "7-h: Number of heads in the multi-head self-attention mechanism in the transformer encoder.\n",
    "8-dropout_val: The dropout value applied throughout the model.\n",
    "9-N: The number of layers in the transformer encoder.\n",
    "10-input_dim: The dimension of the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, in_size, embed_size, hidden_size, batch_size=1, d_model=512, d_ff=2048, h=8, dropout_val=dropout_val, N=6, input_dim=512):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        self.encoder = CoordinatesTransformer(device,dropout1d=dropout_val) #Encoder Transformer (partie cinématique)\n",
    "        self.encoder.apply(init_weights)\n",
    "                \n",
    "        self.decoder =  DecoderTransformer(in_size, embed_size, hidden_size, num_layers=6, nhead=8)\n",
    "        self.decoder.apply(init_weights)\n",
    "        \n",
    "        self.vsn_module = _GestureTransformer(device,dropout1d=dropout_val) #_GestureTransformer(partie vision)                   \n",
    "        self.vsn_module.apply(init_weights)\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool1d((hidden_size)) #on a ajouté pooling (car prob de shape)\n",
    "        \n",
    "\n",
    "            \n",
    "        self.crossAttention = crossAttention(N=6,\n",
    "                                            d_model=256, d_ff=2048, h=8, dropout=0.1)\n",
    "        \n",
    " \n",
    "        \n",
    "        if device.type=='cuda':\n",
    "            self.encoder.cuda()\n",
    "            self.decoder.cuda()\n",
    "            self.vsn_module.cuda()\n",
    "\n",
    "    def forward(self, input_tensor, visual_input_tensor, output_tensor, batch_size, train_mode): \n",
    "        \n",
    "        #print(\"Visual Input_tensor shape: {}\".format(visual_input_tensor.shape)) #torch.Size([32, 8, 3, 256, 256])\n",
    "        #print(\"Input_tensor shape: {}\".format(input_tensor.shape)) #torch.Size([32, 8, 2])\n",
    "\n",
    "        batch_size      = int(input_tensor.size(0))        \n",
    "        \n",
    "        #encoder_outputs \n",
    "        encoder_outputs = torch.zeros(batch_size, T_obs, hidden_size).cuda()\n",
    "#         print(\"shape of encoder_outputs: {}\".format(encoder_outputs.shape)) #torch.Size([32, 8, 256])\n",
    "\n",
    "        start_point     = (input_tensor[:,0,:]).to(device).clone().detach()\n",
    "\n",
    "        if startpoint_mode==\"on\":\n",
    "            input_tensor[:,0,:]    = 0\n",
    "\n",
    "#         for t in range(0,T_obs):\n",
    "            #print(\"Input_tensor[:,t,:] shape: {}\".format(input_tensor[:,t,:].shape)) #torch.Size([32, 2])\n",
    "            \n",
    "        encoder_outputs                  = self.encoder(input_tensor.reshape(batch_size, -1, 2))\n",
    "        #print(\"shape of encoder_outputs: {}\".format(encoder_outputs.shape))\n",
    "            \n",
    "            #print(\"shape of encoder_output: {}\".format(encoder_output.shape)) #torch.Size([40,256])\n",
    "            #print(\"shape of encoder_outputs[:,t,:]: {}\".format(encoder_outputs[:,t,:].shape)) #torch.Size([40,256])\n",
    "\n",
    "        # Encoder outputs : \n",
    "        \n",
    "        #print(\"shape of encoder_outputs view: {}\".format(encoder_outputs.view(batch_size,-1).shape)) #torch.Size([40, 2048])\n",
    "        #encoder_extract       = self.encoder.emb_out(encoder_outputs.view(batch_size,-1))\n",
    "        #print(\"Shape of encoder extract: {}\".format(encoder_extract.shape)) #torch.Size([40, 256])\n",
    "                    \n",
    "        visual_initial_vsn    = self.vsn_module(visual_input_tensor)\n",
    "        #print(\"shape of visual initial vsn : {}\".format(visual_initial_vsn.shape)) #torch.Size([32, 512])\n",
    "        visual_initial_vsn    = self.pooling(visual_initial_vsn) #pooling qu'on a ajouté\n",
    "        #print(\"shape of visual initial vsn apres pooling : {}\".format(visual_initial_vsn.shape)) ##torch.Size([32, 256])\n",
    "        \n",
    "        src_mask = None\n",
    "        obd_enc_mask = None\n",
    "        \n",
    "        cross_ouput = self.crossAttention(encoder_outputs, visual_initial_vsn, src_mask, obd_enc_mask)\n",
    "        #concat the 2 outputs of encoders\n",
    "#         e_outputs = torch.cat([encoder_extract.view(batch_size,-1),visual_initial_vsn.view(batch_size,-1)],dim=-1)\n",
    "#         #print(\"Shape of e_outputs : {}\".format(e_outputs.shape)) #torch.Size([32, 512])\n",
    "#         e_outputss = torch.cat([encoder_extract.view(batch_size,-1),visual_initial_vsn.view(batch_size,-1)],dim=-1).unsqueeze(0)\n",
    "#         #print(\"Shape of e_outputss : {}\".format(e_outputss.shape)) #torch.Size([1, 32, 512])\n",
    "        e_outputss=cross_ouput\n",
    "        \n",
    "        visual_vsn_result   = visual_initial_vsn\n",
    "        \n",
    "        #decoder_input\n",
    "        decoder_input = input_tensor[:,-1,:]\n",
    "        #print(\"Shape of decoder_input: {}\".format(decoder_input.shape)) #torch.Size([32, 2])\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs                         = torch.zeros(batch_size, T_pred , in_size).cuda() #torch.Size([32, 12, 2])\n",
    "        stochastic_outputs              = torch.zeros(batch_size, T_pred , stochastic_out_size).cuda() #torch.Size([32, 12, 4])\n",
    "        teacher_force                   = 1\n",
    "\n",
    "        epsilonX                        = Normal(torch.zeros(batch_size,1),torch.ones(batch_size,1))\n",
    "        epsilonY                        = Normal(torch.zeros(batch_size,1),torch.ones(batch_size,1))\n",
    "        teacher_force                   = int(random.random() < teacher_forcing_ratio) if train_mode else 0\n",
    "        \n",
    "#         stochastic_outputs = self.decoder(output_tensor, cross_ouput)\n",
    "#         print(stochastic_outputs.size())\n",
    "#         outputs[:,:,0] = stochastic_outputs[:,:,0]\n",
    "#         outputs[:,:,1] = stochastic_outputs[:,:,2]\n",
    "        \n",
    "        for t in range(0, T_pred):\n",
    "            #print(\"Shape of output_tensor[:,t,:]: {}\".format(output_tensor[:,t,:].shape)) #torch.Size([32, 2])\n",
    "            #print(\"Shape of output_tensor[:,t,:] reshape: {}\".format(output_tensor[:,t,:].reshape(batch_size, -1, 2).shape)) #torch.Size([32, 1, 2])\n",
    "            \n",
    "            stochastic_decoder_output = self.decoder(decoder_input, e_outputss)\n",
    "            \n",
    "            #stochastic_decoder_output =  self.fine_grained_fusion(e_outputss)\n",
    "            #print(\"Shape of stochastic_decoder_output: {}\".format(stochastic_decoder_output.shape))\n",
    "            #stochastic_decoder_output = self.fc_out(stochastic_decoder_output.view(batch_size,1,-1))\n",
    "            #print(\"Shape of stochastic_decoder_output: {}\".format(stochastic_decoder_output.shape)) # torch.Size([32, 1, 4]) ok\n",
    "            \n",
    "            # Reparameterization Trick :)\n",
    "            decoder_output              = torch.zeros(batch_size,1,2).cuda()\n",
    "            #print(\"Shape of decoder_output: {}\".format(decoder_output.shape)) #torch.Size([32, 1, 2]) ok\n",
    "            \n",
    "            print(\"###\")\n",
    "            print(decoder_output.size())\n",
    "            print(stochastic_decoder_output.size())\n",
    "            print(\"###\")\n",
    "            \n",
    "            if stochastic_mode and path_mode=='single':\n",
    "                decoder_output[:,:,0]  = stochastic_decoder_output[:,:,0] + epsilonX.sample().cuda() * stochastic_decoder_output[:,:,1]\n",
    "                decoder_output[:,:,1]  = stochastic_decoder_output[:,:,2] + epsilonY.sample().cuda() * stochastic_decoder_output[:,:,3]\n",
    "            elif stochastic_mode and path_mode=='avg':\n",
    "                decoder_output[:,:,0]  = stochastic_decoder_output[:,:,0] + epsilonX.sample((avg_n_path_eval,1)).view(-1,avg_n_path_eval,1).mean(-2).cuda() * stochastic_decoder_output[:,:,1]\n",
    "                decoder_output[:,:,1]  = stochastic_decoder_output[:,:,2] + epsilonY.sample((avg_n_path_eval,1)).view(-1,avg_n_path_eval,1).mean(-2).cuda() * stochastic_decoder_output[:,:,3]\n",
    "            elif not(stochastic_mode):\n",
    "                decoder_output[:,:,0]  = stochastic_decoder_output[:,-1,0].clone().view(batch_size,-1) \n",
    "                decoder_output[:,:,1]  = stochastic_decoder_output[:,-1,2].clone().view(batch_size,-1) \n",
    "            elif stochastic_mode and path_mode == \"bst\":\n",
    "                epsilon_x               = torch.randn([batch_size,bst_n_path_eval,1], dtype=torch.float).cuda()\n",
    "                epsilon_y               = torch.randn([batch_size,bst_n_path_eval,1], dtype=torch.float).cuda()\n",
    "                multi_path_x            = stochastic_decoder_output[:,-1,0].clone().view(batch_size,-1) + epsilon_x * stochastic_decoder_output[:,-1,1].clone().view(batch_size,-1)\n",
    "                multi_path_y            = stochastic_decoder_output[:,-1,2].clone().view(batch_size,-1) + epsilon_y * stochastic_decoder_output[:,-1,3].clone().view(batch_size,-1)\n",
    "                ground_truth_x          = output_tensor[:,t,0].view(batch_size,1,1).cuda()\n",
    "                ground_truth_y          = output_tensor[:,t,1].view(batch_size,1,1).cuda()\n",
    "                diff_path_x             = multi_path_x - ground_truth_x\n",
    "                diff_path_y             = multi_path_y - ground_truth_y\n",
    "                diff_path               = (torch.sqrt( diff_path_x.pow(2) + diff_path_y.pow(2) )).sum(dim=-1)\n",
    "                idx                     = torch.arange(batch_size,dtype=torch.long).cuda()\n",
    "                min                     = torch.argmin(diff_path,dim=1).squeeze()\n",
    "                decoder_output[:,:,0]   = multi_path_x[idx,min,:].view(batch_size,1)\n",
    "                decoder_output[:,:,1]   = multi_path_y[idx,min,:].view(batch_size,1)\n",
    "            elif stochastic_mode and path_mode == \"top5\":\n",
    "                k = 5 #top k\n",
    "                epsilon_x               = torch.randn([batch_size,bst_n_path_eval,1], dtype=torch.float).cuda()\n",
    "                epsilon_y               = torch.randn([batch_size,bst_n_path_eval,1], dtype=torch.float).cuda()\n",
    "                multi_path_x            = stochastic_decoder_output[:,:,0].unsqueeze(1) + epsilon_x * stochastic_decoder_output[:,:,1].unsqueeze(1)\n",
    "                multi_path_y            = stochastic_decoder_output[:,:,2].unsqueeze(1) + epsilon_y * stochastic_decoder_output[:,:,3].unsqueeze(1)\n",
    "                ground_truth_x          = output_tensor[:,t,0].view(batch_size,1,1).cuda()\n",
    "                ground_truth_y          = output_tensor[:,t,1].view(batch_size,1,1).cuda()\n",
    "                diff_path_x             = multi_path_x - ground_truth_x\n",
    "                diff_path_y             = multi_path_y - ground_truth_y\n",
    "                diff_path               = (torch.sqrt( diff_path_x.pow(2) + diff_path_y.pow(2) )).sum(dim=-1)\n",
    "                idx                     = torch.arange(batch_size,dtype=torch.long).repeat(k).view(k,-1).transpose(0,1).cuda()\n",
    "                min_val, min            = torch.topk(diff_path, k=k, dim=1,largest=False)\n",
    "                decoder_output[:,:,0]   = multi_path_x[idx,min,:].mean(dim=-2).view(batch_size,1)\n",
    "                decoder_output[:,:,1]   = multi_path_y[idx,min,:].mean(dim=-2).view(batch_size,1)\n",
    "\n",
    "            # Log output\n",
    "            print(\"###\")\n",
    "            print(decoder_output.size())\n",
    "            print(stochastic_decoder_output.size())\n",
    "            print(decoder_input.size())\n",
    "            print(\"###\")\n",
    "            outputs[:,t,:]                        = decoder_output.clone().squeeze() #+ decoder_input.squeeze()\n",
    "            stochastic_outputs[:,t,:]             = stochastic_decoder_output[:,-1,:].clone().squeeze()\n",
    "            decoder_input                         = output_tensor[:,:t+1,:].clone() if teacher_force else outputs[:,:t+1,:].clone()\n",
    "\n",
    "        return outputs, stochastic_outputs, visual_vsn_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    torch.save(state, os.path.join(save_path,filename))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(save_path,filename), os.path.join(save_path,'model_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, criterion, criterion_vision, clip,train_loader, validation_loader):\n",
    "    global batch_size\n",
    "    i               = None\n",
    "    checked_frame   = 0\n",
    "\n",
    "    losses = []\n",
    "    print(\"Data Size \",data_size,\"\\tChunk Size \",chunk_size)\n",
    "    global teacher_forcing_ratio\n",
    "    counter =0\n",
    "    best_val = float(\"inf\")\n",
    "    save_path = './save_models'\n",
    "    for j in range(epoch_num):\n",
    "        model.train()\n",
    "        epoch_loss=0\n",
    "        if j%7 == 6:\n",
    "            teacher_forcing_ratio = (teacher_forcing_ratio - 0.2) if teacher_forcing_ratio>=0.1 else 0.0\n",
    "\n",
    "        # Update TeachForce ratio to gradually change during training\n",
    "        # global teacher_forcing_ratio\n",
    "        # teacher_forcing_ratio-= 1/epoch_num\n",
    "        print(\"TEACHER FORCE RATIO\\t\",teacher_forcing_ratio)\n",
    "        #print(\"Learning Rate\\t\", scheduler.get_last_lr())\n",
    "\n",
    "        start_time = time.time()\n",
    "        if(j>=from_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            ADE = 0\n",
    "            FDE = 0\n",
    "            i   = 0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                #print(\"\\n--------------- Batch %d/ %d ---------------\"%(j,i)) #(epoch/i)\n",
    "                # Forward\n",
    "                obs, pred, visual_obs, frame_tensor              = data\n",
    "                input_tensor, output_tensor                      = obs.float().squeeze().to('cuda', non_blocking=True), pred.float().squeeze().to('cuda', non_blocking=True)               #(obs.to(device), pred.to(device))\n",
    "                visual_input_tensor                              = visual_obs.squeeze().to('cuda', non_blocking=True)  #(visual_obs.to(device), visual_pred.to(device))\n",
    "                prediction, stochastic_prediction, visual_embedding = model(input_tensor,visual_input_tensor,output_tensor,batch_size,train_mode=1)\n",
    "\n",
    "                calculated_prediction = prediction.cumsum(axis=1) #calculated_prediction\n",
    "\n",
    "                loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction) #loss (regularisation term Lreg)\n",
    "                \n",
    "                if biased_loss_mode:\n",
    "                    weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "                    weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "                    loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "                    #loss_vision = criterion_vision()\n",
    "                else:\n",
    "                    loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2)) #mean squared error (lmse)\n",
    "                out_x       = output_tensor[:,:,0].cumsum(axis=1)\n",
    "                out_y       = output_tensor[:,:,1].cumsum(axis=1)\n",
    "                pred_x      = calculated_prediction[:,:,0]\n",
    "                pred_y      = calculated_prediction[:,:,1]\n",
    "                ADE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "                # FDE      += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "                \n",
    "                # Backward Propagation\n",
    "                total_loss      = loss.double() + torch.tensor(regularization_factor).to('cuda', non_blocking=True) * loss_line_regularizer.double() #total loss\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "                #print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "                epoch_loss += total_loss.item()\n",
    "                #print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                torch.cuda.empty_cache()\n",
    "                writer.close()\n",
    "                count_div=i\n",
    "            \n",
    "            # tensorboard log\n",
    "            writer.add_scalar('ADE/train', ADE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('FDE/train', FDE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('LOSS/train', epoch_loss/(count_div+1), counter)\n",
    "            counter += 1\n",
    "\n",
    "        if scheduler.get_last_lr()[0]>0.001:\n",
    "            scheduler.step()\n",
    "        # validation(model, optimizer, criterion, criterion_vision, clip, validation_loader, j) \n",
    "        epoch_loss = epoch_loss / (int(data_size/chunk_size))\n",
    "        losses.append(epoch_loss)\n",
    "        display.clear_output(wait=True)\n",
    "        plt.plot(losses, '--ro', label='train loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'epoch {j}')\n",
    "        plt.show()\n",
    "        print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "        print(\"EPOCH \", j, \"\\tLOSS \", epoch_loss)\n",
    "        writer.add_scalar('epoch_loss/train', epoch_loss/ (int(data_size/chunk_size)), j ) #see how model performs on the training dataset\n",
    "        #torch.save( model.state_dict(), model_path+\"_current\")\n",
    "        print(\"-----------------------------------------------\\n\"+\"-----------------------------------------------\")\n",
    "\n",
    "        # save checkpoint for each epoch and a fine called best_model so far \n",
    "        print(np.argmin(losses))\n",
    "        is_best = epoch_loss < best_val\n",
    "        best_val = min(epoch_loss, best_val)\n",
    "        print(\"bestvaleur\", best_val)\n",
    "        save_checkpoint({'epoch': j+1,'state_dict': model.module.state_dict(),'optimizer': optimizer.state_dict(),'scheduler': scheduler.state_dict(),'best_loss': best_val}, is_best, save_path, 'epoch_{}.pth'.format(j+1))\n",
    "        \n",
    "    return epoch_loss / (int(data_size/chunk_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, optimizer, criterion, criterion_vision, clip, validation_loader, counter):\n",
    "    global batch_size\n",
    "    model.eval()\n",
    "    i           = None\n",
    "    ADEs        = 0\n",
    "    FDEs        = 0\n",
    "    epoch_loss  = 0\n",
    "    loss_line_regularizer = 0\n",
    "    loss = 0 \n",
    "    total_loss = 0\n",
    "    ADE  = 0\n",
    "    FDE  = 0\n",
    "    for i,data in enumerate(test_loader):\n",
    "        # Forward\n",
    "        obs, pred, visual_obs, frame_tensor = data\n",
    "        input_tensor, output_tensor         = obs.float().squeeze().to('cuda', non_blocking=True), pred.float().squeeze().to('cuda', non_blocking=True)#(obs.to(device), pred.to(device))\n",
    "        visual_input_tensor                 = visual_obs.squeeze().to('cuda', non_blocking=True)   #(visual_obs.to(device), visual_pred.to(device))\n",
    "        prediction, stochastic_prediction, visual_embedding = model(input_tensor, visual_input_tensor, output_tensor, batch_size, train_mode=0)\n",
    "        \n",
    "        calculated_prediction = prediction.cumsum(axis=1) \n",
    "\n",
    "        loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction)\n",
    "        \n",
    "        if biased_loss_mode:\n",
    "            weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "            weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "            loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "        else:\n",
    "            loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2))\n",
    "        out_x       = output_tensor[:,:,0].cumsum(axis=1)\n",
    "        out_y       = output_tensor[:,:,1].cumsum(axis=1)\n",
    "        pred_x      = calculated_prediction[:,:,0]\n",
    "        pred_y      = calculated_prediction[:,:,1]\n",
    "        ADE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "        FDE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "        total_loss  += loss.double() + regularization_factor * loss_line_regularizer.double() \n",
    "        print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "\n",
    "    writer.add_scalar('ADE/val_'+path_mode,             ADE.item()/(i+1),             counter)\n",
    "    writer.add_scalar('FDE/val_'+path_mode,             FDE.item()/(i+1),             counter)\n",
    "    writer.add_scalar('LOSS/val_'+path_mode,            total_loss.item()/(i+1)   ,   counter)\n",
    "    writer.add_scalar('LOSS_c/val_'+path_mode,          loss.item()/(i+1)        ,    counter)\n",
    "    writer.add_scalar('L-REGULARIZER/val_'+path_mode,   loss_line_regularizer.item()/(i+1), counter)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_eval(model, optimizer, criterion, criterion_vision, clip, five_fold_cross_validation):\n",
    "    global batch_size\n",
    "    model.eval()\n",
    "    i           = None\n",
    "    ADEs        = 0\n",
    "    FDEs        = 0\n",
    "    epoch_loss  = 0\n",
    "    list_x_obs          = ['x_obs_'+str(i)              for i in range(0,T_obs)] #x_obs_0 --> x_obs_7\n",
    "    list_y_obs          = ['y_obs_'+str(i)              for i in range(0,T_obs)] #y_obs_0 --> y_obs_7\n",
    "    #list_c_context      = ['context_c_'+str(i)          for i in range(0,hidden_size)] #context_c_0 --> context_c_255\n",
    "    #list_h_context      = ['context_h_'+str(i)          for i in range(0,hidden_size)] #context_h_0 --> context_h_255\n",
    "    list_x_pred         = ['x_pred_'+str(i)             for i in range(0,T_pred)] #x_pred_0 --> x_pred_11\n",
    "    list_y_pred         = ['y_pred_'+str(i)             for i in range(0,T_pred)] #y_pred_0 --> y_pred_11\n",
    "    list_x_stoch_pred_m = ['x_stoch_pred_m_'+str(i)     for i in range(0,T_pred)] #x_stoch_pred_m_0 --> x_stoch_pred_m_11\n",
    "    list_y_stoch_pred_m = ['y_stoch_pred_m_'+str(i)     for i in range(0,T_pred)] #y_stoch_pred_m_0 --> y_stoch_pred_m_11\n",
    "    list_x_stoch_pred_s = ['x_stoch_pred_s_'+str(i)     for i in range(0,T_pred)] #x_stoch_pred_s_0 --> x_stoch_pred_s_11\n",
    "    list_y_stoch_pred_s = ['y_stoch_pred_s_'+str(i)     for i in range(0,T_pred)] #y_stoch_pred_s_0 --> y_stoch_pred_s_11\n",
    "    list_x_out          = ['x_out_'+str(i)              for i in range(0,T_pred)] #x_out_0 --> x_out_11\n",
    "    list_y_out          = ['y_out_'+str(i)              for i in range(0,T_pred)] #y_out_0 --> y_out_11\n",
    "    list_vsn            = ['vsn_'+str(i)               for i in range(0,hidden_size)] #vsn_0 --> vsn_255\n",
    "    df_out              = pd.DataFrame(columns=list_x_obs + list_y_obs + list_x_out + list_y_out + list_x_pred + list_y_pred + list_x_stoch_pred_m + list_y_stoch_pred_m + list_x_stoch_pred_s + list_y_stoch_pred_s + list_vsn)# + list_vsn_visual + list_c_context + list_h_context)\n",
    "\n",
    "    for i,data in enumerate(test_loader):\n",
    "\n",
    "        start_time = time.time()\n",
    "        # Forward\n",
    "        obs, pred, visual_obs, frame_tensor                 = data\n",
    "        input_tensor, output_tensor                         = obs.float().squeeze().to('cuda', non_blocking=True), pred.float().squeeze().to('cuda', non_blocking=True)               #(obs.to(device), pred.to(device))\n",
    "        visual_input_tensor                                 = visual_obs.squeeze().cuda()   #(visual_obs.to(device), visual_pred.to(device))\n",
    "        if len(input_tensor.size()) == 2:\n",
    "            break\n",
    "        print(input_tensor.size())\n",
    "        prediction, stochastic_prediction, visual_embedding = model(input_tensor,visual_input_tensor,output_tensor,batch_size,train_mode=0)\n",
    "        \n",
    "        calculated_prediction =  prediction.cumsum(axis=1) \n",
    "\n",
    "        loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction) #lreg\n",
    "\n",
    "        if biased_loss_mode:\n",
    "            weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "            weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "            loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "        else:\n",
    "            loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2)) #lmse\n",
    "        out_x           = output_tensor[:,:,0].cumsum(axis=1)\n",
    "        out_y           = output_tensor[:,:,1].cumsum(axis=1)\n",
    "        pred_x          = calculated_prediction[:,:,0]\n",
    "        pred_y          = calculated_prediction[:,:,1]\n",
    "        ADE             = ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "        FDE             = ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "        total_loss      = loss.double() + regularization_factor * loss_line_regularizer.double() #loss\n",
    "        print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "        epoch_loss += total_loss.item()\n",
    "        ADEs    += ADE.item()\n",
    "        FDEs    += FDE.item()\n",
    "        input_x_lin                 = input_tensor[:,:,0].view(-1, T_obs).cpu() #x_obs\n",
    "        input_y_lin                 = input_tensor[:,:,1].view(-1, T_obs).cpu() #y_obs\n",
    "        output_x_lin                = output_tensor[:,:,0].view(-1, T_pred).cpu() #x_out\n",
    "        output_y_lin                = output_tensor[:,:,1].view(-1, T_pred).cpu() #y_out\n",
    "        prediction_x_lin            = prediction[:,:,0].view(-1, T_pred).cpu() #x_pred\n",
    "        prediction_y_lin            = prediction[:,:,1].view(-1, T_pred).cpu() #y_pred\n",
    "        stoch_prediction_x_m        = stochastic_prediction[:,:,0].view(-1, T_pred).cpu() #x_stoch_pred_m\n",
    "        stoch_prediction_x_s        = stochastic_prediction[:,:,1].view(-1, T_pred).cpu() #x_stoch_pred_s\n",
    "        stoch_prediction_y_m        = stochastic_prediction[:,:,2].view(-1, T_pred).cpu() #y_stoch_pred_m\n",
    "        stoch_prediction_y_s        = stochastic_prediction[:,:,3].view(-1, T_pred).cpu() #y_stoch_pred_s\n",
    "        #context_h_lin               = encoder_hidden[0].view(-1, hidden_size).cpu() #context_h\n",
    "        #context_c_lin               = encoder_hidden[1].view(-1, hidden_size).cpu() #context_c\n",
    "        #visual_embedding_weights    = visual_embedding.cpu()#.view(-1, hidden_size).cpu() #vsn\n",
    "        #print(visual_embedding_weights.shape)\n",
    "        whole_data                  = torch.cat((input_x_lin, input_y_lin, output_x_lin, output_y_lin, prediction_x_lin, prediction_y_lin, stoch_prediction_x_m, stoch_prediction_y_m, stoch_prediction_x_s, stoch_prediction_y_s),1) #visual_embedding_weights), 1) #,context_c_lin, context_h_lin\n",
    "        temp                        = pd.DataFrame(whole_data.detach().cpu().numpy(), columns=list_x_obs + list_y_obs + list_x_out + list_y_out + list_x_pred + list_y_pred + list_x_stoch_pred_m + list_y_stoch_pred_m + list_x_stoch_pred_s + list_y_stoch_pred_s ) #list_vsn+ list_c_context + list_h_context\n",
    "        df_out                      = df_out.append(temp)\n",
    "        df_out.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "\n",
    "    # ADE/FDE Report\n",
    "    out_x  = df_out[['x_out_' +str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    pred_x = df_out[['x_pred_'+str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    out_y  = df_out[['y_out_' +str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    pred_y = df_out[['y_pred_'+str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    ADE = (out_x.sub(pred_x.values)**2).add((out_y.sub(pred_y.values)**2).values, axis=1)**(1/2)\n",
    "    df_out['ADE'] = ADE.mean(axis=1)\n",
    "    FDE = ADE.x_out_11\n",
    "    df_out['FDE'] = FDE\n",
    "    Mean_ADE = df_out.ADE.mean()\n",
    "    Mean_FDE = df_out.FDE.mean()\n",
    "    print(\"MEAN ADE/FDE\\t\",Mean_ADE,Mean_FDE)\n",
    "    writer.add_scalar(\"Final_Test/ADE_\"+path_mode, Mean_ADE, global_step=0)\n",
    "    writer.add_scalar(\"Final_Test/FDE_\"+path_mode, Mean_FDE, global_step=0)\n",
    "\n",
    "    df_out.to_sql(table_out+'_'+path_mode, cnx2, if_exists=\"replace\", index=False)\n",
    "    writer.close()\n",
    "    return ADEs, FDEs, int(data_size/chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model                       = Seq2Seq(in_size, embed_size, hidden_size, dropout_val=dropout_val, batch_size=batch_size)\n",
    "model                       = nn.DataParallel( model ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A summary of the model, where we can see the shape of each layer : \n",
      "DataParallel(\n",
      "  (module): Seq2Seq(\n",
      "    (encoder): CoordinatesTransformer(\n",
      "      (linear_mapper): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (5): ReLU()\n",
      "        (6): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (7): ReLU()\n",
      "      )\n",
      "      (self_attention): EncoderSelfAttention(\n",
      "        (encoder): ModuleList(\n",
      "          (0): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (1): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (2): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (3): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (4): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (5): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): AdaptiveAvgPool2d(output_size=(1, 256))\n",
      "      (fc_out): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "      )\n",
      "      (embedder_out): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (decoder): DecoderTransformer(\n",
      "      (embedder_rho): Linear(in_features=2, out_features=64, bias=True)\n",
      "      (fC_mu): Sequential(\n",
      "        (0): Linear(in_features=514, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=128, out_features=4, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (FC_dim_red): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
      "        (1): Flatten(start_dim=1, end_dim=-1)\n",
      "        (2): Linear(in_features=7396, out_features=514, bias=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (embedding): Linear(in_features=64, out_features=512, bias=True)\n",
      "      (layers): ModuleList(\n",
      "        (0): DecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (1): DecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (2): DecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (3): DecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (4): DecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (5): DecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (output_layer): Linear(in_features=512, out_features=514, bias=True)\n",
      "    )\n",
      "    (vsn_module): _GestureTransformer(\n",
      "      (conv_model): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (4): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (features): features_extraction(\n",
      "        (conv_model): Sequential(\n",
      "          (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "          (4): Sequential(\n",
      "            (0): BasicBlock(\n",
      "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (1): BasicBlock(\n",
      "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (5): Sequential(\n",
      "            (0): BasicBlock(\n",
      "              (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): BasicBlock(\n",
      "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (6): Sequential(\n",
      "            (0): BasicBlock(\n",
      "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): BasicBlock(\n",
      "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (7): Sequential(\n",
      "            (0): BasicBlock(\n",
      "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): BasicBlock(\n",
      "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      )\n",
      "      (self_attention): EncoderSelfAttention(\n",
      "        (encoder): ModuleList(\n",
      "          (0): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (1): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (2): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (3): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (4): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (5): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooling): AdaptiveAvgPool1d(output_size=256)\n",
      "    (crossAttention): crossAttention(\n",
      "      (encoder): CrossAttention(\n",
      "        (encoder): Encoder(\n",
      "          (layers): ModuleList(\n",
      "            (0): EncoderLayer(\n",
      "              (src_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (cross_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (feed_forward): PointerwiseFeedforward(\n",
      "                (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (sublayer): ModuleList(\n",
      "                (0): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (1): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (2): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (3): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): EncoderLayer(\n",
      "              (src_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (cross_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (feed_forward): PointerwiseFeedforward(\n",
      "                (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (sublayer): ModuleList(\n",
      "                (0): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (1): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (2): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (3): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (2): EncoderLayer(\n",
      "              (src_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (cross_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (feed_forward): PointerwiseFeedforward(\n",
      "                (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (sublayer): ModuleList(\n",
      "                (0): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (1): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (2): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (3): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (3): EncoderLayer(\n",
      "              (src_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (cross_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (feed_forward): PointerwiseFeedforward(\n",
      "                (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (sublayer): ModuleList(\n",
      "                (0): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (1): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (2): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (3): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (4): EncoderLayer(\n",
      "              (src_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (cross_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (feed_forward): PointerwiseFeedforward(\n",
      "                (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (sublayer): ModuleList(\n",
      "                (0): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (1): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (2): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (3): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (5): EncoderLayer(\n",
      "              (src_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (cross_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (feed_forward): PointerwiseFeedforward(\n",
      "                (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (sublayer): ModuleList(\n",
      "                (0): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (1): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (2): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (3): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm()\n",
      "        )\n",
      "        (out): Embedding(512, 512)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"A summary of the model, where we can see the shape of each layer : \")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_step               = 40\n",
    "initial_learning_rate       = 0.01\n",
    "clip                        = 1\n",
    "\n",
    "# MSE loss\n",
    "criterion                   = nn.MSELoss(reduction='mean')#nn.NLLLoss()\n",
    "criterion_vision            = nn.MSELoss(reduction='sum')#nn.NLLLoss()\n",
    "\n",
    "# SGD optimizer\n",
    "optimizer                   = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9, weight_decay=0.01) #SGD\n",
    "scheduler                   = torch.optim.lr_scheduler.StepLR(optimizer, step_size=learning_step, gamma=0.1)\n",
    "five_fold_cross_validation  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing train dataset\n"
     ]
    }
   ],
   "source": [
    "#train dataset and loader\n",
    "print(\"Initializing train dataset\")\n",
    "dataset_train = TrajectoryPredictionDataset(image_folder_path, DB_PATH_train, cnx_train)\n",
    "train_loader  = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=1, drop_last=True, pin_memory=True)\n",
    "validation_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time\t\t0.43 sec \n",
      "\n",
      "EPOCH  115 \tLOSS  1.482407736951869\n",
      "-----------------------------------------------\n",
      "-----------------------------------------------\n",
      "87\n",
      "bestvaleur 1.399481601901661\n",
      "TEACHER FORCE RATIO\t 0.0\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 1, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "torch.Size([32, 2, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "torch.Size([32, 3, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "torch.Size([32, 4, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "torch.Size([32, 5, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "torch.Size([32, 6, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "torch.Size([32, 7, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "torch.Size([32, 8, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "torch.Size([32, 9, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "torch.Size([32, 10, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "torch.Size([32, 11, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 1, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "torch.Size([32, 2, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "torch.Size([32, 3, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "torch.Size([32, 4, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "torch.Size([32, 5, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "torch.Size([32, 6, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "torch.Size([32, 7, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "torch.Size([32, 8, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "torch.Size([32, 9, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "torch.Size([32, 10, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "torch.Size([32, 11, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 1, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "torch.Size([32, 2, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "torch.Size([32, 3, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "torch.Size([32, 4, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "torch.Size([32, 5, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "torch.Size([32, 6, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "torch.Size([32, 7, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "torch.Size([32, 8, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "torch.Size([32, 9, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "torch.Size([32, 10, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "torch.Size([32, 11, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 1, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "torch.Size([32, 2, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "torch.Size([32, 3, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "torch.Size([32, 4, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "torch.Size([32, 5, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "torch.Size([32, 6, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "torch.Size([32, 7, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "torch.Size([32, 8, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "torch.Size([32, 9, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "torch.Size([32, 10, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "torch.Size([32, 11, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 1, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "torch.Size([32, 2, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "torch.Size([32, 3, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "torch.Size([32, 4, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "torch.Size([32, 5, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "torch.Size([32, 6, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "torch.Size([32, 7, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "torch.Size([32, 8, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "torch.Size([32, 9, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "torch.Size([32, 10, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "torch.Size([32, 11, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 1, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "torch.Size([32, 2, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "torch.Size([32, 3, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "torch.Size([32, 4, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "torch.Size([32, 5, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "torch.Size([32, 6, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "torch.Size([32, 7, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "torch.Size([32, 8, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "torch.Size([32, 9, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "torch.Size([32, 10, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "torch.Size([32, 11, 2])\n",
      "###\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 1, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "torch.Size([32, 2, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "torch.Size([32, 3, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "torch.Size([32, 4, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "torch.Size([32, 5, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "torch.Size([32, 6, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "torch.Size([32, 7, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "torch.Size([32, 8, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "torch.Size([32, 9, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "torch.Size([32, 10, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "torch.Size([32, 11, 2])\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN\")\n",
    "model.train()\n",
    "print(\"path mode\\t\",path_mode)\n",
    "loss  = train(model, optimizer, scheduler, criterion, criterion_vision, clip, train_loader, validation_loader)\n",
    "print(\"LOSS \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, scheduler, filename='checkpoint.pth.tar'):\n",
    "    start_epoch = 0\n",
    "    best_val=-1\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        try:\n",
    "            best_val=checkpoint['best_loss']\n",
    "        except:\n",
    "            best_val=-1\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\".format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return model, optimizer, scheduler, start_epoch, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD MODEL\n",
      "=> loading checkpoint './save_models/model_best.pth'\n",
      "=> loaded checkpoint './save_models/model_best.pth' (epoch 387)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): CoordinatesTransformer(\n",
       "    (linear_mapper): Sequential(\n",
       "      (0): Linear(in_features=2, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=128, bias=True)\n",
       "      (5): ReLU()\n",
       "      (6): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (7): ReLU()\n",
       "    )\n",
       "    (self_attention): EncoderSelfAttention(\n",
       "      (encoder): ModuleList(\n",
       "        (0): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (4): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
       "            (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pool): AdaptiveAvgPool2d(output_size=(1, 256))\n",
       "    (fc_out): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (embedder_out): Sequential(\n",
       "      (0): Linear(in_features=2048, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (4): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (decoder): DecoderTransformer(\n",
       "    (embedder_rho): Linear(in_features=2, out_features=64, bias=True)\n",
       "    (fC_mu): Sequential(\n",
       "      (0): Linear(in_features=514, out_features=128, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Dropout(p=0.2, inplace=False)\n",
       "      (3): Linear(in_features=128, out_features=4, bias=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "    (FC_dim_red): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Linear(in_features=7396, out_features=514, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (embedding): Linear(in_features=64, out_features=512, bias=True)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (3): DecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (4): DecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (5): DecoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (output_layer): Linear(in_features=512, out_features=514, bias=True)\n",
       "  )\n",
       "  (vsn_module): _GestureTransformer(\n",
       "    (conv_model): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (4): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): BasicBlock(\n",
       "          (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): BasicBlock(\n",
       "          (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (features): features_extraction(\n",
       "      (conv_model): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (4): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (5): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (6): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (7): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    )\n",
       "    (self_attention): EncoderSelfAttention(\n",
       "      (encoder): ModuleList(\n",
       "        (0): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (1): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (2): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (3): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (4): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (5): MultiHeadAttention(\n",
       "          (attention): ScaledDotProductAttention(\n",
       "            (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=1024, bias=True)\n",
       "            (1): ReLU(inplace=True)\n",
       "            (2): Dropout(p=0.2, inplace=False)\n",
       "            (3): Linear(in_features=1024, out_features=512, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooling): AdaptiveAvgPool1d(output_size=256)\n",
       "  (crossAttention): crossAttention(\n",
       "    (encoder): CrossAttention(\n",
       "      (encoder): Encoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (src_attn): MultiHeadAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (cross_attn): MultiHeadAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (feed_forward): PointerwiseFeedforward(\n",
       "              (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (3): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (1): EncoderLayer(\n",
       "            (src_attn): MultiHeadAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (cross_attn): MultiHeadAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (feed_forward): PointerwiseFeedforward(\n",
       "              (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (3): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): EncoderLayer(\n",
       "            (src_attn): MultiHeadAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (cross_attn): MultiHeadAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (feed_forward): PointerwiseFeedforward(\n",
       "              (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (3): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (3): EncoderLayer(\n",
       "            (src_attn): MultiHeadAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (cross_attn): MultiHeadAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (feed_forward): PointerwiseFeedforward(\n",
       "              (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (3): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (4): EncoderLayer(\n",
       "            (src_attn): MultiHeadAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (cross_attn): MultiHeadAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (feed_forward): PointerwiseFeedforward(\n",
       "              (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (3): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (5): EncoderLayer(\n",
       "            (src_attn): MultiHeadAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (cross_attn): MultiHeadAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (feed_forward): PointerwiseFeedforward(\n",
       "              (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (1): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (2): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (3): SubLayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm()\n",
       "      )\n",
       "      (out): Embedding(512, 512)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"LOAD MODEL\")\n",
    "# Change device to cpu\n",
    "#del model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "model      = Seq2Seq(in_size, embed_size, hidden_size, dropout_val=dropout_val, batch_size=batch_size)\n",
    "model, optimizer, scheduler, start_epoch, best_val = load_checkpoint(model, optimizer, scheduler, filename='./save_models/model_best.pth')\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing val dataset\n"
     ]
    }
   ],
   "source": [
    "#test dataset and loader\n",
    "print(\"Initializing val dataset\")\n",
    "dataset_val   = TrajectoryPredictionDataset(image_folder_path, DB_PATH_val, cnx_val)\n",
    "test_loader   = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=1, drop_last=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATE bst\n",
      "path mode\t bst\n",
      "torch.Size([32, 8, 2])\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 1, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "torch.Size([32, 2, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "torch.Size([32, 3, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "torch.Size([32, 4, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "torch.Size([32, 5, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "torch.Size([32, 6, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "torch.Size([32, 7, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "torch.Size([32, 8, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "torch.Size([32, 9, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "torch.Size([32, 10, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "torch.Size([32, 11, 2])\n",
      "###\n",
      "Total Loss\t2.37\n",
      "Time\t\t0.24 sec \n",
      "\n",
      "torch.Size([32, 8, 2])\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 1, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "torch.Size([32, 2, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "torch.Size([32, 3, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "torch.Size([32, 4, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "torch.Size([32, 5, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "torch.Size([32, 6, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "torch.Size([32, 7, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "torch.Size([32, 8, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "torch.Size([32, 9, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "torch.Size([32, 10, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "torch.Size([32, 11, 2])\n",
      "###\n",
      "Total Loss\t6.08\n",
      "Time\t\t0.22 sec \n",
      "\n",
      "torch.Size([32, 8, 2])\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 1, 4])\n",
      "torch.Size([32, 1, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 2, 4])\n",
      "torch.Size([32, 2, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 3, 4])\n",
      "torch.Size([32, 3, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 4, 4])\n",
      "torch.Size([32, 4, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 5, 4])\n",
      "torch.Size([32, 5, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 6, 4])\n",
      "torch.Size([32, 6, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 7, 4])\n",
      "torch.Size([32, 7, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 8, 4])\n",
      "torch.Size([32, 8, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 9, 4])\n",
      "torch.Size([32, 9, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 10, 4])\n",
      "torch.Size([32, 10, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([32, 1, 2])\n",
      "torch.Size([32, 11, 4])\n",
      "torch.Size([32, 11, 2])\n",
      "###\n",
      "Total Loss\t5.43\n",
      "Time\t\t0.16 sec \n",
      "\n",
      "torch.Size([2, 8, 2])\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 1, 4])\n",
      "torch.Size([2, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 1, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 1, 4])\n",
      "torch.Size([2, 1, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 2, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 2, 4])\n",
      "torch.Size([2, 2, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 3, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 3, 4])\n",
      "torch.Size([2, 3, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 4, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 4, 4])\n",
      "torch.Size([2, 4, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 5, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 5, 4])\n",
      "torch.Size([2, 5, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 6, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 6, 4])\n",
      "torch.Size([2, 6, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 7, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 7, 4])\n",
      "torch.Size([2, 7, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 8, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 8, 4])\n",
      "torch.Size([2, 8, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 9, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 9, 4])\n",
      "torch.Size([2, 9, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 10, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 10, 4])\n",
      "torch.Size([2, 10, 2])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 11, 4])\n",
      "###\n",
      "###\n",
      "torch.Size([2, 1, 2])\n",
      "torch.Size([2, 11, 4])\n",
      "torch.Size([2, 11, 2])\n",
      "###\n",
      "Total Loss\t14.50\n",
      "Time\t\t0.11 sec \n",
      "\n",
      "MEAN ADE/FDE\t 2.4502826 4.527212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11.727958679199219, 21.822275161743164, 7)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"EVALUATE bst\")\n",
    "\n",
    "model.eval()\n",
    "path_mode = 'bst'\n",
    "print(\"path mode\\t\",path_mode)\n",
    "evaluate_eval(model, optimizer, criterion, criterion_vision, clip, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EVALUATE top5\")\n",
    "model.eval()\n",
    "path_mode = 'top5'\n",
    "print(\"path mode\\t\",path_mode)\n",
    "evaluate_eval(model, optimizer, criterion, criterion_vision, clip, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "400dc80c028f38e78d0af9f2eedd78117085b98eeacd8fc6d80a8baae662b2d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
