{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c20cb1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import softmax\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Log\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import sqlite3\n",
    "import time\n",
    "import math\n",
    "from copy import deepcopy\n",
    "import os\n",
    "import datetime\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from models.TrajectoryDataset import TrajectoryPredictionDataset\n",
    "\n",
    "from models.indivSeq2Seq_residual import IndividualTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "632bb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"eth\" # dataset options: 'university', 'zara_01', 'zara_02', 'eth', 'hotel'\n",
    "\n",
    "#Building of output folder\n",
    "now = datetime.datetime.now() # current date and time\n",
    "__file__=os.getcwd()\n",
    "current_time_date = now.strftime(\"%d_%m_%y_%H_%M_%S\")\n",
    "run_folder  = \"Outputs/traj_pred_\"+ dataset_name + \"_\" + str(os.path.basename(__file__)) + str(current_time_date)\n",
    "os.makedirs(run_folder)\n",
    "\n",
    "# Make log folder for tensorboard\n",
    "#SummaryWriter_path = run_folder + \"/log\"\n",
    "#os.makedirs(SummaryWriter_path) \n",
    "SummaryWriter_path = '/notebook_data/work_dirs/first_test'\n",
    "writer = SummaryWriter(SummaryWriter_path,comment=\"ADE_FDE_Train\")\n",
    "\n",
    "# Make image folder to save outputs\n",
    "image_path  = run_folder + \"/Visual_Prediction\"\n",
    "os.makedirs(image_path)\n",
    "\n",
    "#cuda env\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    torch.cuda.device_count()\n",
    "    torch.cuda.current_device()\n",
    "else :\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99637dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataBase Variables\n",
    "image_folder_path       = 'data/data_trajpred/'+dataset_name\n",
    "DB_PATH_train     = \"./data/data_trajpred/\"+dataset_name+\"/pos_data_train.db\"\n",
    "cnx_train         = sqlite3.connect(DB_PATH_train)\n",
    "DB_PATH_val     = \"./data/data_trajpred/\"+dataset_name+\"/pos_data_val.db\"\n",
    "cnx_val         = sqlite3.connect(DB_PATH_val)\n",
    "DB_DIR      = run_folder + '/database'\n",
    "os.makedirs( DB_DIR )\n",
    "DB_PATH2    = DB_DIR+'/db_one_ped_delta_coordinates_results.db'\n",
    "cnx2        = sqlite3.connect(DB_PATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cba2fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Other variables\n",
    "T_obs                   = 8\n",
    "T_pred                  = 12\n",
    "T_total                 = T_obs + T_pred #8+12=20\n",
    "data_id                 = 0 \n",
    "batch_size              = 32 #10#100 #15 #2\n",
    "chunk_size              = batch_size * T_total # Chunksize should be multiple of T_total\n",
    "in_size                 = 2\n",
    "stochastic_out_size     = in_size * 2\n",
    "hidden_size             = 256 #!64\n",
    "embed_size              = 64 #16 #!64\n",
    "global dropout_val\n",
    "dropout_val             = 0.2 #0.5\n",
    "teacher_forcing_ratio   = 0.7 # 0.9\n",
    "regularization_factor   = 0.5 # 0.001\n",
    "avg_n_path_eval         = 20\n",
    "bst_n_path_eval         = 20\n",
    "path_mode               = \"top5\" #\"avg\",\"bst\",\"single\",\"top5\"\n",
    "regularization_mode     = \"regular\" #\"weighted\",\"e_weighted\", \"regular\"\n",
    "startpoint_mode         = \"on\" #\"on\",\"off\"\n",
    "enc_out                 = \"on\" #\"on\",\"off\"\n",
    "biased_loss_mode        = 0 # 0 , 1\n",
    "\n",
    "table_out   = \"results_delta\"\n",
    "table       = \"dataset_T_length_20delta_coordinates\" #\"dataset_T_length_\"+str(T_total)+\"delta_coordinates\"\n",
    "df_id       = pd.read_sql_query(\"SELECT data_id FROM \"+table, cnx_train)\n",
    "data_size   = df_id.data_id.max() * T_total\n",
    "epoch_num   = 500\n",
    "from_epoch  = 0\n",
    "\n",
    "#Visual Variables\n",
    "image_size              = 256  \n",
    "image_dimension         = 3\n",
    "mask_size               = 16\n",
    "visual_features_size    = 128 \n",
    "visual_embed_size       = 64  #128 #256 #64\n",
    "vsn_module_out_size    = 256\n",
    "to_pil = torchvision.transforms.ToPILImage()\n",
    "\n",
    "\n",
    "#Model Path\n",
    "model_path = run_folder + \"/NNmodel\"\n",
    "os.makedirs(model_path)   \n",
    "model_path = model_path + str(\"/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dee4ff3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.2, 0.2)\n",
    "\n",
    "        \n",
    "def distance_from_line_regularizer(input_tensor, prediction):\n",
    "    sum_sigma_distance = torch.zeros(1)\n",
    "    input_tensor = input_tensor.double()\n",
    "    prediction = prediction.double()\n",
    "    input_tensor = input_tensor.cumsum(dim=1)\n",
    "    X = torch.ones_like(input_tensor).to('cuda', non_blocking=True)\n",
    "    X[:,:,0] = input_tensor[:,:,0]\n",
    "    Y = (input_tensor[:,:,1]).unsqueeze(-1)\n",
    "    try:\n",
    "        try:\n",
    "            XTX_1 = torch.matmul(X.transpose(-1,-2), X).inverse()\n",
    "        except:\n",
    "            XTX_1 = torch.matmul(X.transpose(-1,-2), X).ppinverse()\n",
    "        XTY = torch.matmul( X.transpose(-1,-2), Y)\n",
    "        theta = torch.matmul( XTX_1.double(), XTY)\n",
    "        \n",
    "        # Calculate real values of prediction instead of delta\n",
    "        prediction[:,:,0] = prediction[:,:,0] + input_tensor[:,-1,0].unsqueeze(-1) \n",
    "        prediction[:,:,1] = prediction[:,:,1] + input_tensor[:,-1,1].unsqueeze(-1)\n",
    "        \n",
    "        # Calculate distance ( predicted_points , observation_fitted_line ) over batch\n",
    "        theta0x0        = theta[:,0,:] * prediction[:,:,0]\n",
    "        denominator     = torch.sqrt( theta[:,0,:] * theta[:,0,:] + 1 )\n",
    "        nominator       = theta0x0 + theta[:,1,:] - prediction[:,:,1]\n",
    "        distance        = nominator.abs() / denominator\n",
    "        if regularization_mode =='weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().double(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(-1,T_pred)\n",
    "            weighted_distance   = weight * distance\n",
    "\n",
    "        elif regularization_mode =='e_weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().double(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(distance.size(0),T_pred)\n",
    "            weight              = torch.exp(weight)\n",
    "            weighted_distance   = weight*distance\n",
    "\n",
    "        else:\n",
    "            weighted_distance = distance\n",
    "        sigma_distance  = torch.mean(weighted_distance,1)\n",
    "        sum_sigma_distance  = torch.mean(sigma_distance)\n",
    "        return sum_sigma_distance\n",
    "    except:\n",
    "        print(\"SINGULAR VALUE\")\n",
    "        sum_sigma_distance = torch.zeros(1).to('cuda', non_blocking=True) + 20\n",
    "        return sum_sigma_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64571e1b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c8a2627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing train dataset\n",
      "Batch_size : 32\n"
     ]
    }
   ],
   "source": [
    "#train dataset and loader\n",
    "\n",
    "batch_size = 32\n",
    "print(\"Initializing train dataset\")\n",
    "print(\"Batch_size : {}\".format(batch_size))\n",
    "dataset_train = TrajectoryPredictionDataset(image_folder_path, cnx_train)\n",
    "train_loader  = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True, pin_memory=True)\n",
    "validation_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9919904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = IndividualTF(device,2,2,2,6,512,2048,dropout=dropout_val)\n",
    "model = model.to(device).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efdd14ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_step               = 40\n",
    "initial_learning_rate       = 0.01\n",
    "clip                        = 1\n",
    "\n",
    "# MSE loss\n",
    "criterion                   = nn.MSELoss(reduction='mean')\n",
    "criterion_vision            = nn.MSELoss(reduction='sum')\n",
    "\n",
    "# SGD optimizer\n",
    "optimizer                   = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9, weight_decay=0.01) #SGD\n",
    "scheduler                   = torch.optim.lr_scheduler.StepLR(optimizer, step_size=learning_step, gamma=0.1)\n",
    "five_fold_cross_validation  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1098bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    torch.save(state, os.path.join(save_path,filename))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(save_path,filename), os.path.join(save_path,'model_best.pth'))\n",
    "        \n",
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    Mask out subsequent positions.\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(mask) == 0\n",
    "        \n",
    "def train(model, optimizer, scheduler, criterion, criterion_vision, clip,train_loader, validation_loader, save_path=None):\n",
    "    global batch_size\n",
    "    i               = None\n",
    "    checked_frame   = 0\n",
    "\n",
    "    losses = []\n",
    "    print(\"Data Size \",data_size,\"\\tChunk Size \",chunk_size)\n",
    "    global teacher_forcing_ratio\n",
    "    counter =0\n",
    "    best_val = float(\"inf\")\n",
    "    for j in range(epoch_num):\n",
    "        model.train()\n",
    "        epoch_loss=0\n",
    "        if j%7 == 6:\n",
    "            teacher_forcing_ratio = (teacher_forcing_ratio - 0.2) if teacher_forcing_ratio>=0.1 else 0.0\n",
    "\n",
    "        # Update TeachForce ratio to gradually change during training\n",
    "        # global teacher_forcing_ratio\n",
    "        # teacher_forcing_ratio-= 1/epoch_num\n",
    "        print(\"TEACHER FORCE RATIO\\t\",teacher_forcing_ratio)\n",
    "        #print(\"Learning Rate\\t\", scheduler.get_last_lr())\n",
    "\n",
    "        \n",
    "        if(j>=from_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            start_time = time.time()\n",
    "            ADE = 0\n",
    "            FDE = 0\n",
    "            i   = 0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                #print(\"\\n--------------- Batch %d/ %d ---------------\"%(j,i)) #(epoch/i)\n",
    "                # Forward\n",
    "                obs, pred, visual_obs, frame_tensor = data\n",
    "                \n",
    "                input_tensor, output_tensor         = obs.double().squeeze(dim=1).to('cuda', non_blocking=True), pred.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "                    \n",
    "                visual_input_tensor                 = visual_obs.double().squeeze(dim=1).to('cuda', non_blocking=True)\n",
    "                \n",
    "                src_att = torch.ones((input_tensor.shape[0], 1, input_tensor.shape[1])).to(device)\n",
    "                trg_att=subsequent_mask(output_tensor.shape[1]).repeat(output_tensor.shape[0],1,1).to(device)\n",
    "                \n",
    "                prediction = model(input_tensor, output_tensor, visual_input_tensor, src_att, trg_att)\n",
    "                \n",
    "                calculated_prediction = prediction.cumsum(axis=1) #calculated_prediction\n",
    "                \n",
    "                loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction) #loss (regularisation term Lreg)\n",
    "                \n",
    "                if biased_loss_mode:\n",
    "                    weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "                    weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "                    loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "                else:\n",
    "                    loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2)) #mean squared error (lmse)\n",
    "                    \n",
    "                out_x       = output_tensor[:,:,0].cumsum(axis=1)\n",
    "                out_y       = output_tensor[:,:,1].cumsum(axis=1)\n",
    "                pred_x      = calculated_prediction[:,:,0]\n",
    "                pred_y      = calculated_prediction[:,:,1]\n",
    "                ADE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "                # FDE      += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "                \n",
    "                # Backward Propagation\n",
    "                total_loss      = loss.double() + torch.tensor(regularization_factor).to('cuda', non_blocking=True) * loss_line_regularizer.double() #total loss\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "                #print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "                epoch_loss += total_loss.item()\n",
    "                #print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                torch.cuda.empty_cache()\n",
    "                writer.close()\n",
    "                count_div=i\n",
    "            \n",
    "            # tensorboard log\n",
    "            writer.add_scalar('ADE/train', ADE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('FDE/train', FDE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('LOSS/train', epoch_loss/(count_div+1), counter)\n",
    "            counter += 1\n",
    "\n",
    "        if scheduler.get_last_lr()[0]>0.001:\n",
    "            scheduler.step()\n",
    "        # validation(model, optimizer, criterion, criterion_vision, clip, validation_loader, j) \n",
    "        epoch_loss = epoch_loss / (int(data_size/chunk_size))\n",
    "        losses.append(epoch_loss)\n",
    "        display.clear_output(wait=True)\n",
    "        plt.plot(losses, '--ro', label='train loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'epoch {j}')\n",
    "        plt.show()\n",
    "        print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "        print(\"EPOCH \", j, \"\\tLOSS \", epoch_loss)\n",
    "        writer.add_scalar('epoch_loss/train', epoch_loss/ (int(data_size/chunk_size)), j ) #see how model performs on the training dataset\n",
    "        #torch.save( model.state_dict(), model_path+\"_current\")\n",
    "        print(\"-----------------------------------------------\\n\"+\"-----------------------------------------------\")\n",
    "\n",
    "        # save checkpoint for each epoch and a fine called best_model so far \n",
    "        print(np.argmin(losses))\n",
    "        is_best = epoch_loss < best_val\n",
    "        best_val = min(epoch_loss, best_val)\n",
    "        print(\"bestvaleur\", best_val)\n",
    "        if save_path and (j+1)%5==0:\n",
    "            save_checkpoint({'epoch': j+1,'state_dict': model.state_dict(),'optimizer': optimizer.state_dict(),'scheduler': scheduler.state_dict(),'best_loss': best_val}, is_best, save_path, 'epoch_{}.pth'.format(j+1))\n",
    "        \n",
    "    return epoch_loss / (int(data_size/chunk_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "419d86ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time\t\t0.41 sec \n",
      "\n",
      "EPOCH  131 \tLOSS  11.848689018663539\n",
      "-----------------------------------------------\n",
      "-----------------------------------------------\n",
      "123\n",
      "bestvaleur 11.547311310828873\n",
      "TEACHER FORCE RATIO\t 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath mode\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,path_mode)\n\u001b[1;32m      5\u001b[0m save_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./save_models/bimodal_transfo/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m loss  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_vision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOSS \u001b[39m\u001b[38;5;124m\"\u001b[39m,loss)\n",
      "Cell \u001b[0;32mIn[9], line 55\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, criterion, criterion_vision, clip, train_loader, validation_loader, save_path)\u001b[0m\n\u001b[1;32m     52\u001b[0m src_att \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((input_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, input_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     53\u001b[0m trg_att\u001b[38;5;241m=\u001b[39msubsequent_mask(output_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mrepeat(output_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 55\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual_input_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_att\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrg_att\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m calculated_prediction \u001b[38;5;241m=\u001b[39m prediction\u001b[38;5;241m.\u001b[39mcumsum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#calculated_prediction\u001b[39;00m\n\u001b[1;32m     59\u001b[0m loss_line_regularizer \u001b[38;5;241m=\u001b[39m distance_from_line_regularizer(input_tensor,calculated_prediction) \u001b[38;5;66;03m#loss (regularisation term Lreg)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/notebook_data/trajectory_bitransformer_-main/trajectory_bitransformer_-main/models/indivSeq2Seq_residual.py:66\u001b[0m, in \u001b[0;36mIndividualTF.forward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerator(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/notebook_data/trajectory_bitransformer_-main/trajectory_bitransformer_-main/models/attempts/seq2seq_residual/encoder_decoder.py:26\u001b[0m, in \u001b[0;36mEncoderDecoder.forward\u001b[0;34m(self, src, tgt, visual_input_tensor, src_mask, tgt_mask)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt, visual_input_tensor, src_mask, tgt_mask):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    Take in and process masked src and target sequences.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisual_input_tensor\u001b[49m\u001b[43m)\u001b[49m, src_mask, tgt, tgt_mask)\n",
      "File \u001b[0;32m/notebook_data/trajectory_bitransformer_-main/trajectory_bitransformer_-main/models/attempts/seq2seq_residual/encoder_decoder.py:30\u001b[0m, in \u001b[0;36mEncoderDecoder.encode\u001b[0;34m(self, src, src_mask, visual_input_tensor)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, src_mask, visual_input_tensor):\n\u001b[1;32m     29\u001b[0m     output_trj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_trj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_embed(src), src_mask)\n\u001b[0;32m---> 30\u001b[0m     output_vsn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder_vsn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisual_input_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     code \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((output_trj,output_vsn),\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39mdouble()\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m#return self.encoder(self.src_embed(src), src_mask)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/notebook_data/trajectory_bitransformer_-main/trajectory_bitransformer_-main/models/encoders/resnet_encoder.py:88\u001b[0m, in \u001b[0;36m_GestureTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     86\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[1;32m     87\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(shape[\u001b[38;5;241m0\u001b[39m],shape[\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 88\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\u001b[38;5;241m.\u001b[39msqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#final pooling\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/notebook_data/trajectory_bitransformer_-main/trajectory_bitransformer_-main/models/encoders/trajectory_encoder.py:15\u001b[0m, in \u001b[0;36mEncoderSelfAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 15\u001b[0m     in_encoder \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[43msinusoid_encoding_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder:\n\u001b[1;32m     17\u001b[0m         in_encoder \u001b[38;5;241m=\u001b[39m l(in_encoder, in_encoder, in_encoder)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"TRAIN\")\n",
    "model.train()\n",
    "print(\"path mode\\t\",path_mode)\n",
    "\n",
    "save_path = \"./save_models/bimodal_transfo/\"\n",
    "loss  = train(model, optimizer, scheduler, criterion, criterion_vision, clip, train_loader, validation_loader, save_path)\n",
    "print(\"LOSS \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de3879b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ff1122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a53903a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22809c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e1de61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c618ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
