{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from torch.distributions.normal import Normal\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread, imshow\n",
    "import datetime\n",
    "from skimage.util import img_as_ubyte\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import argparse\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from models.indiv_crossAttention_seq2seq import crossAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochastic_mode         = 1 #output  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make folder for outputs and logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset name\n",
    "dataset_name = \"university\" # dataset options: 'university', 'zara_01', 'zara_02', 'eth', 'hotel'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebook_data/trajectory_bitransformer_-main/trajectory_bitransformer_-main\n"
     ]
    }
   ],
   "source": [
    "__file__=os.getcwd()\n",
    "print(__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now() # current date and time\n",
    "current_time_date = now.strftime(\"%d_%m_%y_%H_%M_%S\")\n",
    "run_folder  = \"Outputs/traj_pred_\"+ dataset_name + \"_\" + str(os.path.basename(__file__)) + str(current_time_date)\n",
    "os.makedirs(run_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip to left side bar\n",
    ">\n",
    "/\n",
    "Name\n",
    "Last Modified\n",
    "\n",
    "# Make log folder for tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SummaryWriter_path = \"/notebook_data/work_dirs/dec_trans_mayssa_univ/\"\n",
    "os.makedirs(SummaryWriter_path)   \n",
    "writer = SummaryWriter(SummaryWriter_path,comment=\"ADE_FDE_Train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make image folder to save outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path  = run_folder + \"/Visual_Prediction\"\n",
    "os.makedirs(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataBase Variables\n",
    "\n",
    "image_folder_path       = 'data/data_trajpred/'+dataset_name\n",
    "DB_PATH_train     = \"./data/data_trajpred/\"+dataset_name+\"/pos_data_train.db\"\n",
    "cnx_train         = sqlite3.connect(DB_PATH_train)\n",
    "DB_PATH_val     = \"./data/data_trajpred/\"+dataset_name+\"/pos_data_val.db\"\n",
    "cnx_val         = sqlite3.connect(DB_PATH_val)\n",
    "DB_DIR      = run_folder + '/database'\n",
    "os.makedirs( DB_DIR )\n",
    "DB_PATH2    = DB_DIR+'/db_one_ped_delta_coordinates_results.db'\n",
    "cnx2        = sqlite3.connect(DB_PATH2)\n",
    "\n",
    "#Other variables\n",
    "T_obs                   = 8\n",
    "T_pred                  = 12\n",
    "T_total                 = T_obs + T_pred #8+12=20\n",
    "data_id                 = 0 \n",
    "batch_size              = 32\n",
    "chunk_size              = batch_size * T_total # Chunksize should be multiple of T_total\n",
    "in_size                 = 2\n",
    "stochastic_out_size     = in_size * 2\n",
    "hidden_size             = 256 #!64\n",
    "embed_size              = 64 #16 #!64\n",
    "global dropout_val\n",
    "dropout_val             = 0.2 #0.5\n",
    "teacher_forcing_ratio   = 0.7 # 0.9\n",
    "regularization_factor   = 0.5 # 0.001\n",
    "avg_n_path_eval         = 20\n",
    "bst_n_path_eval         = 20\n",
    "path_mode               = \"top5\" #\"avg\",\"bst\",\"single\",\"top5\"\n",
    "regularization_mode     = \"regular\" #\"weighted\",\"e_weighted\", \"regular\"\n",
    "startpoint_mode         = \"on\" #\"on\",\"off\"\n",
    "enc_out                 = \"on\" #\"on\",\"off\"\n",
    "biased_loss_mode        = 0 # 0 , 1\n",
    "\n",
    "\n",
    "table_out   = \"results_delta\"\n",
    "table       = \"dataset_T_length_20delta_coordinates\" #\"dataset_T_length_\"+str(T_total)+\"delta_coordinates\"\n",
    "df_id       = pd.read_sql_query(\"SELECT data_id FROM \"+table, cnx_train) #juste la colonne data_id de database\n",
    "#print(\"dfshape\",df_id.shape) #[4920 rows x 1 columns]\n",
    "data_size   = df_id.data_id.max() * T_total\n",
    "epoch_num   = 500\n",
    "from_epoch  = 0\n",
    "\n",
    "#Visual Variables\n",
    "image_size              = 256  \n",
    "image_dimension         = 3\n",
    "mask_size               = 16\n",
    "visual_features_size    = 128 \n",
    "visual_embed_size       = 64  #128 #256 #64\n",
    "vsn_module_out_size    = 256\n",
    "to_pil = torchvision.transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = run_folder + \"/NNmodel\" \n",
    "os.makedirs(model_path)   \n",
    "model_path = model_path + str(\"/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homography Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\n",
    "# la matrice fournie par eux\n",
    "if dataset_name == 'eth' or dataset_name =='hotel':   # ETH dataset\n",
    "    h = np.array([[0.0110482,0.000669589,-3.32953],[-0.0015966,0.0116324,-5.39514],[0.000111907,0.0000136174,0.542766]])\n",
    "else:                                       # UCY dataset\n",
    "    h = np.array([[47.51,0,476],[0,41.9,117],[0,0,1]])\n",
    "\"\"\"\"\"\n",
    "# Matrice Standard\n",
    "if dataset_name == 'eth':  \n",
    "    h = np.array([[2.8128700e-02, 2.0091900e-03, -4.6693600e+00],[8.0625700e-04, 2.5195500e-02, -5.0608800e+00],[ 3.4555400e-04, 9.2512200e-05, 4.6255300e-01]]) #h pour eth\n",
    "if dataset_name =='hotel':        \n",
    "    h = np.array([[1.1048200e-02, 6.6958900e-04, -3.3295300e+00],[-1.5966000e-03, 1.1632400e-02, -5.3951400e+00],[1.1190700e-04, 1.3617400e-05, 5.4276600e-01]]) #h pour hotel\n",
    "if dataset_name =='zara01': \n",
    "    h = np.array([[0.02174104, 0, -0.15],[0, -0.02461883, 13.77429807],[0, 0, 1]]) #h pour zara1\n",
    "if dataset_name =='zara02': \n",
    "    h = np.array([[0.02174104, 0, -0.4],[0, -0.02386598, 14.98401686],[0, 0, 1]]) #h pour zara2\n",
    "if dataset_name =='univ':\n",
    "    h = np.array([[0.02220407, 0, -0.48],[0, -0.02477289, 13.92551292],[0, 0, 1]]) #h pour univ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Sequential Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryPredictionDataset(torch.utils.data.Dataset): \n",
    "#Enc.cinématique reçoit la trajectoire observée de humain cible (input) de la forme T=(u1,u2-u1,u3-u2,..) qui consiste en les coordonnées de la position de départ et en les déplacements relatifs de l'humain entre les images consécutives.\n",
    "#Ce format a été choisi car il permet au modèle de mieux capturer les similarités entre des trajectoires presque identiques qui peuvent avoir des points de départ différents. \n",
    "    def __init__(self, ROOT_DIR, DB_PATH, cnx):\n",
    "        \n",
    "        self.pos_df    = pd.read_sql_query(\"SELECT * FROM \"+str(table), cnx)\n",
    "        self.root_dir  = ROOT_DIR+'/visual_data'\n",
    "        self.transform = torchvision.transforms.Compose([torchvision.transforms.Resize((image_size,image_size)), \\\n",
    "                                                         torchvision.transforms.ToTensor(), \\\n",
    "                                                         torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n",
    "        self.visual_data = []\n",
    "        # read sorted frames\n",
    "        for img in sorted(os.listdir(self.root_dir)): \n",
    "            self.visual_data.append(self.transform( Image.open(os.path.join(self.root_dir)+\"/\"+img) ))\n",
    "        self.visual_data = torch.stack(self.visual_data)  \n",
    "        #print(\"visual_data:\", self.visual_data) #tensor\n",
    "        #print(\"shapevisual_data:\", self.visual_data.shape) #torch.Size([1298, 3, 256, 256]) : 1298 blocs dans chaque bloc 3 sous blocs, et dans chaque sous bloc: 256 lignes et 256 colonnes\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pos_df.data_id.max() #data_id maximum dans dataset\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist() \n",
    "        #print(\"idx :\", idx)\n",
    "        \n",
    "        extracted_df     = self.pos_df[ self.pos_df[\"data_id\"] == idx ] #table dont data_id=idx\n",
    "\n",
    "        tensor           = torch.tensor(extracted_df[['pos_x_delta','pos_y_delta']].values).reshape(-1,T_total,in_size) #juste pos_x_delta et pos_y_delta de extracted_df (tensor)\n",
    "        #print(\"tensor: \", tensor)\n",
    "        obs, pred        = torch.split(tensor,[T_obs,T_pred],dim=1) #obs de 8 et pred de 12 à partir de tensor construit\n",
    "        #print(\"obs: \", obs)\n",
    "        #print(\"pred: \", pred)\n",
    "\n",
    "        start_frames     = (extracted_df.groupby('data_id').frame_num.min().values/10).astype('int') #extracted_df dont data_id=idx, on prend minimum frame_num et aprés on divise par 10, cela represente start_frame\n",
    "        extracted_frames = []\n",
    "        for i in start_frames:            \n",
    "            extracted_frames.append(self.visual_data[i:i+T_obs])\n",
    "        frames = torch.stack(extracted_frames) #stack concatenates a sequence of tensors along a new dimension.\n",
    "        start_frames = torch.tensor(start_frames) #tensor([start_frames])\n",
    "        return obs, pred, frames, start_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize random weights for NN models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.2, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizer loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance_from_line_regularizer function is implemented as a regularization method to enforce the predicted trajectory to stay close to the observed trajectory. The method is used to minimize the Euclidean distances between each step of the predicted trajectory and a line fitted to the observed trajectory.\n",
    "\n",
    "It takes 2 inputs:\n",
    "1-input_tensor : The observed trajectory in the form of a tensor of size (batch_size, T_obs, 2)\n",
    "2-prediction : The predicted trajectory in the form of a tensor of size (batch_size, T_pred, 2)\n",
    "\n",
    "The function first converts the input tensors to double precision, then it calculates the cumulative sum of the observed trajectory along the time axis. Next, it fits a line to the observed trajectory by calculating the slope and intercept of the line using the least square method. Then it calculates the real values of the predicted trajectory by adding the last point of the observed trajectory to the delta values predicted by the model. Next, it calculates the distance between each predicted point and the fitted line using the formula distance = |(theta0*x0+theta1) - y0| / sqrt(theta0^2 + 1) where x0, y0 are the coordinates of the predicted point, theta0, theta1 are the slope and intercept of the line respectively.\n",
    "\n",
    "Finally, it applies a weighting scheme to the distances based on the regularization_mode:\n",
    "weighted : The weights are linearly decreasing from T_pred to 1\n",
    "e_weighted : The weights are exponentially decreasing from T_pred to 1\n",
    "None : No weighting\n",
    "\n",
    "It then calculates the mean distance over all the predicted points and returns it as sum_sigma_distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_sigma_distance  = torch.zeros(1) #tensor[(0.)]\n",
    "\n",
    "def distance_from_line_regularizer(input_tensor,prediction):\n",
    "    global sum_sigma_distance\n",
    "    #the regularization is defined as the sum of Euclidean distances between each step of the predicted trajectory Tf , and a line fitted to the observed trajectory To.\n",
    "    # Fit a line to observation points over batch \n",
    "    input_tensor    = input_tensor.double()\n",
    "    prediction      = prediction.double()\n",
    "    input_tensor    = input_tensor.cumsum(dim=1).double()\n",
    "    X               = torch.ones_like(input_tensor).double().to('cuda', non_blocking=True)\n",
    "    X[:,:,0]        = input_tensor[:,:,0]\n",
    "    Y               = (input_tensor[:,:,1]).unsqueeze(-1).double()\n",
    "    try:\n",
    "        try:\n",
    "            XTX_1           = torch.matmul( X.transpose(-1,-2), X).double().inverse()\n",
    "        except:\n",
    "            XTX_1           = torch.matmul( X.transpose(-1,-2), X).double().pinverse()\n",
    "        XTY             = torch.matmul( X.transpose(-1,-2), Y)\n",
    "        theta           = torch.matmul( XTX_1.double(), XTY.double())\n",
    "        # Calculate real values of prediction instead of delta\n",
    "        prediction[:,:,0] = prediction[:,:,0] + input_tensor[:,-1,0].unsqueeze(-1) \n",
    "        prediction[:,:,1] = prediction[:,:,1] + input_tensor[:,-1,1].unsqueeze(-1)\n",
    "        \n",
    "        # Calculate distance ( predicted_points , observation_fitted_line ) over batch\n",
    "        theta0x0        = theta[:,0,:].double() * prediction[:,:,0].double()\n",
    "        denominator     = torch.sqrt( theta[:,0,:].double() * theta[:,0,:].double() + 1 )\n",
    "        nominator       = theta0x0 + theta[:,1,:] - prediction[:,:,1].double()\n",
    "        distance        = nominator.abs() / denominator\n",
    "        if regularization_mode =='weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().float(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(-1,T_pred)\n",
    "            weighted_distance   = weight * distance\n",
    "\n",
    "        elif regularization_mode =='e_weighted':\n",
    "            weight              = torch.flip( torch.arange(1,T_pred+1).cuda().float(),[0])\n",
    "            weight              = (weight / T_pred).repeat(distance.size(0)).view(distance.size(0),T_pred)\n",
    "            weight              = torch.exp(weight)\n",
    "            weighted_distance   = weight*distance\n",
    "\n",
    "        else:\n",
    "            weighted_distance = distance\n",
    "        sigma_distance  = torch.mean(weighted_distance,1)\n",
    "        sum_sigma_distance  = torch.mean(sigma_distance)\n",
    "        return sum_sigma_distance\n",
    "    except:\n",
    "        print(\"SINGULAR VALUE\")\n",
    "        sum_sigma_distance = torch.zeros(1).to('cuda', non_blocking=True) + 20\n",
    "        return sum_sigma_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer (partie vision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spatial Features Extraction (Resnet 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader \n",
    "from torchvision import datasets \n",
    "from torchvision.transforms import ToTensor \n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet function creates a ResNet model with the option to use a pretrained model from ImageNet, and to select the number of layers to unfreeze and delete.\n",
    "The function also allows to define the number of input channels for the images (1, 2 or 3). It returns the ResNet model.\n",
    "\n",
    "The function takes 4 parameters as input:\n",
    "1-pretrain: a boolean value that determines whether to use a pretrained model from ImageNet or not. The default value is True, which means the model will be pretrained.\n",
    "2-layers_to_unfreeze: an integer value that defines the number of layers at the end of the ResNet model that will be unfrozen for training. The default value is 8.\n",
    "3-layers_to_delete: an integer value that defines the number of layers at the end of the ResNet model that will be deleted from the model. The default value is 2.\n",
    "4-in_planes: an integer value that defines the number of input channels for the images. The supported values are 1, 2, or 3. The default value is 3.\n",
    "\n",
    "The function starts by initializing the ResNet model with the specified pretrained setting, then creates a new nn.Sequential model to store the modified layers. It calculates the total number of layers in the ResNet model, and if this number is less than the number of layers to unfreeze, it sets the number of layers to unfreeze to the total number of layers. The function then loops through the children layers of the ResNet model and for the first layer it modifies the number of input channels to match the specified number of input channels, in case the input channel is 1 or 2, it modify the weight of the conv layer accordingly. Then the function applies freezing on some layers depending on the layers_to_freeze and layers_to_delete parameters. It appends the child layers to the new model if it's in the range of number_of_layers. Finally, it returns the new model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script defines a features_extraction class that inherits from the PyTorch nn.Module class. The class is used to extract features from an input image using a convolutional model (in this case, ResNet) and a pooling operation.\n",
    "The class takes the convolutional model and the number of input channels for the images as input. The class overrides the forward method to reshape the input image, pass it through the convolutional model, and then apply the pooling operation. The resulting feature map is then returned.\n",
    "\n",
    "The class takes 2 arguments in the constructor:\n",
    "1-conv_model: the convolutional model that will be used for feature extraction. Currently, only ResNet is supported.\n",
    "2-in_planes: the number of input channels for the images.\n",
    "The class also creates an AdaptiveAvgPool2d object with the shape (1,1) to be used for the pooling operation.\n",
    "\n",
    "The class overrides the forward method, which takes an input image, reshape the image to have the number of channels as defined by the in_planes parameter, pass it through the convolutional model, and then apply the pooling operation. The resulting feature map is then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Resnet(pretrain=True,layers_to_unfreeze=8,layers_to_delete=2,in_planes=3):\n",
    "    \"\"\"\n",
    "    param:\n",
    "        pretrain: Define if we load a pretrained model from ImageNet\n",
    "        layers_to_unfreeze: Define the number of layers that we want to train at the end of the Resnet\n",
    "        layers_to_delete: Define the numbers of layers that we want to delete\n",
    "        in_planes: Define the numbers of input channels of images (supported values: 1,2 or 3)\n",
    "    return: The Resnet model\n",
    "    \"\"\"\n",
    "    resnet = torchvision.models.resnet18(pretrained=pretrain)\n",
    "    # Create a new model cause we don't want the pooling operation at the end and the classifier\n",
    "    model = nn.Sequential()\n",
    "    number_of_layers = len(list(resnet.children())) - layers_to_delete # In practice it remove the pooling operation and the classifier\n",
    "\n",
    "    if number_of_layers<layers_to_unfreeze:\n",
    "        layers_to_unfreeze = number_of_layers\n",
    "    layers_to_freeze = number_of_layers - layers_to_unfreeze\n",
    "    i=0\n",
    "    for child in resnet.children():\n",
    "        # For the first layers we create a new weight if in_planes is not 3 cause ResNet is pretrain on image with 3 channels there is no version for 1 channel\n",
    "        if i==0 and in_planes<3:\n",
    "            if i<layers_to_freeze: # Define if we freeze this layer or no\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False # Freeze the layers by passing requires_grad attribute to False\n",
    "            w = child._parameters['weight'].data # Get the weight for 3 channels data\n",
    "            child._modules['0'] = nn.Conv2d(in_planes, 64, kernel_size=3, padding=1) # Define the new conv layer\n",
    "            if in_planes == 1:\n",
    "                child._parameters['weight'].data = w.mean(dim=1, keepdim=True) # If the number of channels is 1 we made the mean of channels to set the new weight\n",
    "            else:\n",
    "                child._parameters['weight'].data = w[:, :-1] * 1.5\n",
    "\n",
    "        if i<layers_to_freeze: # Define if we freeze this layer or no\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False # Freeze the layers by passing requires_grad attribute to False\n",
    "        if i<number_of_layers: # To define if we keep this layer or not\n",
    "            model.append(child) \n",
    "        i+=1\n",
    "    return model\n",
    "\n",
    "\n",
    "class features_extraction(nn.Module):\n",
    "    \"\"\"\n",
    "    param:\n",
    "    conv_model: The convolution model used before capsules for the moment only ResNet is supported\n",
    "    in_planes: Numbers of channels for the image\n",
    "    \"\"\"\n",
    "    def __init__(self,conv_model,in_planes: int):\n",
    "        super().__init__()\n",
    "        self.conv_model = conv_model\n",
    "        self.in_planes = in_planes\n",
    "        self.pooling = nn.AdaptiveAvgPool2d((1,1))\n",
    "        \n",
    "    def forward(self,input):\n",
    "        shape = input.size()\n",
    "        x = input.view(-1,self.in_planes,shape[-2],shape[-1])\n",
    "        x = self.conv_model(x)\n",
    "        x = self.pooling(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The position_embedding function takes 2 arguments as input:\n",
    "1-input: a tensor representing the input of the function, it's a 1-D tensor with the shape of (batch_size,)\n",
    "2-d_model: an integer value representing the depth of the model, it's used to define the size of the output tensor.\n",
    "The function starts by reshaping the input tensor to have a shape of (-1, 1) and creating a 1-D tensor called dim of shape (d_model/2,). Then it creates two tensors, sin and cos, by applying sin and cos functions on the input tensor element-wise with a scaling factor of 2 * dim / d_model. The function then creates an output tensor of shape (batch_size, d_model) and assigns the values of sin and cos tensors to the even and odd indices respectively. Finally, it returns the output tensor.\n",
    "\n",
    "The sinusoid_encoding_table function takes 2 arguments as input:\n",
    "1-max_len: an integer value representing the maximum length of the input tensor that the function will be applied on.\n",
    "2-d_model: an integer value representing the depth of the model, it's used to define the size of the output tensor.\n",
    "The function starts by creating a 1-D tensor called pos of shape (max_len) with integer values ranging from 0 to max_len-1, then it applies the position_embedding function on the pos tensor with the d_model value passed to the function as an argument. Finally, it returns the output of the position_embedding function.\n",
    "\n",
    "These functions are used in the Transformer model to add position information to the input embeddings. The sinusoid_encoding_table function is used to create a lookup table of position embeddings, which is then used to add position information to the input embeddings in the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_embedding(input, d_model):\n",
    "    input = input.view(-1, 1)\n",
    "    dim = torch.arange(d_model // 2, dtype=torch.float32, device=input.device).view(1, -1)\n",
    "    sin = torch.sin(input / 10000 ** (2 * dim / d_model))\n",
    "    cos = torch.cos(input / 10000 ** (2 * dim / d_model))\n",
    "\n",
    "    out = torch.zeros((input.shape[0], d_model), device=input.device)\n",
    "    out[:, ::2] = sin\n",
    "    out[:, 1::2] = cos\n",
    "    return out\n",
    "\n",
    "def sinusoid_encoding_table(max_len, d_model):\n",
    "    pos = torch.arange(max_len, dtype=torch.float32)\n",
    "    out = position_embedding(pos, d_model)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ScaledDotProductAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script defines a ScaledDotProductAttention class that inherits from the PyTorch nn.Module class. This class implements the scaled dot-product attention mechanism as described in the Transformer model.\n",
    "\n",
    "The class takes 4 parameters in the constructor:\n",
    "1-d_model: the output dimensionality of the model\n",
    "2-d_k: the dimensionality of queries and keys\n",
    "3-d_v: the dimensionality of values\n",
    "4-h: the number of heads\n",
    "\n",
    "It then initializes four nn.Linear modules, fc_q, fc_k, fc_v and fc_o, which are used to project the queries, keys, values, and the final output, respectively. It also defines an init_weights function which is used to initialize the weights of the linear layers using the Xavier initialization method.\n",
    "\n",
    "The forward function takes three inputs, queries, keys, and values, and applies the scaled dot-product attention mechanism on them. It starts by reshaping the inputs to have the shape (batch_size, num_queries, h, d_k/d_v) and permutes the dimensions to have the shape (batch_size, h, num_queries/d_k, d_k/d_v) for queries, keys, and values respectively. Then it computes the dot product between the queries and keys and scales it by 1/sqrt(d_k) and applies a softmax function on the resulting tensor to get the attention weights. Then, it takes a weighted sum of the values tensor with the attention weights to get the final output. Finally, it applies the final linear layer fc_o on the output and returns it.\n",
    "\n",
    "This class is used in the transformer model as one of the building blocks to compute the attention scores between the input and output sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h):\n",
    "        \"\"\"\n",
    "        param:\n",
    "        d_model: Output dimensionality of the model\n",
    "        d_k: Dimensionality of queries and keys\n",
    "        d_v: Dimensionality of values\n",
    "        h: Number of heads\n",
    "        \"\"\"\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.fc_q = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_k = nn.Linear(d_model, h * d_k)\n",
    "        self.fc_v = nn.Linear(d_model, h * d_v)\n",
    "        self.fc_o = nn.Linear(h * d_v, d_model)\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.h = h\n",
    "\n",
    "        self.init_weights(gain=1.0)\n",
    "\n",
    "    def init_weights(self, gain=1.0):\n",
    "        nn.init.xavier_normal_(self.fc_q.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_k.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_v.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.fc_o.weight, gain=gain)\n",
    "        nn.init.constant_(self.fc_q.bias, 0)\n",
    "        nn.init.constant_(self.fc_k.bias, 0)\n",
    "        nn.init.constant_(self.fc_v.bias, 0)\n",
    "        nn.init.constant_(self.fc_o.bias, 0)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        \"\"\"\n",
    "        Computes\n",
    "        :param queries: Queries (b_s, nq, d_model)\n",
    "        :param keys: Keys (b_s, nk, d_model)\n",
    "        :param values: Values (b_s, nk, d_model)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        b_s, nq = queries.shape[:2]\n",
    "        nk = keys.shape[1]\n",
    "        q = self.fc_q(queries).view(b_s, nq, self.h, self.d_k).permute(0, 2, 1, 3)  # (b_s, h, nq, d_k)\n",
    "        k = self.fc_k(keys).view(b_s, nk, self.h, self.d_k).permute(0, 2, 3, 1)  # (b_s, h, d_k, nk)\n",
    "        v = self.fc_v(values).view(b_s, nk, self.h, self.d_v).permute(0, 2, 1, 3)  # (b_s, h, nk, d_v)\n",
    "\n",
    "        att = torch.matmul(q, k) / np.sqrt(self.d_k)  # (b_s, h, nq, nk)\n",
    "\n",
    "        att = torch.softmax(att, -1)\n",
    "\n",
    "        out = torch.matmul(att, v).permute(0, 2, 1, 3).contiguous().view(b_s, nq, self.h * self.d_v)  # (b_s, nq, h*d_v)\n",
    "        out = self.fc_o(out)  # (b_s, nq, d_model)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script defines a MultiHeadAttention class that inherits from the PyTorch nn.Module class. This class implements the multi-head attention mechanism as described in the Transformer model.\n",
    "\n",
    "The class takes 6 parameters in the constructor:\n",
    "1-d_model: the output dimensionality of the model\n",
    "2-d_k: the dimensionality of queries and keys\n",
    "3-d_v: the dimensionality of values\n",
    "4-h: the number of heads\n",
    "5-dff: the dimensionality of the feed-forward layer\n",
    "6-dropout: the dropout rate for the dropout layers\n",
    "\n",
    "It then initializes an instance of the ScaledDotProductAttention class, a dropout layer, a layer normalization layer, and a feed-forward layer which is implemented using two linear layers with a ReLU activation in between them.\n",
    "\n",
    "The forward function takes three inputs, queries, keys, and values, and applies the multi-head attention mechanism on them. It starts by applying the attention mechanism on the inputs using the attention instance. Then it applies the dropout layer on the resulting tensor. Then it applies the feed-forward layer on the resulting tensor. Finally, it applies dropout layer again and then applies the layer normalization on the resulting tensor. It then sums the resulting tensor with the queries tensor and returns it.\n",
    "\n",
    "This class is used in the transformer model as one of the building blocks to compute the attention scores between the input and output sequences. By using multi-head attention, the transformer can attend to different parts of the input sequence in parallel for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Multi-head attention layer with Dropout and Layer Normalization\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_k, d_v, h, dff=2048, dropout=.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(d_model=d_model, d_k=d_k, d_v=d_v, h=h)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.fc = nn.Sequential(*[nn.Linear(d_model, dff), nn.ReLU(inplace=True), nn.Dropout(p=dropout),nn.Linear(dff, d_model)])\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        att = self.attention(queries, keys, values)\n",
    "        att = self.dropout(att)\n",
    "        att = self.fc(att)\n",
    "        att = self.dropout(att)\n",
    "        return self.layer_norm(queries + att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EncoderSelfAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above defines a class for the Encoder Self-Attention module. The main goal of this module is to perform self-attention on the input tensor and output the encoded tensor. The EncoderSelfAttention class takes in several parameters such as device, d_model, d_k, d_v, n_head, dff, dropout_transformer, and n_module.\n",
    "\n",
    "The device parameter is used to specify the device that the tensors will be allocated on (CPU or GPU). d_model, d_k, d_v, and n_head are hyperparameters used in the MultiHeadAttention module. dff is the dimensionality of the feedforward network and dropout_transformer is the dropout rate applied to the output of the MultiHeadAttention module. n_module is the number of MultiHeadAttention modules stacked in the encoder.\n",
    "\n",
    "The forward method takes in the input tensor x, and applies the sinusoid_encoding_table to it, which is used to add positional encoding to the input tensor. Then it applies the MultiHeadAttention module n_module times to the input tensor, and returns the encoded tensor.\n",
    "\n",
    "This is applying the MultiHeadAttention module to the input tensor \"in_encoder\" multiple times. The input tensor is passed as the queries, keys and values to the MultiHeadAttention module and the output of the MultiHeadAttention is then passed back as the input to the next MultiHeadAttention module in the nn.ModuleList \"self.encoder\" through the for loop. This process is repeated n_module times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderSelfAttention(nn.Module):\n",
    "    def __init__(self, device, d_model, d_k, d_v, n_head, dff=2048, dropout_transformer=.1, n_module=6):\n",
    "        super(EncoderSelfAttention, self).__init__()\n",
    "        self.encoder = nn.ModuleList([MultiHeadAttention(d_model, d_k, d_v, n_head, dff, dropout_transformer) for _ in range(n_module)])\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        #print(\"x shape: {}\".format(x.shape)) #torch.Size([32, 8, 512])\n",
    "        \n",
    "        in_encoder = x + sinusoid_encoding_table(x.shape[1], x.shape[2]).expand(x.shape).to(self.device)\n",
    "        #print(\"in_encoder shape: {}\".format(in_encoder.shape)) #torch.Size([32, 8, 512])\n",
    "        \n",
    "        for l in self.encoder:\n",
    "            in_encoder = l(in_encoder, in_encoder, in_encoder)\n",
    "            #print(\"in_encoder shape: {}\".format(in_encoder.shape)) #torch.Size([32, 8, 512])\n",
    "            \n",
    "        return in_encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partie vision : Resnet + Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a class definition for the _GestureTransformer model, which is a multi-modal model that can take in either 3 or 1 channel images as input. \n",
    "The model uses a convolutional backbone, such as ResNet, to extract features from the input images, followed by a self-attention mechanism implemented using the EncoderSelfAttention module. The final output of the model is obtained by applying a global average pooling operation on the output of the self-attention mechanism. \n",
    "The constructor of the class takes in several parameters, such as the device to run the model on, the choice of convolutional backbone, the number of input channels, whether to use a pre-trained model, the input dimension, the number of layers to unfreeze, the number of layers to delete, the number of attention heads, the number of attention modules, the feed-forward size and the dropout rate for the 1D dropout. \n",
    "Overall, the _GestureTransformer model is designed to perform feature extraction and self-attention on the input images in order to identify gestures in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class _GestureTransformer(nn.Module):\n",
    "    \"\"\"Multi-Modal model on 3 or 1 channel\"\"\"\n",
    "    def __init__(self,device,backbone=\"resnet\",in_planes=3,pretrained= True,input_dim=512,layers_to_unfreeze=8,layers_to_delete=2,n_head=8,n_module=6,ff_size=1024,dropout1d=0.5):\n",
    "        super(_GestureTransformer, self).__init__()\n",
    "\n",
    "        self.in_planes = in_planes\n",
    "        self.device = device\n",
    "        self.conv_name = backbone\n",
    "        self.conv_model = None\n",
    "        \n",
    "        if self.conv_name.lower()==\"resnet\":\n",
    "            self.conv_model = Resnet(pretrained,layers_to_unfreeze,layers_to_delete,in_planes)\n",
    "        else:\n",
    "            raise NotImplementedError(\"The model {} is not supported!\".format(self.conv_name))\n",
    "            \n",
    "        self.conv_model.to(device)\n",
    "        self.features = features_extraction(self.conv_model,in_planes)\n",
    "\n",
    "        self.self_attention = EncoderSelfAttention(device,input_dim,64,64,n_head=n_head,dff=ff_size,dropout_transformer=dropout1d,n_module=n_module)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,input_dim)) #final pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        shape = x.shape\n",
    "        #print(\"x avant encoder shape: {}\".format(x.shape)) # torch.Size([32, 8, 3, 256, 256])\n",
    "        \n",
    "        x = self.features(x)\n",
    "        #print(\"x avant encoder shape: {}\".format(x.shape)) #torch.Size([256, 512, 1, 1])\n",
    "        \n",
    "        x = x.view(shape[0],shape[1],-1)\n",
    "        #print(\"x avant encoder shape: {}\".format(x.shape)) #torch.Size([32, 8, 512])\n",
    "        \n",
    "        x = self.self_attention(x)\n",
    "        #print(\"x avant encoder shape: {}\".format(x.shape)) #torch.Size([32, 8, 512])\n",
    "\n",
    "        x = self.pool(x).squeeze(dim=1) #final pooling\n",
    "        #print(\"x avant encoder shape: {}\".format(x.shape)) #torch.Size([32, 512])\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Transformer (partie cinématique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CoordinatesTransformer(nn.Module):\n",
    "    \"\"\"Multi-Modal model on 3 or 1 channel\"\"\"\n",
    "    def __init__(self,device,pretrained= True,input_dim=512,layers_to_unfreeze=8,layers_to_delete=2,n_head=8,n_module=6,ff_size=1024,dropout1d=0.5):\n",
    "        super(CoordinatesTransformer, self).__init__()\n",
    "        \n",
    "        # self.conv_model.to(device)\n",
    "        self.linear_mapper= torch.nn.Sequential(\n",
    "                              torch.nn.Linear(2, 32),\n",
    "                              torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(32, 64),\n",
    "                              torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(64, 128),\n",
    "                              torch.nn.ReLU(),\n",
    "                              torch.nn.Linear(128, 256),\n",
    "                              torch.nn.ReLU(),)\n",
    "        \n",
    "        \n",
    "        self.self_attention = EncoderSelfAttention(device,256,64,64,n_head=n_head,dff=ff_size,dropout_transformer=dropout1d,n_module=n_module)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1,256)) #final pooling\n",
    "        \n",
    "        \n",
    "        self.embedder_out = nn.Sequential(nn.Linear(8*256, 256),nn.ReLU(),nn.Dropout(p=dropout_val),nn.Linear(256, 256),nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = x.shape\n",
    "        #print(\"x avant:\", x.shape) #torch.Size([40, 1, 2])\n",
    "        x=self.linear_mapper(x)\n",
    "        #print(\"x apres linear:\", x.shape) #torch.Size([40, 1, 256])\n",
    "        x = x.view(shape[0],shape[1],-1)\n",
    "        #print(\"x apres view:\", x.shape) #torch.Size([40, 1, 256])\n",
    "        x = self.self_attention(x)\n",
    "        #print(\"x apres attention\", x.shape) #torch.Size([40, 1, 256])\n",
    "\n",
    "        x = self.pool(x).squeeze(dim=1) #final pooling\n",
    "        #print(\"x apres pool\", x.shape) #torch.Size([40, 256])\n",
    "        return x\n",
    "    \n",
    "    def emb_out(self,input):\n",
    "        #print(\"input shape avant embedder out: {}\".format(input.shape)) #torch.Size([40, 2048])\n",
    "        out= self.embedder_out(input)\n",
    "        #print(\"out shape apres embedder out: {}\".format(out.shape)) #torch.Size([40, 256])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is a decoder part of a transformer-based architecture for a gesture recognition task, where it takes in the current time step's input and the encoded outputs from the encoder, processes them through multi-head self-attention, feed-forward layers and linear layers, and produces an output. The input is first passed through an embedding layer to get a 64-dimensional embedding, and then processed through a stack of num_layers DecoderLayer modules, each consisting of multi-head self-attention and feed-forward layers. Finally, the output from the last layer is passed through a linear layer to produce the final output of the decoder. The final output's size is 514, which can be used for gesture recognition.\n",
    "\n",
    "This is the implementation of a single layer of the decoder in a transformer architecture. \n",
    "The decoder layer consists of 2 main components: multi-head self-attention and feed-forward neural network.\n",
    "\n",
    "The multi-head self-attention mechanism allows the decoder to consider different parts of the input sequence at different positions, and to weigh the importance of each part for the current position. This is done by computing the attention weights between the input sequence and itself, and using those weights to compute a weighted sum of the input sequence. The result is then added to the input to obtain the attention output.\n",
    "\n",
    "The feed-forward neural network is a simple linear neural network that transforms the input into a new representation. In this case, it consists of a linear layer followed by a ReLU activation function. The output of the feed-forward network is then added to the input, and the result is passed through two layer normalization layers. This final output is the output of the decoder layer.\n",
    "\n",
    "The x variable passed as input is the input to the decoder, and the e_output variable is the output of the encoder. This variable is used in the attention mechanism to compute the attention weights between the input and the encoder output.\n",
    "\n",
    "the DecoderTransformer class is a decoder for a transformer-based model. The class initializes various linear and non-linear layers, and defines the forward pass. The forward pass starts by taking the input x, which is the current position of the agent, and passing it through a linear layer self.embedder_rho to create an embedding. This embedding is then passed through a ReLU activation function and a dropout layer. The class also defines several other layers, including self.fC_mu, self.FC_dim_red, self.embedding, self.layers and self.output_layer. These layers are used in the forward pass to perform various computations, such as multihead self-attention and feed forward, on the input and the encoder outputs. Finally, the class uses a linear layer self.fC_mu to produce the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, nhead):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.feed_forward = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, e_output):\n",
    "        # Multihead self-attention\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = x + self.norm1(attn_output)\n",
    "        \n",
    "        # Feed forward\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.norm2(ff_output)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, in_size, embed_size, hidden_size, d_model=512, dropout_val=dropout_val, batch_size=1, nhead=8, num_layers=6):\n",
    "        super(DecoderTransformer, self).__init__()\n",
    "                \n",
    "        self.in_size                = in_size #2\n",
    "        self.stochastic_out_size    = stochastic_out_size #2*2=4\n",
    "        self.hidden_size            = hidden_size #256\n",
    "        self.batch_size             = batch_size\n",
    "        self.embed_size             = embed_size #64\n",
    "        self.seq_length             = T_pred #12\n",
    "        self.dropout_val            = dropout_val #0.2\n",
    "        self.visual_embed_size      = visual_embed_size #64\n",
    "        self.visual_embed_size      = visual_embed_size\n",
    "        self.visual_size            = image_dimension * image_size * image_size #3*256*256\n",
    "        \n",
    "        self.d_model=d_model\n",
    "        self.nhead=nhead\n",
    "        self.num_layers=num_layers\n",
    "        \n",
    "        self.embedder_rho = nn.Linear(self.in_size, self.embed_size) #(2,64)\n",
    "        self.fC_mu = nn.Sequential(nn.Linear(self.hidden_size + self.hidden_size + in_size, int(self.hidden_size/2), bias=True),nn.ReLU(),nn.Dropout(p=dropout_val),nn.Linear(int(self.hidden_size/2), self.stochastic_out_size, bias=True))\n",
    "        self.dropout = nn.Dropout(dropout_val)\n",
    "        self.reducted_size = int((self.hidden_size-1)/3)+1\n",
    "        self.reducted_size2 = int((self.hidden_size+in_size-1)/3)+1\n",
    "        self.FC_dim_red = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=3, padding=1),nn.Flatten(start_dim=1, end_dim=-1),nn.Linear(self.reducted_size*self.reducted_size2, 2*self.hidden_size+in_size, bias=True),nn.ReLU())\n",
    "        \n",
    "        self.embedding = nn.Linear(64, d_model)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model, nhead) for _ in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(d_model, 514)\n",
    "                \n",
    "    def forward(self, x, encoder_outputs):\n",
    "        \n",
    "        #print(\"x forward shape: {}\".format(x.shape)) #torch.Size([32, 2]) ok\n",
    "\n",
    "        # Coordination Embedding\n",
    "        embedding = self.embedder_rho(x.view(-1,2)) #torch.Size([32, 64]) ok\n",
    "        #print(\"embedding1 forward shape: {}\".format(embedding.shape))\n",
    "        embedding = F.relu(self.dropout(embedding))\n",
    "        #print(\"embedding2 forward shape: {}\".format(embedding.shape)) #torch.Size([32, 64]) ok\n",
    "\n",
    "        # Embed the decoder input\n",
    "        x = self.embedding(embedding.unsqueeze(1))\n",
    "        #print(\"embedding unsqueeze forward shape: {}\".format(embedding.unsqueeze(1).shape)) #torch.Size([32, 1, 64]) ok\n",
    "\n",
    "        #x = self.embedding(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_outputs)\n",
    "        output = self.output_layer(x)\n",
    "        #print(\"output forward shape: {}\".format(output.shape)) #torch.Size([32, 1, 514]) ok\n",
    "        \n",
    "        #print(\"output squeeze forward shape: {}\".format(output.squeeze(0).shape)) #output squeeze forward shape: torch.Size([32, 1, 514])\n",
    "        prediction = self.fC_mu(output.squeeze(0)) \n",
    "        #print(\"prediction forward shape: {}\".format(prediction.shape)) #torch.Size([32, 1, 4]) ok\n",
    "        \n",
    "        return prediction\n",
    "    \n",
    "    def dim_red(self, input):\n",
    "        #print(\"input DIM_RED shape: {}\".format(input.shape)) #torch.Size([32, 257, 257])\n",
    "        output = self.FC_dim_red(input)\n",
    "        #print(\"output DIM_RED shape: {}\".format(output.shape)) #torch.Size([32, 514])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a PyTorch implementation of a sequence-to-sequence model, which is composed of several neural network modules. \n",
    "\n",
    "The Seq2Seq class has several components:\n",
    "1-An encoder module (encoder) which is a Transformer encoder that takes in the input_tensor and encodes it to a fixed-size representation. It has multi-head self-attention mechanism, layer normalization, dropout, and feed-forward neural network as sub-layers.\n",
    "2-An encoder module (encoderrnn) which is a RNN encoder that takes in the input_tensor and encodes it to a fixed-size representation. It has coordination embedding, RNN, and Linear sub-layers.\n",
    "3-A decoder module (decoder) which is a transformer decoder that takes the encoded representation and generates the output_tensor.\n",
    "4-A visual module (vsn_module) that takes visual_input_tensor and encodes it to a fixed-size representation.\n",
    "5-A pooling layer (pooling) that applies pooling on the encoder_outputs.\n",
    "6-A linear layer (out) which is applied on the encoded representations to produce the final output.\n",
    "\n",
    "In the forward method, the input_tensor and visual_input_tensor are passed through the encoder, encoder_rnn, and visual module respectively. Then the pooled outputs are passed through the decoder to generate the output_tensor. Finally, the encoded representations are passed through the linear layer to produce the final output.\n",
    "\n",
    "the Seq2Seq class takes several inputs:\n",
    "1-in_size: The size of the input features.\n",
    "2-embed_size: The size of the embedding for the input features.\n",
    "3-hidden_size: The size of the hidden state for the encoder and decoder.\n",
    "4-batch_size: The batch size of the input data.\n",
    "5-d_model: The desired dimension of the input after passing through the linear layer in the transformer encoder.\n",
    "6-d_ff: The dimension of the feed-forward neural network in the transformer encoder.\n",
    "7-h: Number of heads in the multi-head self-attention mechanism in the transformer encoder.\n",
    "8-dropout_val: The dropout value applied throughout the model.\n",
    "9-N: The number of layers in the transformer encoder.\n",
    "10-input_dim: The dimension of the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, in_size, embed_size, hidden_size, batch_size=1, d_model=512, d_ff=2048, h=8, dropout_val=dropout_val, N=6, input_dim=512):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        self.encoder = CoordinatesTransformer(device,dropout1d=dropout_val) #Encoder Transformer (partie cinématique)\n",
    "        self.encoder.apply(init_weights)\n",
    "                \n",
    "        self.decoder =  DecoderTransformer(in_size, embed_size, hidden_size, num_layers=6, nhead=8)\n",
    "        self.decoder.apply(init_weights)\n",
    "        \n",
    "        self.vsn_module = _GestureTransformer(device,dropout1d=dropout_val) #_GestureTransformer(partie vision)                   \n",
    "        self.vsn_module.apply(init_weights)\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool1d((hidden_size)) #on a ajouté pooling (car prob de shape)\n",
    "        \n",
    "        if device.type=='cuda':\n",
    "            self.encoder.cuda()\n",
    "            self.decoder.cuda()\n",
    "            self.vsn_module.cuda()\n",
    "            \n",
    "        self.crossAttention = crossAttention(N=3,\n",
    "                                            d_model=256, d_ff=2048, h=8, dropout=0.1)\n",
    "\n",
    "    def forward(self, input_tensor, visual_input_tensor, output_tensor, batch_size, train_mode): \n",
    "        \n",
    "        #print(\"Visual Input_tensor shape: {}\".format(visual_input_tensor.shape)) #torch.Size([32, 8, 3, 256, 256])\n",
    "        #print(\"Input_tensor shape: {}\".format(input_tensor.shape)) #torch.Size([32, 8, 2])\n",
    "\n",
    "        batch_size      = int(input_tensor.size(0))        \n",
    "        \n",
    "        #encoder_outputs \n",
    "        encoder_outputs = torch.zeros(batch_size, T_obs, hidden_size).cuda()\n",
    "        #print(\"shape of encoder_outputs: {}\".format(encoder_outputs.shape)) #torch.Size([32, 8, 256])\n",
    "\n",
    "        start_point     = (input_tensor[:,0,:]).to(device).clone().detach()\n",
    "\n",
    "        if startpoint_mode==\"on\":\n",
    "            input_tensor[:,0,:]    = 0\n",
    "\n",
    "        for t in range(0,T_obs):\n",
    "            #print(\"Input_tensor[:,t,:] shape: {}\".format(input_tensor[:,t,:].shape)) #torch.Size([32, 2])\n",
    "            \n",
    "            encoder_output                  = self.encoder(input_tensor[:,t,:].reshape(batch_size, -1, 2))\n",
    "            encoder_outputs[:,t,:]          = encoder_output.squeeze(1)\n",
    "            \n",
    "            #print(\"shape of encoder_output: {}\".format(encoder_output.shape)) #torch.Size([40,256])\n",
    "            #print(\"shape of encoder_outputs[:,t,:]: {}\".format(encoder_outputs[:,t,:].shape)) #torch.Size([40,256])\n",
    "\n",
    "        # Encoder outputs : \n",
    "        \n",
    "        #print(\"shape of encoder_outputs view: {}\".format(encoder_outputs.view(batch_size,-1).shape)) #torch.Size([40, 2048])\n",
    "        encoder_extract       = self.encoder.emb_out(encoder_outputs.view(batch_size,-1))\n",
    "        #print(\"Shape of encoder extract: {}\".format(encoder_extract.shape)) #torch.Size([40, 256])\n",
    "                    \n",
    "        visual_initial_vsn    = self.vsn_module(visual_input_tensor)\n",
    "        #print(\"shape of visual initial vsn : {}\".format(visual_initial_vsn.shape)) #torch.Size([32, 512])\n",
    "        visual_initial_vsn    = self.pooling(visual_initial_vsn) #pooling qu'on a ajouté\n",
    "        #print(\"shape of visual initial vsn apres pooling : {}\".format(visual_initial_vsn.shape)) ##torch.Size([32, 256])\n",
    "        \n",
    "        src_mask = None\n",
    "        obd_enc_mask = None\n",
    "        \n",
    "        cross_ouput = self.crossAttention(encoder_extract, visual_initial_vsn, src_mask, obd_enc_mask)\n",
    "        #concat the 2 outputs of encoders\n",
    "#         e_outputs = torch.cat([encoder_extract.view(batch_size,-1),visual_initial_vsn.view(batch_size,-1)],dim=-1)\n",
    "#         #print(\"Shape of e_outputs : {}\".format(e_outputs.shape)) #torch.Size([32, 512])\n",
    "#         e_outputss = torch.cat([encoder_extract.view(batch_size,-1),visual_initial_vsn.view(batch_size,-1)],dim=-1).unsqueeze(0)\n",
    "#         #print(\"Shape of e_outputss : {}\".format(e_outputss.shape)) #torch.Size([1, 32, 512])\n",
    "        e_outputss = cross_ouput\n",
    "        visual_vsn_result   = visual_initial_vsn\n",
    "        \n",
    "        #decoder_input\n",
    "        decoder_input = input_tensor[:,-1,:]\n",
    "        #print(\"Shape of decoder_input: {}\".format(decoder_input.shape)) #torch.Size([32, 2])\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs                         = torch.zeros(batch_size, T_pred , in_size).cuda() #torch.Size([32, 12, 2])\n",
    "        stochastic_outputs              = torch.zeros(batch_size, T_pred , stochastic_out_size).cuda() #torch.Size([32, 12, 4])\n",
    "        teacher_force                   = 1\n",
    "\n",
    "        epsilonX                        = Normal(torch.zeros(batch_size,1),torch.ones(batch_size,1))\n",
    "        epsilonY                        = Normal(torch.zeros(batch_size,1),torch.ones(batch_size,1))\n",
    "        teacher_force                   = int(random.random() < teacher_forcing_ratio) if train_mode else 0\n",
    "        \n",
    "        for t in range(0, T_pred):\n",
    "            #print(\"Shape of output_tensor[:,t,:]: {}\".format(output_tensor[:,t,:].shape)) #torch.Size([32, 2])\n",
    "            #print(\"Shape of output_tensor[:,t,:] reshape: {}\".format(output_tensor[:,t,:].reshape(batch_size, -1, 2).shape)) #torch.Size([32, 1, 2])\n",
    "            \n",
    "            stochastic_decoder_output = self.decoder(decoder_input, e_outputss)\n",
    "            #print(\"Shape of stochastic_decoder_output: {}\".format(stochastic_decoder_output.shape)) # torch.Size([32, 1, 4]) ok\n",
    "\n",
    "            # Reparameterization Trick :)\n",
    "            decoder_output              = torch.zeros(batch_size,1,2).cuda()\n",
    "            #print(\"Shape of decoder_output: {}\".format(decoder_output.shape)) #torch.Size([32, 1, 2]) ok\n",
    "            \n",
    "            if stochastic_mode and path_mode=='single':\n",
    "                decoder_output[:,:,0]  = stochastic_decoder_output[:,:,0] + epsilonX.sample().cuda() * stochastic_decoder_output[:,:,1]\n",
    "                decoder_output[:,:,1]  = stochastic_decoder_output[:,:,2] + epsilonY.sample().cuda() * stochastic_decoder_output[:,:,3]\n",
    "            elif stochastic_mode and path_mode=='avg':\n",
    "                decoder_output[:,:,0]  = stochastic_decoder_output[:,:,0] + epsilonX.sample((avg_n_path_eval,1)).view(-1,avg_n_path_eval,1).mean(-2).cuda() * stochastic_decoder_output[:,:,1]\n",
    "                decoder_output[:,:,1]  = stochastic_decoder_output[:,:,2] + epsilonY.sample((avg_n_path_eval,1)).view(-1,avg_n_path_eval,1).mean(-2).cuda() * stochastic_decoder_output[:,:,3]\n",
    "            elif not(stochastic_mode):\n",
    "                decoder_output[:,:,0]  = stochastic_decoder_output[:,:,0] \n",
    "                decoder_output[:,:,1]  = stochastic_decoder_output[:,:,2] \n",
    "            elif stochastic_mode and path_mode == \"bst\":\n",
    "                epsilon_x               = torch.randn([batch_size,bst_n_path_eval,1], dtype=torch.float).cuda()\n",
    "                epsilon_y               = torch.randn([batch_size,bst_n_path_eval,1], dtype=torch.float).cuda()\n",
    "                multi_path_x            = stochastic_decoder_output[:,:,0].unsqueeze(1) + epsilon_x * stochastic_decoder_output[:,:,1].unsqueeze(1)\n",
    "                multi_path_y            = stochastic_decoder_output[:,:,2].unsqueeze(1) + epsilon_y * stochastic_decoder_output[:,:,3].unsqueeze(1)\n",
    "                ground_truth_x          = output_tensor[:,t,0].view(batch_size,1,1).cuda()\n",
    "                ground_truth_y          = output_tensor[:,t,1].view(batch_size,1,1).cuda()\n",
    "                diff_path_x             = multi_path_x - ground_truth_x\n",
    "                diff_path_y             = multi_path_y - ground_truth_y\n",
    "                diff_path               = (torch.sqrt( diff_path_x.pow(2) + diff_path_y.pow(2) )).sum(dim=-1)\n",
    "                idx                     = torch.arange(batch_size,dtype=torch.long).cuda()\n",
    "                min                     = torch.argmin(diff_path,dim=1).squeeze()\n",
    "                decoder_output[:,:,0]   = multi_path_x[idx,min,:].view(batch_size,1)\n",
    "                decoder_output[:,:,1]   = multi_path_y[idx,min,:].view(batch_size,1)\n",
    "            elif stochastic_mode and path_mode == \"top5\":\n",
    "                k = 5 #top k\n",
    "                epsilon_x               = torch.randn([batch_size,bst_n_path_eval,1], dtype=torch.float).cuda()\n",
    "                epsilon_y               = torch.randn([batch_size,bst_n_path_eval,1], dtype=torch.float).cuda()\n",
    "                multi_path_x            = stochastic_decoder_output[:,:,0].unsqueeze(1) + epsilon_x * stochastic_decoder_output[:,:,1].unsqueeze(1)\n",
    "                multi_path_y            = stochastic_decoder_output[:,:,2].unsqueeze(1) + epsilon_y * stochastic_decoder_output[:,:,3].unsqueeze(1)\n",
    "                ground_truth_x          = output_tensor[:,t,0].view(batch_size,1,1).cuda()\n",
    "                ground_truth_y          = output_tensor[:,t,1].view(batch_size,1,1).cuda()\n",
    "                diff_path_x             = multi_path_x - ground_truth_x\n",
    "                diff_path_y             = multi_path_y - ground_truth_y\n",
    "                diff_path               = (torch.sqrt( diff_path_x.pow(2) + diff_path_y.pow(2) )).sum(dim=-1)\n",
    "                idx                     = torch.arange(batch_size,dtype=torch.long).repeat(k).view(k,-1).transpose(0,1).cuda()\n",
    "                min_val, min            = torch.topk(diff_path, k=k, dim=1,largest=False)\n",
    "                decoder_output[:,:,0]   = multi_path_x[idx,min,:].mean(dim=-2).view(batch_size,1)\n",
    "                decoder_output[:,:,1]   = multi_path_y[idx,min,:].mean(dim=-2).view(batch_size,1)\n",
    "\n",
    "            # Log output\n",
    "            outputs[:,t,:]                        = decoder_output.squeeze(1)\n",
    "            stochastic_outputs[:,t,:]             = stochastic_decoder_output.squeeze(1)\n",
    "            decoder_input                         = output_tensor[:,t,:] if teacher_force else decoder_output\n",
    "\n",
    "        return outputs, stochastic_outputs, visual_vsn_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def save_checkpoint(state, is_best, save_path, filename):\n",
    "    torch.save(state, os.path.join(save_path,filename))\n",
    "    if is_best:\n",
    "        shutil.copyfile(os.path.join(save_path,filename), os.path.join(save_path,'model_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, criterion, criterion_vision, clip,train_loader, validation_loader):\n",
    "    global batch_size\n",
    "    i               = None\n",
    "    checked_frame   = 0\n",
    "\n",
    "    losses = []\n",
    "    print(\"Data Size \",data_size,\"\\tChunk Size \",chunk_size)\n",
    "    global teacher_forcing_ratio\n",
    "    counter =0\n",
    "    best_val = float(\"inf\")\n",
    "    save_path = './save_models'\n",
    "    for j in range(epoch_num):\n",
    "        model.train()\n",
    "        epoch_loss=0\n",
    "        if j%7 == 6:\n",
    "            teacher_forcing_ratio = (teacher_forcing_ratio - 0.2) if teacher_forcing_ratio>=0.1 else 0.0\n",
    "\n",
    "        # Update TeachForce ratio to gradually change during training\n",
    "        # global teacher_forcing_ratio\n",
    "        # teacher_forcing_ratio-= 1/epoch_num\n",
    "        print(\"TEACHER FORCE RATIO\\t\",teacher_forcing_ratio)\n",
    "        #print(\"Learning Rate\\t\", scheduler.get_last_lr())\n",
    "\n",
    "        \n",
    "        if(j>=from_epoch):\n",
    "            optimizer.zero_grad()\n",
    "            start_time = time.time()\n",
    "            ADE = 0\n",
    "            FDE = 0\n",
    "            i   = 0\n",
    "            for i,data in enumerate(train_loader):\n",
    "                #print(\"\\n--------------- Batch %d/ %d ---------------\"%(j,i)) #(epoch/i)\n",
    "                # Forward\n",
    "                obs, pred, visual_obs, frame_tensor              = data\n",
    "                input_tensor, output_tensor                      = obs.float().squeeze().to('cuda', non_blocking=True), pred.float().squeeze().to('cuda', non_blocking=True)               #(obs.to(device), pred.to(device))\n",
    "                visual_input_tensor                              = visual_obs.squeeze().to('cuda', non_blocking=True)  #(visual_obs.to(device), visual_pred.to(device))\n",
    "                prediction, stochastic_prediction, visual_embedding = model(input_tensor,visual_input_tensor,output_tensor,batch_size,train_mode=1)\n",
    "\n",
    "                calculated_prediction = prediction.cumsum(axis=1) #calculated_prediction\n",
    "\n",
    "                loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction) #loss (regularisation term Lreg)\n",
    "                \n",
    "                if biased_loss_mode:\n",
    "                    weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "                    weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "                    loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "                else:\n",
    "                    loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2)) #mean squared error (lmse)\n",
    "                out_x       = output_tensor[:,:,0].cumsum(axis=1)\n",
    "                out_y       = output_tensor[:,:,1].cumsum(axis=1)\n",
    "                pred_x      = calculated_prediction[:,:,0]\n",
    "                pred_y      = calculated_prediction[:,:,1]\n",
    "                ADE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "                # FDE      += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "                \n",
    "                # Backward Propagation\n",
    "                total_loss      = loss.double() + torch.tensor(regularization_factor).to('cuda', non_blocking=True) * loss_line_regularizer.double() #total loss\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "                optimizer.step()\n",
    "                #print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "                epoch_loss += total_loss.item()\n",
    "                #print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "                start_time = time.time()\n",
    "                torch.cuda.empty_cache()\n",
    "                writer.close()\n",
    "                count_div=i\n",
    "            \n",
    "            # tensorboard log\n",
    "            writer.add_scalar('ADE/train', ADE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('FDE/train', FDE.item()/(count_div+1), counter)\n",
    "            # writer.add_scalar('LOSS/train', epoch_loss/(count_div+1), counter)\n",
    "            counter += 1\n",
    "\n",
    "        if scheduler.get_last_lr()[0]>0.001:\n",
    "            scheduler.step()\n",
    "        # validation(model, optimizer, criterion, criterion_vision, clip, validation_loader, j) \n",
    "        epoch_loss = epoch_loss / (int(data_size/chunk_size))\n",
    "        losses.append(epoch_loss)\n",
    "        display.clear_output(wait=True)\n",
    "        plt.plot(losses, '--ro', label='train loss')\n",
    "        plt.legend()\n",
    "        plt.title(f'epoch {j}')\n",
    "        plt.show()\n",
    "        print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "        print(\"EPOCH \", j, \"\\tLOSS \", epoch_loss)\n",
    "        writer.add_scalar('epoch_loss/train', epoch_loss/ (int(data_size/chunk_size)), j ) #see how model performs on the training dataset\n",
    "        #torch.save( model.state_dict(), model_path+\"_current\")\n",
    "        print(\"-----------------------------------------------\\n\"+\"-----------------------------------------------\")\n",
    "\n",
    "        # save checkpoint for each epoch and a fine called best_model so far \n",
    "        print(np.argmin(losses))\n",
    "        is_best = epoch_loss < best_val\n",
    "        best_val = min(epoch_loss, best_val)\n",
    "        print(\"bestvaleur\", best_val)\n",
    "        save_checkpoint({'epoch': j+1,'state_dict': model.module.state_dict(),'optimizer': optimizer.state_dict(),'scheduler': scheduler.state_dict(),'best_loss': best_val}, is_best, save_path, 'epoch_{}.pth'.format(j+1))\n",
    "        \n",
    "    return epoch_loss / (int(data_size/chunk_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, optimizer, criterion, criterion_vision, clip, validation_loader, counter):\n",
    "    global batch_size\n",
    "    model.eval()\n",
    "    i           = None\n",
    "    ADEs        = 0\n",
    "    FDEs        = 0\n",
    "    epoch_loss  = 0\n",
    "    loss_line_regularizer = 0\n",
    "    loss = 0 \n",
    "    total_loss = 0\n",
    "    ADE  = 0\n",
    "    FDE  = 0\n",
    "    for i,data in enumerate(test_loader):\n",
    "        # Forward\n",
    "        obs, pred, visual_obs, frame_tensor = data\n",
    "        input_tensor, output_tensor         = obs.float().squeeze().to('cuda', non_blocking=True), pred.float().squeeze().to('cuda', non_blocking=True)#(obs.to(device), pred.to(device))\n",
    "        visual_input_tensor                 = visual_obs.squeeze().to('cuda', non_blocking=True)   #(visual_obs.to(device), visual_pred.to(device))\n",
    "        prediction, stochastic_prediction, visual_embedding = model(input_tensor, visual_input_tensor, output_tensor, batch_size, train_mode=0)\n",
    "        \n",
    "        calculated_prediction = prediction.cumsum(axis=1) \n",
    "\n",
    "        loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction)\n",
    "        \n",
    "        if biased_loss_mode:\n",
    "            weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "            weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "            loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "        else:\n",
    "            loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2))\n",
    "        out_x       = output_tensor[:,:,0].cumsum(axis=1)\n",
    "        out_y       = output_tensor[:,:,1].cumsum(axis=1)\n",
    "        pred_x      = calculated_prediction[:,:,0]\n",
    "        pred_y      = calculated_prediction[:,:,1]\n",
    "        ADE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "        FDE         += ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "        total_loss  += loss.double() + regularization_factor * loss_line_regularizer.double() \n",
    "        print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "\n",
    "    writer.add_scalar('ADE/val_'+path_mode,             ADE.item()/(i+1),             counter)\n",
    "    writer.add_scalar('FDE/val_'+path_mode,             FDE.item()/(i+1),             counter)\n",
    "    writer.add_scalar('LOSS/val_'+path_mode,            total_loss.item()/(i+1)   ,   counter)\n",
    "    writer.add_scalar('LOSS_c/val_'+path_mode,          loss.item()/(i+1)        ,    counter)\n",
    "    writer.add_scalar('L-REGULARIZER/val_'+path_mode,   loss_line_regularizer.item()/(i+1), counter)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_eval(model, optimizer, criterion, criterion_vision, clip, five_fold_cross_validation):\n",
    "    global batch_size\n",
    "    model.eval()\n",
    "    i           = None\n",
    "    ADEs        = 0\n",
    "    FDEs        = 0\n",
    "    epoch_loss  = 0\n",
    "    list_x_obs          = ['x_obs_'+str(i)              for i in range(0,T_obs)] #x_obs_0 --> x_obs_7\n",
    "    list_y_obs          = ['y_obs_'+str(i)              for i in range(0,T_obs)] #y_obs_0 --> y_obs_7\n",
    "    #list_c_context      = ['context_c_'+str(i)          for i in range(0,hidden_size)] #context_c_0 --> context_c_255\n",
    "    #list_h_context      = ['context_h_'+str(i)          for i in range(0,hidden_size)] #context_h_0 --> context_h_255\n",
    "    list_x_pred         = ['x_pred_'+str(i)             for i in range(0,T_pred)] #x_pred_0 --> x_pred_11\n",
    "    list_y_pred         = ['y_pred_'+str(i)             for i in range(0,T_pred)] #y_pred_0 --> y_pred_11\n",
    "    list_x_stoch_pred_m = ['x_stoch_pred_m_'+str(i)     for i in range(0,T_pred)] #x_stoch_pred_m_0 --> x_stoch_pred_m_11\n",
    "    list_y_stoch_pred_m = ['y_stoch_pred_m_'+str(i)     for i in range(0,T_pred)] #y_stoch_pred_m_0 --> y_stoch_pred_m_11\n",
    "    list_x_stoch_pred_s = ['x_stoch_pred_s_'+str(i)     for i in range(0,T_pred)] #x_stoch_pred_s_0 --> x_stoch_pred_s_11\n",
    "    list_y_stoch_pred_s = ['y_stoch_pred_s_'+str(i)     for i in range(0,T_pred)] #y_stoch_pred_s_0 --> y_stoch_pred_s_11\n",
    "    list_x_out          = ['x_out_'+str(i)              for i in range(0,T_pred)] #x_out_0 --> x_out_11\n",
    "    list_y_out          = ['y_out_'+str(i)              for i in range(0,T_pred)] #y_out_0 --> y_out_11\n",
    "    list_vsn            = ['vsn_'+str(i)               for i in range(0,hidden_size)] #vsn_0 --> vsn_255\n",
    "    df_out              = pd.DataFrame(columns=list_x_obs + list_y_obs + list_x_out + list_y_out + list_x_pred + list_y_pred + list_x_stoch_pred_m + list_y_stoch_pred_m + list_x_stoch_pred_s + list_y_stoch_pred_s + list_vsn)# + list_vsn_visual + list_c_context + list_h_context)\n",
    "\n",
    "    for i,data in enumerate(test_loader):\n",
    "        start_time = time.time()\n",
    "        # Forward\n",
    "        obs, pred, visual_obs, frame_tensor                 = data\n",
    "        input_tensor, output_tensor                         = obs.float().squeeze().to('cuda', non_blocking=True), pred.float().squeeze().to('cuda', non_blocking=True)               #(obs.to(device), pred.to(device))\n",
    "        visual_input_tensor                                 = visual_obs.squeeze().cuda()   #(visual_obs.to(device), visual_pred.to(device))\n",
    "        prediction, stochastic_prediction, visual_embedding = model(input_tensor,visual_input_tensor,output_tensor,batch_size,train_mode=0)\n",
    "\n",
    "        calculated_prediction =  prediction.cumsum(axis=1) \n",
    "\n",
    "        loss_line_regularizer = distance_from_line_regularizer(input_tensor,calculated_prediction) #lreg\n",
    "\n",
    "        if biased_loss_mode:\n",
    "            weight  = torch.arange(1,2*T_pred+1,2).cuda().float()\n",
    "            weight  = torch.exp(weight / T_pred).repeat(prediction.size(0)).view(prediction.size(0),T_pred,1)\n",
    "            loss    = criterion( (calculated_prediction)*weight, torch.cumsum(output_tensor,dim=-2)*weight)\n",
    "        else:\n",
    "            loss    = criterion( (calculated_prediction), torch.cumsum(output_tensor,dim=-2)) #lmse\n",
    "        out_x           = output_tensor[:,:,0].cumsum(axis=1)\n",
    "        out_y           = output_tensor[:,:,1].cumsum(axis=1)\n",
    "        pred_x          = calculated_prediction[:,:,0]\n",
    "        pred_y          = calculated_prediction[:,:,1]\n",
    "        ADE             = ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0).mean(0)   \n",
    "        FDE             = ((out_x.sub(pred_x)**2).add((out_y.sub(pred_y)**2))**(1/2)).mean(0)[-1]\n",
    "        total_loss      = loss.double() + regularization_factor * loss_line_regularizer.double() #loss\n",
    "        print(\"Total Loss\\t{:.2f}\".format(total_loss.item()))\n",
    "        epoch_loss += total_loss.item()\n",
    "        ADEs    += ADE.item()\n",
    "        FDEs    += FDE.item()\n",
    "        input_x_lin                 = input_tensor[:,:,0].view(-1, T_obs).cpu() #x_obs\n",
    "        input_y_lin                 = input_tensor[:,:,1].view(-1, T_obs).cpu() #y_obs\n",
    "        output_x_lin                = output_tensor[:,:,0].view(-1, T_pred).cpu() #x_out\n",
    "        output_y_lin                = output_tensor[:,:,1].view(-1, T_pred).cpu() #y_out\n",
    "        prediction_x_lin            = prediction[:,:,0].view(-1, T_pred).cpu() #x_pred\n",
    "        prediction_y_lin            = prediction[:,:,1].view(-1, T_pred).cpu() #y_pred\n",
    "        stoch_prediction_x_m        = stochastic_prediction[:,:,0].view(-1, T_pred).cpu() #x_stoch_pred_m\n",
    "        stoch_prediction_x_s        = stochastic_prediction[:,:,1].view(-1, T_pred).cpu() #x_stoch_pred_s\n",
    "        stoch_prediction_y_m        = stochastic_prediction[:,:,2].view(-1, T_pred).cpu() #y_stoch_pred_m\n",
    "        stoch_prediction_y_s        = stochastic_prediction[:,:,3].view(-1, T_pred).cpu() #y_stoch_pred_s\n",
    "        #context_h_lin               = encoder_hidden[0].view(-1, hidden_size).cpu() #context_h\n",
    "        #context_c_lin               = encoder_hidden[1].view(-1, hidden_size).cpu() #context_c\n",
    "        visual_embedding_weights    = visual_embedding.view(-1, hidden_size).cpu() #vsn\n",
    "\n",
    "        whole_data                  = torch.cat((input_x_lin, input_y_lin, output_x_lin, output_y_lin, prediction_x_lin, prediction_y_lin, stoch_prediction_x_m, stoch_prediction_y_m, stoch_prediction_x_s, stoch_prediction_y_s, visual_embedding_weights), 1) #,context_c_lin, context_h_lin\n",
    "        temp                        = pd.DataFrame(whole_data.detach().cpu().numpy(), columns=list_x_obs + list_y_obs + list_x_out + list_y_out + list_x_pred + list_y_pred + list_x_stoch_pred_m + list_y_stoch_pred_m + list_x_stoch_pred_s + list_y_stoch_pred_s + list_vsn) #+ list_c_context + list_h_context\n",
    "        df_out                      = df_out.append(temp)\n",
    "        df_out.reset_index(drop=True,inplace=True)\n",
    "\n",
    "        print(\"Time\\t\\t{:.2f} sec \\n\".format(time.time() - start_time))\n",
    "\n",
    "    # ADE/FDE Report\n",
    "    out_x  = df_out[['x_out_' +str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    pred_x = df_out[['x_pred_'+str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    out_y  = df_out[['y_out_' +str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    pred_y = df_out[['y_pred_'+str(i) for i in range(0,T_pred)]].cumsum(axis=1)\n",
    "    ADE = (out_x.sub(pred_x.values)**2).add((out_y.sub(pred_y.values)**2).values, axis=1)**(1/2)\n",
    "    df_out['ADE'] = ADE.mean(axis=1)\n",
    "    FDE = ADE.x_out_11\n",
    "    df_out['FDE'] = FDE\n",
    "    Mean_ADE = df_out.ADE.mean()\n",
    "    Mean_FDE = df_out.FDE.mean()\n",
    "    print(\"MEAN ADE/FDE\\t\",Mean_ADE,Mean_FDE)\n",
    "    writer.add_scalar(\"Final_Test/ADE_\"+path_mode, Mean_ADE, global_step=0)\n",
    "    writer.add_scalar(\"Final_Test/FDE_\"+path_mode, Mean_FDE, global_step=0)\n",
    "\n",
    "    df_out.to_sql(table_out+'_'+path_mode, cnx2, if_exists=\"replace\", index=False)\n",
    "    writer.close()\n",
    "    return ADEs, FDEs, int(data_size/chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model                       = Seq2Seq(in_size, embed_size, hidden_size, dropout_val=dropout_val, batch_size=batch_size)\n",
    "model                       = nn.DataParallel( model ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A summary of the model, where we can see the shape of each layer : \n",
      "DataParallel(\n",
      "  (module): Seq2Seq(\n",
      "    (encoder): CoordinatesTransformer(\n",
      "      (linear_mapper): Sequential(\n",
      "        (0): Linear(in_features=2, out_features=32, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (3): ReLU()\n",
      "        (4): Linear(in_features=64, out_features=128, bias=True)\n",
      "        (5): ReLU()\n",
      "        (6): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (7): ReLU()\n",
      "      )\n",
      "      (self_attention): EncoderSelfAttention(\n",
      "        (encoder): ModuleList(\n",
      "          (0): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (1): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (2): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (3): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (4): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (5): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=256, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=256, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): AdaptiveAvgPool2d(output_size=(1, 256))\n",
      "      (embedder_out): Sequential(\n",
      "        (0): Linear(in_features=2048, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "        (4): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (decoder): DecoderTransformer(\n",
      "      (embedder_rho): Linear(in_features=2, out_features=64, bias=True)\n",
      "      (fC_mu): Sequential(\n",
      "        (0): Linear(in_features=514, out_features=128, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Dropout(p=0.2, inplace=False)\n",
      "        (3): Linear(in_features=128, out_features=4, bias=True)\n",
      "      )\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "      (FC_dim_red): Sequential(\n",
      "        (0): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
      "        (1): Flatten(start_dim=1, end_dim=-1)\n",
      "        (2): Linear(in_features=7396, out_features=514, bias=True)\n",
      "        (3): ReLU()\n",
      "      )\n",
      "      (embedding): Linear(in_features=64, out_features=512, bias=True)\n",
      "      (layers): ModuleList(\n",
      "        (0): DecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (1): DecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (2): DecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (3): DecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (4): DecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (5): DecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "          (feed_forward): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (output_layer): Linear(in_features=512, out_features=514, bias=True)\n",
      "    )\n",
      "    (vsn_module): _GestureTransformer(\n",
      "      (conv_model): Sequential(\n",
      "        (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "        (4): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (7): Sequential(\n",
      "          (0): BasicBlock(\n",
      "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (downsample): Sequential(\n",
      "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (1): BasicBlock(\n",
      "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (features): features_extraction(\n",
      "        (conv_model): Sequential(\n",
      "          (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "          (4): Sequential(\n",
      "            (0): BasicBlock(\n",
      "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "            (1): BasicBlock(\n",
      "              (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (5): Sequential(\n",
      "            (0): BasicBlock(\n",
      "              (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): BasicBlock(\n",
      "              (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (6): Sequential(\n",
      "            (0): BasicBlock(\n",
      "              (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): BasicBlock(\n",
      "              (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (7): Sequential(\n",
      "            (0): BasicBlock(\n",
      "              (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (downsample): Sequential(\n",
      "                (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "                (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              )\n",
      "            )\n",
      "            (1): BasicBlock(\n",
      "              (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "              (relu): ReLU(inplace=True)\n",
      "              (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "              (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (pooling): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      )\n",
      "      (self_attention): EncoderSelfAttention(\n",
      "        (encoder): ModuleList(\n",
      "          (0): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (1): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (2): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (3): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (4): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "          (5): MultiHeadAttention(\n",
      "            (attention): ScaledDotProductAttention(\n",
      "              (fc_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "              (fc_o): Linear(in_features=512, out_features=512, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.2, inplace=False)\n",
      "            (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "            (fc): Sequential(\n",
      "              (0): Linear(in_features=512, out_features=1024, bias=True)\n",
      "              (1): ReLU(inplace=True)\n",
      "              (2): Dropout(p=0.2, inplace=False)\n",
      "              (3): Linear(in_features=1024, out_features=512, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pool): AdaptiveAvgPool2d(output_size=(1, 512))\n",
      "    )\n",
      "    (pooling): AdaptiveAvgPool1d(output_size=256)\n",
      "    (crossAttention): crossAttention(\n",
      "      (encoder): CrossAttention(\n",
      "        (encoder): Encoder(\n",
      "          (layers): ModuleList(\n",
      "            (0): EncoderLayer(\n",
      "              (src_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (cross_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (feed_forward): PointerwiseFeedforward(\n",
      "                (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (sublayer): ModuleList(\n",
      "                (0): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (1): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (2): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (3): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (1): EncoderLayer(\n",
      "              (src_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (cross_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (feed_forward): PointerwiseFeedforward(\n",
      "                (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (sublayer): ModuleList(\n",
      "                (0): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (1): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (2): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (3): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "            (2): EncoderLayer(\n",
      "              (src_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (cross_attn): MultiHeadAttention(\n",
      "                (linears): ModuleList(\n",
      "                  (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (1): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (2): Linear(in_features=256, out_features=256, bias=True)\n",
      "                  (3): Linear(in_features=256, out_features=256, bias=True)\n",
      "                )\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (feed_forward): PointerwiseFeedforward(\n",
      "                (w_1): Linear(in_features=256, out_features=2048, bias=True)\n",
      "                (w_2): Linear(in_features=2048, out_features=256, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (sublayer): ModuleList(\n",
      "                (0): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (1): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (2): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (3): SubLayerConnection(\n",
      "                  (norm): LayerNorm()\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (norm): LayerNorm()\n",
      "        )\n",
      "        (out): Embedding(512, 512)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"A summary of the model, where we can see the shape of each layer : \")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_step               = 40\n",
    "initial_learning_rate       = 0.01\n",
    "clip                        = 1\n",
    "\n",
    "# MSE loss\n",
    "criterion                   = nn.MSELoss(reduction='mean')#nn.NLLLoss()\n",
    "criterion_vision            = nn.MSELoss(reduction='sum')#nn.NLLLoss()\n",
    "\n",
    "# SGD optimizer\n",
    "optimizer                   = optim.SGD(model.parameters(), lr=initial_learning_rate, momentum=0.9, weight_decay=0.01) #SGD\n",
    "scheduler                   = torch.optim.lr_scheduler.StepLR(optimizer, step_size=learning_step, gamma=0.1)\n",
    "five_fold_cross_validation  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing train dataset\n"
     ]
    }
   ],
   "source": [
    "#train dataset and loader\n",
    "print(\"Initializing train dataset\")\n",
    "dataset_train = TrajectoryPredictionDataset(image_folder_path, DB_PATH_train, cnx_train)\n",
    "train_loader  = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=2, drop_last=True, pin_memory=True)\n",
    "validation_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGzCAYAAAAxPS2EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABDM0lEQVR4nO3deXxU1f3/8fckkCEBJmFLAhIgfkUggJbFQpRFa0qwcSlgLRQRWeQHhlagKuJX0dpqEL+1ggtU/Sr2W1GkggqINLK6RJZgZJOUSjARSIJgZtiSQHJ+f8TcMrJlZpLcxHk9H4/7kLn3zJnPPVjn3XPnnuswxhgBAADUIyF2FwAAAOArAgwAAKh3CDAAAKDeIcAAAIB6hwADAADqHQIMAACodwgwAACg3iHAAACAeocAAwAA6h0CDIB649FHH5XD4dC3335rdykAbEaAARA0srOzNXXqVF199dVq1KiRHA6H9u3bd862ixYt0u23366OHTvK4XDo2muvPW+/mZmZGjx4sFwul5o2bapBgwYpKyurRs4BQAUCDICgkZGRoblz5+ro0aPq0qXLBdvOmzdP7777ruLi4tSsWbPzttu6dav69eunvXv36pFHHtHMmTO1Z88eDRw4UNnZ2dV9CgC+18DuAgCgttx8880qKipS06ZN9T//8z8XnCX5v//7P11yySUKCQlRt27dztvu4YcfVnh4uDIyMtSiRQtJ0u23367LL79cDz74oN5+++3qPg0AYgYGwA/s379fY8eOVUxMjJxOp7p27apXXnnFq826devkcDi0aNEiPfjgg4qNjVXjxo118803Ky8v76w+Fy9erF69eik8PFwtW7bU7bffrv3795/Vbvfu3brtttvUqlUrhYeHq1OnTvrv//7vs9oVFRXpzjvvVFRUlCIjIzVmzBidOHHioufWvHlzNW3atErjEBcXp5CQi/8n8qOPPlJSUpIVXiSpdevWGjhwoJYvX65jx45V6fMA+IYZGACWgoIC9e3bVw6HQ5MnT1arVq20cuVKjRs3Th6PR1OmTPFq//jjj8vhcGj69OkqLCzUM888o6SkJGVlZSk8PFyStGDBAo0ZM0ZXXXWV0tLSVFBQoDlz5uiTTz7R559/rqioKEnStm3b1L9/fzVs2FATJkxQhw4d9NVXX2nZsmV6/PHHvT73tttuU3x8vNLS0rR161a9/PLLio6O1pNPPlkbw+SlpKTEOtczRUREqLS0VDt27FDfvn1rvS7gR88AwPfGjRtnWrdubb799luv/cOHDzeRkZHmxIkTxhhj1q5daySZSy65xHg8HqvdW2+9ZSSZOXPmGGOMKS0tNdHR0aZbt27m5MmTVrvly5cbSWbmzJnWvgEDBpimTZuar7/+2uuzy8vLrT8/8sgjRpIZO3asV5shQ4aYFi1a+HSuTz31lJFkcnJyLtq2a9euZuDAgec81r17d3P55Zeb06dPW/tKSkpMu3btjCTzj3/8w6e6AFQNl5AASJKMMXr77bd10003yRijb7/91tqSk5Pldru1detWr/fccccdXpdkbr31VrVu3Vrvv/++JGnLli0qLCzU3XffrUaNGlntUlJS1LlzZ61YsUKSdOjQIW3YsEFjx45Vu3btvD7D4XCcVevEiRO9Xvfv31+HDx+Wx+MJbBD8cPfdd+tf//qXxo0bp127dmnHjh264447dPDgQUnSyZMna70mIBgQYABIqggRRUVFevHFF9WqVSuvbcyYMZKkwsJCr/d07NjR67XD4dBll11m3Zr89ddfS5I6dep01ud17tzZOr53715JuuCPZc/0w5BTeZfQd999V6X3V6eJEyfqwQcf1MKFC9W1a1d1795dX331le6//35JUpMmTWq9JiAY8BsYAJKk8vJySRV30IwePfqcba644oraLOm8QkNDz7nfGFPLlVR4/PHHde+992rnzp2KjIxU9+7d9eCDD0qSLr/8cltqAn7sCDAAJEmtWrVS06ZNVVZWpqSkpCq9Z8+ePV6vjTH697//bQWd9u3bS6pYQO5nP/uZV9vs7Gzr+KWXXipJ2rFjR0DnYKdmzZqpX79+1usPP/xQbdu2VefOnW2sCvjx4hISAEkVsxrDhg3T22+/fc4gcejQobP2/e1vf9PRo0et1//4xz908OBB3XDDDZKk3r17Kzo6WvPnz1dJSYnVbuXKlfryyy+VkpIiqSI8DRgwQK+88opyc3O9PsOuWZVALFq0SJs3b9aUKVOqdCs2AN8xAwPAMmvWLK1du1Z9+vTRXXfdpYSEBB05ckRbt27Vhx9+qCNHjni1b968ufr166cxY8aooKBAzzzzjC677DLdddddkqSGDRvqySef1JgxYzRw4ECNGDHCuo26Q4cOmjp1qtXX3Llz1a9fP/Xs2VMTJkxQfHy89u3bpxUrVlTbsvxut1vPPvusJOmTTz6RJD333HOKiopSVFSUJk+ebLXdsGGDNmzYIKkivB0/flx/+tOfJEkDBgzQgAEDrHaPPfaYBg0apBYtWuizzz7Tq6++qsGDB+uee+6plroBnIOt90ABqHMKCgpMamqqiYuLMw0bNjSxsbHm+uuvNy+++KLVpvI26jfeeMPMmDHDREdHm/DwcJOSknLWbdDGGLNo0SLTo0cP43Q6TfPmzc3IkSPNN998c1a7HTt2mCFDhpioqCjTqFEj06lTJ/Pwww9bxytvoz506JDX+1599dUq3RKdk5NjJJ1za9++vVfbys861/bII49Y7f7973+bQYMGmZYtWxqn02k6d+5s0tLSTElJyQVrARAYhzH1cH4WgK3WrVun6667TosXL9att95qdzkAghAXZwEAQL1DgAEAAPUOAQYAANQ7/AYGAADUO8zAAACAeocAAwAA6h2fFrLr0KGD9fC1M9199916/vnnVVxcrN///vd68803VVJSouTkZL3wwguKiYmx2ubm5mrSpElau3atmjRpotGjRystLU0NGvynlHXr1mnatGnauXOn4uLi9NBDD+nOO+/06cTKy8t14MABNW3a9JxPswUAAHWPMUZHjx5VmzZtLryStS+LxhQWFpqDBw9aW3p6upFk1q5da4wxZuLEiSYuLs6sXr3abNmyxfTt29dcffXV1vtPnz5tunXrZpKSksznn39u3n//fdOyZUszY8YMq83evXtNRESEmTZtmtm1a5d59tlnTWhoqPnggw98WuAmLy/vvItQsbGxsbGxsdXtLS8v74Lf8wH9iHfKlClavny59uzZI4/Ho1atWmnhwoXWwla7d+9Wly5dlJGRob59+2rlypW68cYbdeDAAWtWZv78+Zo+fboOHTqksLAwTZ8+XStWrPB6Fsvw4cNVVFSkDz744Ly1lJSUeD1rxe12q127dsrLy5PL5fL3FAEAQC3yeDyKi4tTUVGRIiMjz9vO72chlZaW6u9//7umTZsmh8OhzMxMnTp1yusptp07d1a7du2sAJORkaHu3bt7XVJKTk7WpEmTtHPnTvXo0UMZGRlnPQk3OTlZU6ZMuWA9aWlp+sMf/nDWfpfLRYABAKCeudjPP/z+Ee8777yjoqIi67cp+fn5CgsLU1RUlFe7mJgY5efnW23ODC+VxyuPXaiNx+PRyZMnz1vPjBkz5Ha7rS0vL8/fUwMAAHWc3zMw//u//6sbbrhBbdq0qc56/OZ0OuV0Ou0uAwAA1AK/ZmC+/vprffjhhxo/fry1LzY2VqWlpSoqKvJqW1BQoNjYWKtNQUHBWccrj12ojcvlUnh4uD/lAgCAHxm/ZmBeffVVRUdHKyUlxdrXq1cvNWzYUKtXr9awYcMkSdnZ2crNzVViYqIkKTExUY8//rgKCwsVHR0tSUpPT5fL5VJCQoLV5v333/f6vPT0dKsPAACMMTp9+rTKysrsLgU+Cg0NVYMGDQJe4sTnAFNeXq5XX31Vo0eP9lq7JTIyUuPGjdO0adPUvHlzuVwu/fa3v1ViYqL69u0rSRo0aJASEhI0atQozZ49W/n5+XrooYeUmppqXf6ZOHGinnvuOd1///0aO3as1qxZo7feeksrVqwI6EQBAD8OpaWlOnjwoE6cOGF3KfBTRESEWrdurbCwML/78DnAfPjhh8rNzdXYsWPPOvaXv/xFISEhGjZsmNdCdpVCQ0O1fPlyTZo0SYmJiWrcuLFGjx6txx57zGoTHx+vFStWaOrUqZozZ47atm2rl19+WcnJyX6eIgDgx6K8vFw5OTkKDQ1VmzZtFBYWxmKl9YgxRqWlpTp06JBycnLUsWPHCy9WdwE/2oc5ejweRUZGyu12cxs1APxIFBcXKycnR+3bt1dERITd5cBPJ06c0Ndff634+Hg1atTI61hVv795FhIAoN7x9/+1o26ojr8/v2+jDkplZdJHH0kHD0qtW0v9+0uhoXZXBQBA0CHAVNWSJdI990jffPOffW3bSnPmSEOH2lcXAABBiDm4qliyRLr1Vu/wIkn791fsX7LEnroAAP4rK5PWrZPeeKPin/XsluwOHTromWeesb0PuxBgLqasrGLm5Vy/da7cN2VKvfsXHwCC2pIlUocO0nXXSb/5TcU/O3So0f9Deu211170uX6+2Lx5syZMmFBt/dU3BJiL+eijs2dezmSMlJdX0Q4AUPfV4Vn1ygX6qqJVq1ZBfScWAeZiDh6s3nYAgJpx/Pj5t+LiijZVmVW/5x7vWfXz9emDO++8U+vXr9ecOXPkcDjkcDi0b98+rVu3Tg6HQytXrlSvXr3kdDr18ccf66uvvtItt9yimJgYNWnSRFdddZU+/PBDrz5/ePnH4XDo5Zdf1pAhQxQREaGOHTvqvffe86nO3Nxc3XLLLWrSpIlcLpduu+02r8f7fPHFF7ruuuvUtGlTuVwu9erVS1u2bJFU8Zihm266Sc2aNVPjxo3VtWvXs1bWr04EmItp3bp62wEAakaTJuffvn/ETZVm1b/5xntWvUOHc/fpgzlz5igxMVF33XWXDh48qIMHDyouLs46/sADD2jWrFn68ssvdcUVV+jYsWP6xS9+odWrV+vzzz/X4MGDddNNNyk3N/eCn/OHP/xBt912m7Zt26Zf/OIXGjlypI4cOVKlGsvLy3XLLbfoyJEjWr9+vdLT07V37179+te/ttqMHDlSbdu21ebNm5WZmakHHnhADRs2lCSlpqaqpKREGzZs0Pbt2/Xkk0+qiY/j5AvuQrqY/v0r7jbav//cid3hqDjev3/t1wYA8I1Ns+qRkZEKCwtTRESE9fDiMz322GP6+c9/br1u3ry5rrzySuv1H//4Ry1dulTvvfeeJk+efN7PufPOOzVixAhJ0hNPPKG5c+dq06ZNGjx48EVrXL16tbZv366cnBwrXP3tb39T165dtXnzZl111VXKzc3Vfffdp86dO0uSOnbsaL0/NzdXw4YNU/fu3SVJl1566UU/MxDMwFxMaGjFrdJSRVg5U+XrZ55hPRgAsNuxY+ff3n67oo0/s+r79p27z2rUu3dvr9fHjh3Tvffeqy5duigqKkpNmjTRl19+edEZmCuuuML6c+PGjeVyuVRYWFilGr788kvFxcV5zQwlJCQoKipKX375pSRp2rRpGj9+vJKSkjRr1ix99dVXVtvf/e53+tOf/qRrrrlGjzzyiLZt21alz/UXAaYqhg6V/vEP6ZJLvPe3bVuxn3VgAMB+jRuff6tcrr5yVv18z09yOKS4OO9Z9fP1Wa2le/d37733aunSpXriiSf00UcfKSsrS927d1dpaekF+6m8nFPJ4XCovLy82up89NFHtXPnTqWkpGjNmjVKSEjQ0qVLJUnjx4/X3r17NWrUKG3fvl29e/fWs88+W22f/UMEmKoaOrQihd94Y8XrMWOknBzCCwDUJzbOqoeFhamsiktufPLJJ7rzzjs1ZMgQde/eXbGxsdq3b1+113SmLl26KC8vT3l5eda+Xbt2qaioSAkJCda+yy+/XFOnTtU///lPDR06VK+++qp1LC4uThMnTtSSJUv0+9//Xi+99FKN1UuA8UVoaEUyl6R27bhsBAD1kU2z6h06dNDGjRu1b98+ffvttxecGenYsaOWLFmirKwsffHFF/rNb35TrTMp55KUlKTu3btr5MiR2rp1qzZt2qQ77rhDAwcOVO/evXXy5ElNnjxZ69at09dff61PPvlEmzdvVpcuXSRJU6ZM0apVq5STk6OtW7dq7dq11rGaQIDxVWVoYeE6AKi/KmfV166VFi6s+GcNz6rfe++9Cg0NVUJCglq1anXB37M8/fTTatasma6++mrddNNNSk5OVs+ePWusNqnictO7776rZs2aacCAAUpKStKll16qRYsWSZJCQ0N1+PBh3XHHHbr88st122236YYbbtAf/vAHSVJZWZlSU1PVpUsXDR48WJdffrleeOGFmqvXmHPdWlP/VfVx3D7Ly5MOH5ZiYys2AECtKS4uVk5OjuLj49Wo8nctqHcu9PdY1e9vbqP2VVzcfy4jAQAAW3AJCQAA1DvMwPhq3Trp44+lq66SkpPtrgYAgKDEDIyvPvxQevhhacUKuysBACBoEWB8xV1IAGC7H+n9J0GjOv7+CDC+qgwwNXw/PgDgbJUrzZ44ccLmShCIyr+/H64c7At+A+OrkO8zHzMwAFDrQkNDFRUVZT3fJyIiQo7zPRYAdY4xRidOnFBhYaGioqIUGsCCsAQYX3EJCQBsVfk056o+pBB1T1RU1Dmfyu0LAoyvCDAAYCuHw6HWrVsrOjpap06dsrsc+Khhw4YBzbxUIsD4igADAHVCaGhotXwRon4iwPhq+HApMVGKjra7EgAAghYBxleXXHL2E0wBAECt4jZqAABQ7zAD46vt26X0dOnSS6Vf/tLuagAACErMwPhq40bp97+XXnnF7koAAAhaBBhfcRcSAAC2I8D4qnIlXh4lAACAbQgwvmIGBgAA2xFgfEWAAQDAdgQYXxFgAACwHQHGVwQYAABsxzowvurXT/rnP6Xmze2uBACAoEWA8VVMjPTzn9tdBQAAQY1LSAAAoN7xOcDs379ft99+u1q0aKHw8HB1795dW7ZssY4bYzRz5ky1bt1a4eHhSkpK0p49e7z6OHLkiEaOHCmXy6WoqCiNGzdOx44d82qzbds29e/fX40aNVJcXJxmz57t5ylWs2++kV58UVq82O5KAAAIWj4FmO+++07XXHONGjZsqJUrV2rXrl3685//rGbNmlltZs+erblz52r+/PnauHGjGjdurOTkZBUXF1ttRo4cqZ07dyo9PV3Lly/Xhg0bNGHCBOu4x+PRoEGD1L59e2VmZuqpp57So48+qhdffLEaTjlAu3dL/+//SX/8o92VAAAQvIwPpk+fbvr163fe4+Xl5SY2NtY89dRT1r6ioiLjdDrNG2+8YYwxZteuXUaS2bx5s9Vm5cqVxuFwmP379xtjjHnhhRdMs2bNTElJiddnd+rU6byfXVxcbNxut7Xl5eUZScbtdvtyihe3erUxkjFdu1ZvvwAAwLjd7ip9f/s0A/Pee++pd+/e+tWvfqXo6Gj16NFDL730knU8JydH+fn5SkpKsvZFRkaqT58+ysjIkCRlZGQoKipKvXv3ttokJSUpJCREGzdutNoMGDBAYWFhVpvk5GRlZ2fru+++O2dtaWlpioyMtLa4uDhfTq3quI0aAADb+RRg9u7dq3nz5qljx45atWqVJk2apN/97nd67bXXJEn5+fmSpJiYGK/3xcTEWMfy8/MVHR3tdbxBgwZq3ry5V5tz9XHmZ/zQjBkz5Ha7rS0vL8+XU6s6AgwAALbz6Tbq8vJy9e7dW0888YQkqUePHtqxY4fmz5+v0aNH10iBVeV0OuV0Omv+gwgwAADYzqcZmNatWyshIcFrX5cuXZSbmytJio2NlSQVFBR4tSkoKLCOxcbGqrCw0Ov46dOndeTIEa825+rjzM+wDQEGAADb+RRgrrnmGmVnZ3vt+9e//qX27dtLkuLj4xUbG6vVq1dbxz0ejzZu3KjExERJUmJiooqKipSZmWm1WbNmjcrLy9WnTx+rzYYNG3Tq1CmrTXp6ujp16uR1x5MtCDAAANjPl18Gb9q0yTRo0MA8/vjjZs+ePeb11183ERER5u9//7vVZtasWSYqKsq8++67Ztu2beaWW24x8fHx5uTJk1abwYMHmx49epiNGzeajz/+2HTs2NGMGDHCOl5UVGRiYmLMqFGjzI4dO8ybb75pIiIizF//+tcq11rVXzH77LvvjFm61Jh//rN6+wUAAFX+/vYpwBhjzLJly0y3bt2M0+k0nTt3Ni+++KLX8fLycvPwww+bmJgY43Q6zfXXX2+ys7O92hw+fNiMGDHCNGnSxLhcLjNmzBhz9OhRrzZffPGF6devn3E6neaSSy4xs2bN8qnOGgswAACgxlT1+9thjDH2zgHVDI/Ho8jISLndbrlcLrvLAQAAVVDV728e5ugrj0d67z2pQQNp+HC7qwEAICgRYHxVUCCNGiW5XAQYAABswtOofcVdSAAA2I4A4ysCDAAAtiPA+IoAAwCA7QgwviLAAABgOwKMryoDTHm59OO8Ax0AgDqPAOOrygAjVYQYAABQ67iN2ldNmkivv14RZBwOu6sBACAoEWB85XRKv/mN3VUAABDUuIQEAADqHWZgfFVeXvEogbIy6eabpYYN7a4IAICgQ4DxVVmZNGRIxZ+PHJGaNbO3HgAAghCXkHx15l1IrAUDAIAtCDC+CjljyAgwAADYggDjD1bjBQDAVgQYfxBgAACwFQHGHwQYAABsRYDxR+XvYHiUAAAAtuA2an8891zF7EvLlnZXAgBAUCLA+OPOO+2uAACAoMYlJAAAUO8wA+OP9eulEyeka66RXC67qwEAIOgwA+OP3/xG+sUvpK++srsSAACCEgHGH9xGDQCArQgw/iDAAABgKwKMPwgwAADYigDjDwIMAAC2IsD4ozLAsBIvAAC2IMD4o/JRAszAAABgC9aB8ccDD0jffSd17Gh3JQAABCUCjD9uv93uCgAACGpcQgIAAPUOMzD+2LZNKiqSEhJ4IjUAADZgBsYfkyZJAwdKH31kdyUAAAQlAow/WAcGAABbEWD8QYABAMBWBBh/EGAAALAVAcYfrMQLAICtfAowjz76qBwOh9fWuXNn63hxcbFSU1PVokULNWnSRMOGDVNBQYFXH7m5uUpJSVFERISio6N133336fTp015t1q1bp549e8rpdOqyyy7TggUL/D/DmsBKvAAA2MrnGZiuXbvq4MGD1vbxxx9bx6ZOnaply5Zp8eLFWr9+vQ4cOKChQ4dax8vKypSSkqLS0lJ9+umneu2117RgwQLNnDnTapOTk6OUlBRdd911ysrK0pQpUzR+/HitWrUqwFOtRlxCAgDAVg5jjKlq40cffVTvvPOOsrKyzjrmdrvVqlUrLVy4ULfeeqskaffu3erSpYsyMjLUt29frVy5UjfeeKMOHDigmJgYSdL8+fM1ffp0HTp0SGFhYZo+fbpWrFihHTt2WH0PHz5cRUVF+uCDD85bW0lJiUpKSqzXHo9HcXFxcrvdcrlcVT3FqnnrLemrr6Rf/EK68srq7RsAgCDm8XgUGRl50e9vn2dg9uzZozZt2ujSSy/VyJEjlZubK0nKzMzUqVOnlJSUZLXt3Lmz2rVrp4yMDElSRkaGunfvboUXSUpOTpbH49HOnTutNmf2Udmmso/zSUtLU2RkpLXFxcX5empVd9tt0owZhBcAAGziU4Dp06ePFixYoA8++EDz5s1TTk6O+vfvr6NHjyo/P19hYWGKioryek9MTIzy8/MlSfn5+V7hpfJ45bELtfF4PDp58uR5a5sxY4bcbre15eXl+XJqAACgHvHpUQI33HCD9ecrrrhCffr0Ufv27fXWW28pPDy82ovzhdPplNPprJ0P27dPOnxYattW+kHYAgAANS+g26ijoqJ0+eWX69///rdiY2NVWlqqoqIirzYFBQWKjY2VJMXGxp51V1Ll64u1cblctocky4MPSr17SwsX2l0JAABBKaAAc+zYMX311Vdq3bq1evXqpYYNG2r16tXW8ezsbOXm5ioxMVGSlJiYqO3bt6uwsNBqk56eLpfLpYSEBKvNmX1Utqnso07gLiQAAGzlU4C59957tX79eu3bt0+ffvqphgwZotDQUI0YMUKRkZEaN26cpk2bprVr1yozM1NjxoxRYmKi+vbtK0kaNGiQEhISNGrUKH3xxRdatWqVHnroIaWmplqXfyZOnKi9e/fq/vvv1+7du/XCCy/orbfe0tSpU6v/7P1FgAEAwFY+/Qbmm2++0YgRI3T48GG1atVK/fr102effaZWrVpJkv7yl78oJCREw4YNU0lJiZKTk/XCCy9Y7w8NDdXy5cs1adIkJSYmqnHjxho9erQee+wxq018fLxWrFihqVOnas6cOWrbtq1efvllJScnV9MpVwMCDAAAtvJpHZj6pKr3kftlwgTppZekP/5Reuih6u0bAIAgVmPrwEDMwAAAYDMCjD8IMAAA2Mqn38Dge4MHS82aSQMH2l0JAABBiQDjjxtvrNgAAIAtuIQEAADqHWZg/PHttxVbVJT0/QrCAACg9jAD44+nn5a6dJHS0uyuBACAoESA8Qd3IQEAYCsCjD8IMAAA2IoA44/KAFNebm8dAAAEKQKMP5iBAQDAVgQYfxBgAACwFQHGHwQYAABsxTow/ujdW5o6teKfAACg1hFg/HHttRUbAACwBZeQAABAvcMMjD+OH5cOH5YaNZKio+2uBgCAoMMMjD9ef11q31666y67KwEAICgRYPzBXUgAANiKAOMPVuIFAMBWBBh/MAMDAICtCDD+IMAAAGArAow/CDAAANiKAOMPAgwAALZiHRh//Nd/SRMmSJdfbnclAAAEJQKMP3r0kP76V7urAAAgaHEJCQAA1DvMwPjj1Cnp6FHJ4ZCaNbO7GgAAgg4zMP5Yu1Zq0YInUgMAYBMCjD9YiRcAAFsRYPzBbdQAANiKAOMPAgwAALYiwPiDAAMAgK0IMP4gwAAAYCsCjD8IMAAA2Ip1YPzRsqU0cmTFPwEAQK0jwPijQwfp73+3uwoAAIIWl5AAAEC9Q4DxhzFScbF04oTdlQAAEJQCCjCzZs2Sw+HQlClTrH3FxcVKTU1VixYt1KRJEw0bNkwFBQVe78vNzVVKSooiIiIUHR2t++67T6dPn/Zqs27dOvXs2VNOp1OXXXaZFixYEEip1WvPHik8XGrTxu5KAAAISn4HmM2bN+uvf/2rrrjiCq/9U6dO1bJly7R48WKtX79eBw4c0NChQ63jZWVlSklJUWlpqT799FO99tprWrBggWbOnGm1ycnJUUpKiq677jplZWVpypQpGj9+vFatWuVvudWLu5AAALCX8cPRo0dNx44dTXp6uhk4cKC55557jDHGFBUVmYYNG5rFixdbbb/88ksjyWRkZBhjjHn//fdNSEiIyc/Pt9rMmzfPuFwuU1JSYowx5v777zddu3b1+sxf//rXJjk5+bw1FRcXG7fbbW15eXlGknG73f6c4oXl5BgjGRMeXv19AwAQxNxud5W+v/2agUlNTVVKSoqSkpK89mdmZurUqVNe+zt37qx27dopIyNDkpSRkaHu3bsrJibGapOcnCyPx6OdO3dabX7Yd3JystXHuaSlpSkyMtLa4uLi/Dm1qmEGBgAAW/kcYN58801t3bpVaWlpZx3Lz89XWFiYoqKivPbHxMQoPz/fanNmeKk8XnnsQm08Ho9Onjx5zrpmzJght9ttbXl5eb6eWtWFfD9sBBgAAGzh0zoweXl5uueee5Senq5GjRrVVE1+cTqdcjqdtfNhzMAAAGArn2ZgMjMzVVhYqJ49e6pBgwZq0KCB1q9fr7lz56pBgwaKiYlRaWmpioqKvN5XUFCg2NhYSVJsbOxZdyVVvr5YG5fLpfDwcJ9OsEZUBhhJKi+3rw4AAIKUTwHm+uuv1/bt25WVlWVtvXv31siRI60/N2zYUKtXr7bek52drdzcXCUmJkqSEhMTtX37dhUWFlpt0tPT5XK5lJCQYLU5s4/KNpV92C48XBoyRLr11oo1YQAAQK3y6RJS06ZN1a1bN699jRs3VosWLaz948aN07Rp09S8eXO5XC799re/VWJiovr27StJGjRokBISEjRq1CjNnj1b+fn5euihh5SammpdApo4caKee+453X///Ro7dqzWrFmjt956SytWrKiOcw5ckybSkiV2VwEAQNCq9mch/eUvf1FISIiGDRumkpISJScn64UXXrCOh4aGavny5Zo0aZISExPVuHFjjR49Wo899pjVJj4+XitWrNDUqVM1Z84ctW3bVi+//LKSk5Oru1wAAFAPOYz5cV4D8Xg8ioyMlNvtlsvlsrscAABQBVX9/uZZSP4oLZUaNJAcDsnttrsaAACCDgHGHyEh/7mFmlupAQCodQQYf5x5GzUBBgCAWkeA8YfDUbFJBBgAAGxAgPEXq/ECAGAbAoy/CDAAANiGAOMvAgwAALap9oXsgsbPfy6VlEi19QBJAABgIcD465137K4AAICgxSUkAABQ7xBgAABAvUOA8VenTlLjxtK2bXZXAgBA0CHA+Ov4cenECen0absrAQAg6BBg/MVt1AAA2IYA4y8CDAAAtiHA+IsAAwCAbQgw/iLAAABgGwKMv0K+HzoCDAAAtY6VeP111VVSq1aSy2V3JQAABB0CjL9ee83uCgAACFpcQgIAAPUOAQYAANQ7BBh/3XSTFB0trVxpdyUAAAQdAoy/vvtOOnRIOnnS7koAAAg6BBh/sQ4MAAC2IcD4iwADAIBtCDD+IsAAAGAbAoy/WIkXAADbEGD8xQwMAAC2YSVef3XsKBUUSM2b210JAABBhwDjrzlz7K4AAICgxSUkAABQ7xBgAABAvUOA8dfUqVJ8vPTKK3ZXAgBA0CHA+OvQIWnfPqmoyO5KAAAIOgQYf3EbNQAAtiHA+IsAAwCAbQgw/iLAAABgGwKMv3iUAAAAtvEpwMybN09XXHGFXC6XXC6XEhMTtXLlSut4cXGxUlNT1aJFCzVp0kTDhg1TQUGBVx+5ublKSUlRRESEoqOjdd999+n06dNebdatW6eePXvK6XTqsssu04IFC/w/w5rCDAwAALbxKcC0bdtWs2bNUmZmprZs2aKf/exnuuWWW7Rz505J0tSpU7Vs2TItXrxY69ev14EDBzR06FDr/WVlZUpJSVFpaak+/fRTvfbaa1qwYIFmzpxptcnJyVFKSoquu+46ZWVlacqUKRo/frxWrVpVTadcTVq3ljp3llq0sLsSAACCjsMYYwLpoHnz5nrqqad06623qlWrVlq4cKFuvfVWSdLu3bvVpUsXZWRkqG/fvlq5cqVuvPFGHThwQDExMZKk+fPna/r06Tp06JDCwsI0ffp0rVixQjt27LA+Y/jw4SoqKtIHH3xw3jpKSkpUUlJivfZ4PIqLi5Pb7ZbL5QrkFAEAQC3xeDyKjIy86Pe337+BKSsr05tvvqnjx48rMTFRmZmZOnXqlJKSkqw2nTt3Vrt27ZSRkSFJysjIUPfu3a3wIknJycnyeDzWLE5GRoZXH5VtKvs4n7S0NEVGRlpbXFycv6cGAADqOJ8DzPbt29WkSRM5nU5NnDhRS5cuVUJCgvLz8xUWFqaoqCiv9jExMcrPz5ck5efne4WXyuOVxy7UxuPx6OTJk+eta8aMGXK73daWl5fn66kBAIB6wucA06lTJ2VlZWnjxo2aNGmSRo8erV27dtVEbT5xOp3Wj4srtxo1d67UrZs0a1bNfg4AADhLA1/fEBYWpssuu0yS1KtXL23evFlz5szRr3/9a5WWlqqoqMhrFqagoECxsbGSpNjYWG3atMmrv8q7lM5s88M7lwoKCuRyuRQeHu5ruTWnsFDauVM6cMDuSgAACDoBrwNTXl6ukpIS9erVSw0bNtTq1autY9nZ2crNzVViYqIkKTExUdu3b1dhYaHVJj09XS6XSwkJCVabM/uobFPZR53BbdQAANjGpxmYGTNm6IYbblC7du109OhRLVy4UOvWrdOqVasUGRmpcePGadq0aWrevLlcLpd++9vfKjExUX379pUkDRo0SAkJCRo1apRmz56t/Px8PfTQQ0pNTZXT6ZQkTZw4Uc8995zuv/9+jR07VmvWrNFbb72lFStWVP/ZB4IAAwCAbXwKMIWFhbrjjjt08OBBRUZG6oorrtCqVav085//XJL0l7/8RSEhIRo2bJhKSkqUnJysF154wXp/aGioli9frkmTJikxMVGNGzfW6NGj9dhjj1lt4uPjtWLFCk2dOlVz5sxR27Zt9fLLLys5ObmaTrmasBIvAAC2CXgdmLqqqveR+y0tTXrwQWnMGOmVV6q/fwAAglCNrwMT9LiEBACAbQgw/oqKkuLipObN7a4EAICgwyUkAABQZ3AJCQAA/GgRYAAAQL1DgPHXO+9IP/2pdO+9dlcCAEDQ8flRAvjet99KmzdL3z8CAQAA1B5mYPzFbdQAANiGAOMvVuIFAMA2BBh/MQMDAIBtCDD+IsAAAGAbAoy/CDAAANiGAOOv8PCKxwiwyi8AALWO26j9ddNN0uHDdlcBAEBQYgYGAADUOwQYAABQ7xBg/LVli/Szn0njx9tdCQAAQYffwPirqEhau7bikQIAAKBWMQPjr8qVeMvL7a0DAIAgRIDxF+vAAABgGwKMvwgwAADYhgDjLwIMAAC2IcD4iwADAIBtCDD+atBAcjorNgAAUKu4jdpfPXtKxcV2VwEAQFBiBgYAANQ7BBgAAFDvEGD8tX+/dOON0m232V0JAABBh9/A+OvECWnFCsnlsrsSAACCDjMw/uI2agAAbEOA8RcBBgAA2xBg/EWAAQDANgQYfxFgAACwDQHGX5UBprxcMsbeWgAACDIEGH9VBhipIsQAAIBaw23U/mrevOLyUQgZEACA2kaA8ZfDUbEBAIBax/QBAACodwgw/jp1Sho+XPrVrypW5QUAALXGpwCTlpamq666Sk2bNlV0dLR++ctfKjs726tNcXGxUlNT1aJFCzVp0kTDhg1TQUGBV5vc3FylpKQoIiJC0dHRuu+++3T69GmvNuvWrVPPnj3ldDp12WWXacGCBf6dYU1atEj6xz+kkhK7KwEAIKj4FGDWr1+v1NRUffbZZ0pPT9epU6c0aNAgHT9+3GozdepULVu2TIsXL9b69et14MABDR061DpeVlamlJQUlZaW6tNPP9Vrr72mBQsWaObMmVabnJwcpaSk6LrrrlNWVpamTJmi8ePHa9WqVdVwytXkzLuQWAsGAIBa5TDG/0VMDh06pOjoaK1fv14DBgyQ2+1Wq1attHDhQt16662SpN27d6tLly7KyMhQ3759tXLlSt144406cOCAYmJiJEnz58/X9OnTdejQIYWFhWn69OlasWKFduzYYX3W8OHDVVRUpA8++OCctZSUlKjkjJkQj8ejuLg4ud1uuWrqgYuVP+LNz5e+PxcAAOA/j8ejyMjIi35/B/QbGLfbLUlq3ry5JCkzM1OnTp1SUlKS1aZz585q166dMjIyJEkZGRnq3r27FV4kKTk5WR6PRzt37rTanNlHZZvKPs4lLS1NkZGR1hYXFxfIqVUNq/ECAGALvwNMeXm5pkyZomuuuUbdunWTJOXn5yssLExRUVFebWNiYpSfn2+1ifnBbEXl64u18Xg8Onny5DnrmTFjhtxut7Xl5eX5e2pVR4ABAMAWfq8Dk5qaqh07dujjjz+uznr85nQ65XQ6a/dDCTAAANjCrxmYyZMna/ny5Vq7dq3atm1r7Y+NjVVpaamKioq82hcUFCg2NtZq88O7kipfX6yNy+VSeHi4PyXXjMpVeHmUAAAAtcqnAGOM0eTJk7V06VKtWbNG8fHxXsd79eqlhg0bavXq1da+7Oxs5ebmKjExUZKUmJio7du3q7Cw0GqTnp4ul8ulhIQEq82ZfVS2qeyjzjh4UDp2TOrQwe5KAAAIKj7dhXT33Xdr4cKFevfdd9WpUydrf2RkpDUzMmnSJL3//vtasGCBXC6Xfvvb30qSPv30U0kVt1H/5Cc/UZs2bTR79mzl5+dr1KhRGj9+vJ544glJFbdRd+vWTampqRo7dqzWrFmj3/3ud1qxYoWSk5OrVGtVf8UMAADqjip/fxsfSDrn9uqrr1ptTp48ae6++27TrFkzExERYYYMGWIOHjzo1c++ffvMDTfcYMLDw03Lli3N73//e3Pq1CmvNmvXrjU/+clPTFhYmLn00ku9PqMq3G63kWTcbrdP7wMAAPap6vd3QOvA1GW1MgMzbZp0+LD0xz9K7drVzGcAABBEamUdmKC3aJH0t79VhBgAAFBrCDCB4DZqAABsQYAJBAEGAABbEGACQYABAMAWBJhAEGAAALAFASYQlQGGlXgBAKhVBJhAVD5KgBkYAABqld8Pc4SkdeskY6TISLsrAQAgqBBgAtGypd0VAAAQlLiEBAAA6h0CTCCeflq6+27piy/srgQAgKBCgAnE229L8+ZJe/faXQkAAEGFABMI1oEBAMAWBJhAEGAAALAFASYQBBgAAGxBgAkEK/ECAGALAkwgWIkXAABbEGACwSUkAABswUq8gXjpJam4WGrVyu5KAAAIKgSYQLRubXcFAAAEJS4hAQCAeocAE4g33pDuu0/66CO7KwEAIKgQYAKxfLn0P/8jbdlidyUAAAQVAkwguAsJAABbEGACQYABAMAWBJhAsBIvAAC2IMAEgpV4AQCwBQEmEFxCAgDAFgSYQBBgAACwBSvxBuLBB6VJk6ToaLsrAQAgqBBgAtGmTcUGAABqFZeQAABAvcMMTCBWr5Y2bJD69pVuuMHuagAACBrMwARizRrpscekDz6wuxIAAIIKASYQ3IUEAIAtCDCBIMAAAGALAkwgKlfi5VECAADUKgJMIJiBAQDAFgSYQBBgAACwhc8BZsOGDbrpppvUpk0bORwOvfPOO17HjTGaOXOmWrdurfDwcCUlJWnPnj1ebY4cOaKRI0fK5XIpKipK48aN07Fjx7zabNu2Tf3791ejRo0UFxen2bNn+352NY0AAwCALXwOMMePH9eVV16p559//pzHZ8+erblz52r+/PnauHGjGjdurOTkZBUXF1ttRo4cqZ07dyo9PV3Lly/Xhg0bNGHCBOu4x+PRoEGD1L59e2VmZuqpp57So48+qhdffNGPU6xBI0dKGzdKf/iD3ZUAABBcTAAkmaVLl1qvy8vLTWxsrHnqqaesfUVFRcbpdJo33njDGGPMrl27jCSzefNmq83KlSuNw+Ew+/fvN8YY88ILL5hmzZqZkpISq8306dNNp06dzltLcXGxcbvd1paXl2ckGbfbHcgpAgCAWuR2u6v0/V2tv4HJyclRfn6+kpKSrH2RkZHq06ePMjIyJEkZGRmKiopS7969rTZJSUkKCQnRxo0brTYDBgxQWFiY1SY5OVnZ2dn67rvvzvnZaWlpioyMtLa4uLjqPDUAAFCHVGuAyc/PlyTFxMR47Y+JibGO5efnK/oHT29u0KCBmjdv7tXmXH2c+Rk/NGPGDLndbmvLy8sL/IQu5osvpNmzpSVLav6zAACA5UfzLCSn0ymn01m7H7pxozR9unTzzdLQobX72QAABLFqnYGJjY2VJBUUFHjtLygosI7FxsaqsLDQ6/jp06d15MgRrzbn6uPMz6gTuAsJAABbVGuAiY+PV2xsrFavXm3t83g82rhxoxITEyVJiYmJKioqUmZmptVmzZo1Ki8vV58+faw2GzZs0KlTp6w26enp6tSpk5o1a1adJQemMsCwEi8AALXK5wBz7NgxZWVlKSsrS1LFD3ezsrKUm5srh8OhKVOm6E9/+pPee+89bd++XXfccYfatGmjX/7yl5KkLl26aPDgwbrrrru0adMmffLJJ5o8ebKGDx+uNm3aSJJ+85vfKCwsTOPGjdPOnTu1aNEizZkzR9OmTau2E68WzMAAAGALn38Ds2XLFl133XXW68pQMXr0aC1YsED333+/jh8/rgkTJqioqEj9+vXTBx98oEaNGlnvef311zV58mRdf/31CgkJ0bBhwzR37lzreGRkpP75z38qNTVVvXr1UsuWLTVz5kyvtWLqBAIMAAC2cBhjjN1F1ASPx6PIyEi53W65XK6a+ZBFi6Thw6Vrr5XWrq2ZzwAAIIhU9fubZyEFghkYAABs8aO5jdoWAwZIa9ZIdemHxQAABAECTCCioys2AABQq7iEBAAA6h1mYAKRmystWya1aFHxY14AAFArmIEJxO7d0uTJUlqa3ZUAABBUCDCBYCVeAABsQYAJBLdRAwBgCwJMIAgwAADYggATCAIMAAC2IMAEggADAIAtCDCBIMAAAGAL1oEJRMeOFevANG5sdyUAAAQVAkwgIiOlG2+0uwoAAIIOl5AAAEC9wwxMINxuaelSqUED6fbb7a4GAICgQYAJREGBNGZMxaUkAgwAALWGS0iB4C4kAABsQYAJBAEGAABbEGACQYABAMAWBJhAEGAAALAFASYQBBgAAGxBgAlEyBnDV15uXx0AAAQZbqMOhMslLVr0n5kYAABQKwgwgXA6pdtus7sKAACCDpeQAABAvcMMTCDKyqQlSyr+OWyY1LCh3RUBABAUCDCBOH36P5eQvvtOioqytRwAAIIFl5ACceaPd7mVGgCAWkOACQQBBgAAWxBgAuFwVGwSAQYAgFpEgAkUq/ECAFDrCDCBIsAAAFDrCDCBqgwwPEoAAIBaw23UgZo/vyK8tGxpdyUAAAQNAkygRo2yuwIAAIIOl5AAAEC9wwxMIMrKpGeekQ4elK69Vho0SPr004rXrVtLV199/tfR0RV9FBZevG11vjcY+qorddTVvupKHXW1r7pSR13tq67UUVf7qit11GZf/ft7r4tWSwgw/lqyRLrnHumbbype//nPFX+BZ96NdLHXZ6rN9wZDX3WljrraV12po672VVfqqKt91ZU66mpfdaWO2uqrbVtpzhxp6NBzH68hdfoS0vPPP68OHTqoUaNG6tOnjzZt2mR3SRWWLJFuvfU/4aXSD/9yL/barvcGQ191pY662lddqaOu9lVX6qirfdWVOupqX3Wljtrqa//+iu/EJUvO36YG1NkAs2jRIk2bNk2PPPKItm7dqiuvvFLJyckqLCy0t7CysoqZF2PsrQMAgLqg8vtwypQLB51qVmcDzNNPP6277rpLY8aMUUJCgubPn6+IiAi98sor52xfUlIij8fjtdWIjz46e+YFAIBgZoyUl1fxHVlL6mSAKS0tVWZmppKSkqx9ISEhSkpKUkZGxjnfk5aWpsjISGuLi4urmeIOHqyZfgEAqO9q8TuyTgaYb7/9VmVlZYqJifHaHxMTo/z8/HO+Z8aMGXK73daWl5dXM8W1bl0z/QIAUN/V4nfkj+YuJKfTKafTWfMf1L9/xS+u9+/ndzAAAEiSw1Hx3di/f619ZJ2cgWnZsqVCQ0NVUFDgtb+goECxsbE2VfW90NCK28Wkir8wAACCWeV34TPP1Op6MHUywISFhalXr15avXq1ta+8vFyrV69WYmKijZV9b+hQ6R//kC65xHv/D//iLvbarvcGQ191pY662lddqaOu9lVX6qirfdWVOupqX3Wljtrqq23biu/EWl4Hps5eQpo2bZpGjx6t3r1766c//ameeeYZHT9+XGPGjLG7tApDh0q33FLxi2tWeqxbfdWVOupqX3WljrraV12po672VVfqqKt91ZU6gmAlXocxdfeHHM8995yeeuop5efn6yc/+Ynmzp2rPn36VOm9Ho9HkZGRcrvdcrlcNVwpAACoDlX9/q7TASYQBBgAAOqfqn5/18nfwAAAAFwIAQYAANQ7BBgAAFDvEGAAAEC9Q4ABAAD1DgEGAADUOwQYAABQ7xBgAABAvVNnHyUQqMr1+Twej82VAACAqqr83r7YOrs/2gBz9OhRSVJcXJzNlQAAAF8dPXpUkZGR5z3+o32UQHl5uQ4cOKCmTZvKUfmo72rg8XgUFxenvLw8HlFQBYyXbxgv3zBevmG8qo6x8k11jpcxRkePHlWbNm0UEnL+X7r8aGdgQkJC1LZt2xrr3+Vy8S+1Dxgv3zBevmG8fMN4VR1j5ZvqGq8LzbxU4ke8AACg3iHAAACAeocA4yOn06lHHnlETqfT7lLqBcbLN4yXbxgv3zBeVcdY+caO8frR/ogXAAD8eDEDAwAA6h0CDAAAqHcIMAAAoN4hwAAAgHqHAAMAAOodAoyPnn/+eXXo0EGNGjVSnz59tGnTJrtLsl1aWpquuuoqNW3aVNHR0frlL3+p7OxsrzbFxcVKTU1VixYt1KRJEw0bNkwFBQU2VVy3zJo1Sw6HQ1OmTLH2MV7e9u/fr9tvv10tWrRQeHi4unfvri1btljHjTGaOXOmWrdurfDwcCUlJWnPnj02VmyfsrIyPfzww4qPj1d4eLj+67/+S3/84x+9HowXzOO1YcMG3XTTTWrTpo0cDofeeecdr+NVGZsjR45o5MiRcrlcioqK0rhx43Ts2LFaPIvacaGxOnXqlKZPn67u3burcePGatOmje644w4dOHDAq4+aHCsCjA8WLVqkadOm6ZFHHtHWrVt15ZVXKjk5WYWFhXaXZqv169crNTVVn332mdLT03Xq1CkNGjRIx48ft9pMnTpVy5Yt0+LFi7V+/XodOHBAQ4cOtbHqumHz5s3661//qiuuuMJrP+P1H999952uueYaNWzYUCtXrtSuXbv05z//Wc2aNbPazJ49W3PnztX8+fO1ceNGNW7cWMnJySouLraxcns8+eSTmjdvnp577jl9+eWXevLJJzV79mw9++yzVptgHq/jx4/ryiuv1PPPP3/O41UZm5EjR2rnzp1KT0/X8uXLtWHDBk2YMKG2TqHWXGisTpw4oa1bt+rhhx/W1q1btWTJEmVnZ+vmm2/2alejY2VQZT/96U9Namqq9bqsrMy0adPGpKWl2VhV3VNYWGgkmfXr1xtjjCkqKjINGzY0ixcvttp8+eWXRpLJyMiwq0zbHT161HTs2NGkp6ebgQMHmnvuuccYw3j90PTp002/fv3Oe7y8vNzExsaap556ytpXVFRknE6neeONN2qjxDolJSXFjB071mvf0KFDzciRI40xjNeZJJmlS5dar6syNrt27TKSzObNm602K1euNA6Hw+zfv7/Waq9tPxyrc9m0aZORZL7++mtjTM2PFTMwVVRaWqrMzEwlJSVZ+0JCQpSUlKSMjAwbK6t73G63JKl58+aSpMzMTJ06dcpr7Dp37qx27doF9dilpqYqJSXFa1wkxuuH3nvvPfXu3Vu/+tWvFB0drR49euill16yjufk5Cg/P99rvCIjI9WnT5+gHK+rr75aq1ev1r/+9S9J0hdffKGPP/5YN9xwgyTG60KqMjYZGRmKiopS7969rTZJSUkKCQnRxo0ba73musTtdsvhcCgqKkpSzY/Vj/Zp1NXt22+/VVlZmWJiYrz2x8TEaPfu3TZVVfeUl5drypQpuuaaa9StWzdJUn5+vsLCwqx/qSvFxMQoPz/fhirt9+abb2rr1q3avHnzWccYL2979+7VvHnzNG3aND344IPavHmzfve73yksLEyjR4+2xuRc/9sMxvF64IEH5PF41LlzZ4WGhqqsrEyPP/64Ro4cKUmM1wVUZWzy8/MVHR3tdbxBgwZq3rx5UI9fcXGxpk+frhEjRlhPo67psSLAoFqlpqZqx44d+vjjj+0upc7Ky8vTPffco/T0dDVq1Mjucuq88vJy9e7dW0888YQkqUePHtqxY4fmz5+v0aNH21xd3fPWW2/p9ddf18KFC9W1a1dlZWVpypQpatOmDeOFGnHq1CnddtttMsZo3rx5tfa5XEKqopYtWyo0NPSsO0EKCgoUGxtrU1V1y+TJk7V8+XKtXbtWbdu2tfbHxsaqtLRURUVFXu2DdewyMzNVWFionj17qkGDBmrQoIHWr1+vuXPnqkGDBoqJiWG8ztC6dWslJCR47evSpYtyc3MlyRoT/rdZ4b777tMDDzyg4cOHq3v37ho1apSmTp2qtLQ0SYzXhVRlbGJjY8+6ceP06dM6cuRIUI5fZXj5+uuvlZ6ebs2+SDU/VgSYKgoLC1OvXr20evVqa195eblWr16txMREGyuznzFGkydP1tKlS7VmzRrFx8d7He/Vq5caNmzoNXbZ2dnKzc0NyrG7/vrrtX37dmVlZVlb7969NXLkSOvPjNd/XHPNNWfdlv+vf/1L7du3lyTFx8crNjbWa7w8Ho82btwYlON14sQJhYR4/6c9NDRU5eXlkhivC6nK2CQmJqqoqEiZmZlWmzVr1qi8vFx9+vSp9ZrtVBle9uzZow8//FAtWrTwOl7jYxXwz4CDyJtvvmmcTqdZsGCB2bVrl5kwYYKJiooy+fn5dpdmq0mTJpnIyEizbt06c/DgQWs7ceKE1WbixImmXbt2Zs2aNWbLli0mMTHRJCYm2lh13XLmXUjGMF5n2rRpk2nQoIF5/PHHzZ49e8zrr79uIiIizN///nerzaxZs0xUVJR59913zbZt28wtt9xi4uPjzcmTJ22s3B6jR482l1xyiVm+fLnJyckxS5YsMS1btjT333+/1SaYx+vo0aPm888/N59//rmRZJ5++mnz+eefW3fOVGVsBg8ebHr06GE2btxoPv74Y9OxY0czYsQIu06pxlxorEpLS83NN99s2rZta7Kysrz+219SUmL1UZNjRYDx0bPPPmvatWtnwsLCzE9/+lPz2Wef2V2S7SSdc3v11VetNidPnjR33323adasmYmIiDBDhgwxBw8etK/oOuaHAYbx8rZs2TLTrVs343Q6TefOnc2LL77odby8vNw8/PDDJiYmxjidTnP99deb7Oxsm6q1l8fjMffcc49p166dadSokbn00kvNf//3f3t9qQTzeK1du/ac/70aPXq0MaZqY3P48GEzYsQI06RJE+NyucyYMWPM0aNHbTibmnWhscrJyTnvf/vXrl1r9VGTY+Uw5ozlGQEAAOoBfgMDAADqHQIMAACodwgwAACg3iHAAACAeocAAwAA6h0CDAAAqHcIMAAAoN4hwAAAgHqHAAMAAOodAgwAAKh3CDAAAKDe+f9/lY4uVYumzAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time\t\t0.33 sec \n",
      "\n",
      "EPOCH  119 \tLOSS  0.3681258052837629\n",
      "-----------------------------------------------\n",
      "-----------------------------------------------\n",
      "118\n",
      "bestvaleur 0.3435190572758365\n",
      "TEACHER FORCE RATIO\t 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAIN\")\n",
    "model.train()\n",
    "print(\"path mode\\t\",path_mode)\n",
    "loss  = train(model, optimizer, scheduler, criterion, criterion_vision, clip, train_loader, validation_loader)\n",
    "print(\"LOSS \",loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, optimizer, scheduler, filename='checkpoint.pth.tar'):\n",
    "    start_epoch = 0\n",
    "    best_val=-1\n",
    "    if os.path.isfile(filename):\n",
    "        print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "        checkpoint = torch.load(filename)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler'])\n",
    "        try:\n",
    "            best_val=checkpoint['best_loss']\n",
    "        except:\n",
    "            best_val=-1\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\".format(filename, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "\n",
    "    return model, optimizer, scheduler, start_epoch, best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LOAD MODEL\")\n",
    "# Change device to cpu\n",
    "del model\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "model      = Seq2Seq(in_size, embed_size, hidden_size, dropout_val=dropout_val, batch_size=batch_size)\n",
    "model, optimizer, scheduler, start_epoch, best_val = load_checkpoint(model, optimizer, scheduler, filename='./save_models/model_best.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test dataset and loader\n",
    "print(\"Initializing val dataset\")\n",
    "dataset_val   = TrajectoryPredictionDataset(image_folder_path, DB_PATH_val, cnx_val)\n",
    "test_loader   = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=4, drop_last=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EVALUATE bst\")\n",
    "model.eval()\n",
    "path_mode = 'bst'\n",
    "print(\"path mode\\t\",path_mode)\n",
    "evaluate_eval(model, optimizer, criterion, criterion_vision, clip, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"EVALUATE top5\")\n",
    "model.eval()\n",
    "path_mode = 'top5'\n",
    "print(\"path mode\\t\",path_mode)\n",
    "evaluate_eval(model, optimizer, criterion, criterion_vision, clip, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "400dc80c028f38e78d0af9f2eedd78117085b98eeacd8fc6d80a8baae662b2d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
